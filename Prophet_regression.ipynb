{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b880fd",
   "metadata": {},
   "source": [
    "# Submission Volume Prediction: A Comparative Analysis\n",
    "\n",
    "This notebook walks through the process of forecasting submission volumes using various time series models, including **Prophet**, **LSTM**, and **hybrid approaches combined with Bayesian Model Averaging (BMA)**.  \n",
    "The goal is to identify the most accurate model for this specific dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup: Importing Libraries and Loading Data\n",
    "\n",
    "**Explanation:**  \n",
    "This first block imports all necessary Python libraries for:\n",
    "- Data manipulation\n",
    "- Time series modeling\n",
    "- Neural networks\n",
    "- Evaluation\n",
    "\n",
    "It also defines the path to the input CSV file and loads the dataset into a `pandas` DataFrame.  \n",
    "Basic error handling is included to ensure the file is found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b344529",
   "metadata": {},
   "source": [
    "# Submission Volume Prediction: A Comparative Analysis\n",
    "\n",
    "This notebook walks through the process of forecasting submission volumes using various time series models, including **Prophet**, **LSTM**, and **hybrid approaches combined with Bayesian Model Averaging (BMA)**.  \n",
    "The goal is to identify the most accurate model for this specific dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup: Importing Libraries and Loading Data\n",
    "\n",
    "**Explanation:**  \n",
    "This first block imports all necessary Python libraries for:\n",
    "- Data manipulation\n",
    "- Time series modeling\n",
    "- Neural networks\n",
    "- Evaluation\n",
    "\n",
    "It also defines the path to the input CSV file and loads the dataset into a `pandas` DataFrame.  \n",
    "Basic error handling is included to ensure the file is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3dccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Head:\n",
      "                           SubmissionID                  TitleName  TitleTier  \\\n",
      "0  668FB0E7-0D43-410F-8B46-6CED9483439F              Slam and Roll          5   \n",
      "1  7C9F567D-49AB-482A-AEE4-AD6192712BA9                   Valorant          1   \n",
      "2  78B1B956-FAD4-41E4-9263-D038442FEC4B           Hamster on Rails          5   \n",
      "3  DB7D5F5C-132E-4461-80F2-D0204B4E17CE         River City Girls 2          5   \n",
      "4  52A3126F-1973-4FD6-8D38-33092DEDCFD3       Farming Simulator 22          2   \n",
      "5  2666AB36-1B79-4C8F-A23B-180235CA653D  50 Years (for Windows 10)          5   \n",
      "6  B456C456-D640-4D52-9A40-A468634589A1            Killer Instinct          2   \n",
      "7  F822C3FF-B736-4070-8B7F-618D9838BF16              Diesel Legacy          4   \n",
      "8  EBFC4322-1E33-48B2-882D-CE275E459C91           Thimbleweed Park          5   \n",
      "9  6E2EE8F6-9708-49F0-B327-92EFDDB6346F        Company of Heroes 3          2   \n",
      "\n",
      "                         PublisherName  Is1PP   Region     SubmissionType  \\\n",
      "0             Jaime Dominguez Blazquez      0    India               Game   \n",
      "1                           Riot Games      0       UK               Game   \n",
      "2                         DX Gameworks      0       UK               Game   \n",
      "3              WayForward Technologies      0       UK               Game   \n",
      "4                      Giants Software      0       UK               Game   \n",
      "5                              Xitilon      0       US               Game   \n",
      "6  Xbox Game Studios - Core Publishing      1       UK               Game   \n",
      "7                        Maximum Games      0       UK               Game   \n",
      "8                      Terrible Toybox      0       UK  Handheld Verified   \n",
      "9                  Relic Entertainment      0  US & UK          Stub file   \n",
      "\n",
      "  SubmissionKickOffDate SubmissionCreatedDate SubmissionState ResultName  \\\n",
      "0            06/09/2024            06/09/2024        Complete   Complete   \n",
      "1            11/04/2024            11/04/2024        Complete   Complete   \n",
      "2            04/08/2023            04/08/2023        Complete       Fail   \n",
      "3            15/09/2022            15/09/2022        Complete       Pass   \n",
      "4            02/09/2021            02/09/2021        Complete       Fail   \n",
      "5            09/04/2021            09/04/2021        Complete       Pass   \n",
      "6            18/11/2023            18/11/2023        Complete       Fail   \n",
      "7            14/11/2024            14/11/2024        Complete       Fail   \n",
      "8            30/03/2025            30/03/2025        Complete   Complete   \n",
      "9            14/12/2022            14/12/2022          Ignore    Interim   \n",
      "\n",
      "                                  BugID  Severity  \\\n",
      "0  1FC4760B-DF7E-44BD-BE4A-8CCFD4C2953F       0.0   \n",
      "1  41C72C9C-BC43-4180-BE5C-D0736B0E7481       6.0   \n",
      "2  E25F028B-546C-4841-98C3-E83CF5A5815D       6.0   \n",
      "3                                   NaN       NaN   \n",
      "4                                   NaN       NaN   \n",
      "5                                   NaN       NaN   \n",
      "6  0D2B85A3-DE80-47FD-A0D8-0B8ACB7C6B3F      12.0   \n",
      "7  8205697A-4734-4DCA-BE5E-1C3A1F95DE2D      12.0   \n",
      "8  6F50804D-EB91-42E8-B292-9C68A87208FF       0.0   \n",
      "9                                   NaN       NaN   \n",
      "\n",
      "           TestRequirementCategoryName  \\\n",
      "0                 X1 Base Requirements   \n",
      "1                 X1 Base Requirements   \n",
      "2  X1 Events, Achievements, and Awards   \n",
      "3                                  NaN   \n",
      "4                                  NaN   \n",
      "5                                  NaN   \n",
      "6                 X1 Base Requirements   \n",
      "7                     X1 User Profiles   \n",
      "8                             Handheld   \n",
      "9                                  NaN   \n",
      "\n",
      "                                      BugDescription   IsBypassed  \\\n",
      "0                    This is an Optional submission.  Full Test 1   \n",
      "1  The title does not display the focus indicator...  Full Test 1   \n",
      "2  The title unlocks the King Hamster's Pride ach...  Full Test 1   \n",
      "3                                                NaN  Full Test 1   \n",
      "4                                                NaN  Full Test 1   \n",
      "5                                                NaN  Full Test 1   \n",
      "6  [1S and 1X 0/5 - Was previously only seen on D...  Full Test 1   \n",
      "7  The title does not display an appropriate syst...  Full Test 1   \n",
      "8             Handheld Verified: Defaults Just Work.  Full Test 1   \n",
      "9                                                NaN  Full Test 1   \n",
      "\n",
      "   PackageCount  \n",
      "0          15.0  \n",
      "1          36.0  \n",
      "2           9.0  \n",
      "3          30.0  \n",
      "4          69.0  \n",
      "5           4.0  \n",
      "6          19.0  \n",
      "7           9.0  \n",
      "8          18.0  \n",
      "9          29.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import matplotlib.pyplot as plt # Plotting is currently commented out\n",
    "# import seaborn as sns # Plotting is currently commented out\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Load the cleaned submission data exported from SQL\n",
    "CSV_PATH = \"c:/Users/t-matasert/OneDrive - Microsoft/desktop/L7 Data Science & AI/Project/VolumePredictionProject/data/input/SQL_Results_Submissions_Prophet_v3.csv\"\n",
    "\n",
    "# Read CSV and confirm it loaded properly\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(\"Original Data Head:\")\n",
    "    print(df.head(10))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CSV file not found at: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d86caa",
   "metadata": {},
   "source": [
    "## 2. Initial Data Preprocessing\n",
    "\n",
    "This block focuses on cleaning the raw data. It involves:\n",
    "\n",
    "- Stripping any leading/trailing whitespace from column names.\n",
    "- Converting the `SubmissionCreatedDate` column to datetime objects, ensuring correct parsing of **day-first** dates.\n",
    "- Standardizing the `BypassFlag` based on the `IsBypassed` column, mapping `'bypass'` and `'bypass once released'` to `1` and others to `0`.\n",
    "- Filtering out rows where `BypassFlag` is `1` (i.e., keeping only non-bypassed submissions).\n",
    "- Excluding specified `SubmissionType` categories that are not relevant for this volume prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8afb210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape after initial preprocessing: (241890, 18)\n",
      "Processed Data Head:\n",
      "                           SubmissionID             TitleName  TitleTier  \\\n",
      "0  668FB0E7-0D43-410F-8B46-6CED9483439F         Slam and Roll          5   \n",
      "1  7C9F567D-49AB-482A-AEE4-AD6192712BA9              Valorant          1   \n",
      "2  78B1B956-FAD4-41E4-9263-D038442FEC4B      Hamster on Rails          5   \n",
      "3  DB7D5F5C-132E-4461-80F2-D0204B4E17CE    River City Girls 2          5   \n",
      "4  52A3126F-1973-4FD6-8D38-33092DEDCFD3  Farming Simulator 22          2   \n",
      "\n",
      "              PublisherName  Is1PP Region SubmissionType  \\\n",
      "0  Jaime Dominguez Blazquez      0  India           Game   \n",
      "1                Riot Games      0     UK           Game   \n",
      "2              DX Gameworks      0     UK           Game   \n",
      "3   WayForward Technologies      0     UK           Game   \n",
      "4           Giants Software      0     UK           Game   \n",
      "\n",
      "  SubmissionKickOffDate SubmissionCreatedDate SubmissionState ResultName  \\\n",
      "0            06/09/2024            2024-09-06        Complete   Complete   \n",
      "1            11/04/2024            2024-04-11        Complete   Complete   \n",
      "2            04/08/2023            2023-08-04        Complete       Fail   \n",
      "3            15/09/2022            2022-09-15        Complete       Pass   \n",
      "4            02/09/2021            2021-09-02        Complete       Fail   \n",
      "\n",
      "                                  BugID  Severity  \\\n",
      "0  1FC4760B-DF7E-44BD-BE4A-8CCFD4C2953F       0.0   \n",
      "1  41C72C9C-BC43-4180-BE5C-D0736B0E7481       6.0   \n",
      "2  E25F028B-546C-4841-98C3-E83CF5A5815D       6.0   \n",
      "3                                   NaN       NaN   \n",
      "4                                   NaN       NaN   \n",
      "\n",
      "           TestRequirementCategoryName  \\\n",
      "0                 X1 Base Requirements   \n",
      "1                 X1 Base Requirements   \n",
      "2  X1 Events, Achievements, and Awards   \n",
      "3                                  NaN   \n",
      "4                                  NaN   \n",
      "\n",
      "                                      BugDescription   IsBypassed  \\\n",
      "0                    This is an Optional submission.  Full Test 1   \n",
      "1  The title does not display the focus indicator...  Full Test 1   \n",
      "2  The title unlocks the King Hamster's Pride ach...  Full Test 1   \n",
      "3                                                NaN  Full Test 1   \n",
      "4                                                NaN  Full Test 1   \n",
      "\n",
      "   PackageCount  BypassFlag  \n",
      "0          15.0           0  \n",
      "1          36.0           0  \n",
      "2           9.0           0  \n",
      "3          30.0           0  \n",
      "4          69.0           0  \n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert date column, handling potential dayfirst format\n",
    "df['SubmissionCreatedDate'] = pd.to_datetime(df['SubmissionCreatedDate'], dayfirst=True)\n",
    "\n",
    "# Update the BypassFlag logic to include 'bypass once released' as bypass\n",
    "df['BypassFlag'] = df['IsBypassed'].apply(lambda x: 1 if str(x).lower() in ['bypass', 'bypass once released'] else 0)\n",
    "\n",
    "# Filter out rows where BypassFlag is 1\n",
    "df = df[df['BypassFlag'] == 0]\n",
    "\n",
    "# Filter out unnecessary SubmissionTypes\n",
    "submission_types_to_exclude_prophet = ['Handheld Verified','Stub', 'Canary File', 'Hub App (General)', 'Closed Beta - Not Tested',\n",
    "                                     'Other', 'Compilation Disc', 'External Beta', 'Internal Beta', 'Beyond Console', 'Open Beta', 'Mouse & Keyboard']\n",
    "df = df[~df['SubmissionType'].astype(str).str.lower().isin([st.lower() for st in submission_types_to_exclude_prophet])]\n",
    "\n",
    "print(\"\\nData shape after initial preprocessing:\", df.shape)\n",
    "print(\"Processed Data Head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11be17b",
   "metadata": {},
   "source": [
    "## 3. Data Aggregation and Feature Engineering for Time Series\n",
    "\n",
    "The preprocessed data is now aggregated to a **monthly frequency**. Key steps include:\n",
    "\n",
    "- Resampling the data by month (`'M'`) based on `SubmissionCreatedDate`.\n",
    "- Calculating the **distinct count of `SubmissionID`** for each month to get the **monthly submission volume**. This becomes our target variable `y`.\n",
    "- Storing the original `y` values before transformation for later comparison.\n",
    "- Applying a **log transformation** (`np.log1p`) to the target variable `y`. This helps stabilize variance and often improves model performance for time series with **exponential trends**.\n",
    "- Calculating the **average `PackageCount`** per month and adding it as an **exogenous regressor** (`avg_package_count`) to the monthly DataFrame.\n",
    "- Any missing values in this regressor are **forward-filled** and then **backward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c0a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monthly Aggregated Data Head:\n",
      "          ds         y  y_original  avg_package_count\n",
      "0 2021-01-31  6.018593         410          26.072765\n",
      "1 2021-02-28  6.156979         471          34.388060\n",
      "2 2021-03-31  6.436150         623          36.368421\n",
      "3 2021-04-30  6.304449         546          37.331024\n",
      "4 2021-05-31  6.458338         637          30.225017\n",
      "\n",
      "Monthly Aggregated Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53 entries, 0 to 52\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   ds                 53 non-null     datetime64[ns]\n",
      " 1   y                  53 non-null     float64       \n",
      " 2   y_original         53 non-null     int64         \n",
      " 3   avg_package_count  53 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1)\n",
      "memory usage: 1.8 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-matasert\\AppData\\Local\\Temp\\ipykernel_32012\\2009404742.py:15: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  monthly_df['avg_package_count'] = monthly_df['avg_package_count'].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data monthly: count distinct SubmissionID per month\n",
    "monthly_df = df.resample('M', on='SubmissionCreatedDate')['SubmissionID'].nunique().reset_index(name='y')\n",
    "monthly_df.columns = ['ds', 'y'] # Prophet requires 'ds' for date and 'y' for target\n",
    "\n",
    "# Store original 'y' for later comparison and log transform the target variable 'y'\n",
    "monthly_df['y_original'] = monthly_df['y']\n",
    "monthly_df['y'] = np.log1p(monthly_df['y'])\n",
    "\n",
    "# Add average PackageCount per month as a regressor\n",
    "monthly_avg_package = df.resample('M', on='SubmissionCreatedDate')['PackageCount'].mean().reset_index()\n",
    "monthly_avg_package.columns = ['ds', 'avg_package_count']\n",
    "monthly_df = pd.merge(monthly_df, monthly_avg_package, on='ds', how='left')\n",
    "\n",
    "# Fill NaNs in avg_package_count that might arise from resampling, then ffill/bfill\n",
    "monthly_df['avg_package_count'] = monthly_df['avg_package_count'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(\"\\nMonthly Aggregated Data Head:\")\n",
    "print(monthly_df.head())\n",
    "print(\"\\nMonthly Aggregated Data Info:\")\n",
    "monthly_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df32ee4",
   "metadata": {},
   "source": [
    "## 4. Defining Holiday Effects\n",
    "\n",
    "Prophet allows for the inclusion of custom **holiday effects**.  \n",
    "This block defines a `DataFrame` specifying **\"year-end slowdown\"** periods around **Christmas** and **New Year**.\n",
    "\n",
    "For each holiday instance:\n",
    "- A **lower window** and an **upper window** are defined.\n",
    "- These windows capture the effects **leading up to and following** the specific holiday dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f6da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holiday Events DataFrame:\n",
      "             holiday         ds  lower_window  upper_window\n",
      "0  year_end_slowdown 2021-12-24            -3             3\n",
      "1  year_end_slowdown 2021-12-25            -3             3\n",
      "2  year_end_slowdown 2022-01-01            -3             3\n",
      "3  year_end_slowdown 2022-12-24            -3             3\n",
      "4  year_end_slowdown 2022-12-25            -3             3\n"
     ]
    }
   ],
   "source": [
    "# Create holiday slowdown dataframe for Prophet\n",
    "holiday_events = pd.DataFrame({\n",
    "    'holiday': 'year_end_slowdown',\n",
    "    'ds': pd.to_datetime([\n",
    "        '2021-12-24', '2021-12-25', '2022-01-01',\n",
    "        '2022-12-24', '2022-12-25', '2023-01-01',\n",
    "        '2023-12-24', '2023-12-25', '2024-01-01',\n",
    "        '2024-12-24', '2024-12-25', '2025-01-01' # Include future holidays for forecasting\n",
    "    ]),\n",
    "    'lower_window': -3, # Affects 3 days before\n",
    "    'upper_window': 3  # Affects 3 days after\n",
    "})\n",
    "print(\"\\nHoliday Events DataFrame:\")\n",
    "print(holiday_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517a31a",
   "metadata": {},
   "source": [
    "## 5. Defining Training Cutoff Date\n",
    "\n",
    "**Explanation:**  \n",
    "A `cutoff_date` is defined to separate **historical data** used for training the models from the data used for **evaluation** (the \"future\").\n",
    "\n",
    "- All models will be trained on data **before** this date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc89ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data up to 2024-05-01 will be used for training the initial models.\n",
      "Prophet training data shape: (40, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define train/test split for Prophet and training cutoff for LSTMs\n",
    "cutoff_date = pd.to_datetime('2024-05-01') # Marks end of training for Prophet and LSTMs\n",
    "train_monthly_prophet = monthly_df[monthly_df['ds'] < cutoff_date].copy()\n",
    "\n",
    "print(f\"\\nData up to {cutoff_date.date()} will be used for training the initial models.\")\n",
    "print(f\"Prophet training data shape: {train_monthly_prophet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b5c55",
   "metadata": {},
   "source": [
    "## 6. Prophet Model (Model P) – Initialization and Training\n",
    "\n",
    "This block initializes the main **Prophet model** (referred to as **Model P**).\n",
    "\n",
    "- It uses pre-determined **best hyperparameters** for:\n",
    "  - `changepoint_prior_scale`\n",
    "  - `seasonality_mode`\n",
    "  - `seasonality_prior_scale`\n",
    "  - `holidays_prior_scale`\n",
    "- **Yearly seasonality** is enabled.\n",
    "- **Weekly seasonality** is disabled (since the data is monthly).\n",
    "- `avg_package_count` is added as an **external regressor**.\n",
    "- A custom **monthly seasonality** is also added.\n",
    "- The model is then **trained (fit)** using the `train_monthly_prophet` DataFrame, which includes:\n",
    "  - The **log-transformed target `y`**\n",
    "  - The **`avg_package_count` regressor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8f46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Prophet Model (Model P) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:27 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Prophet model on data up to: 2024-04-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Prophet Model (Model P) ---\n",
    "print(\"\\n--- Training Prophet Model (Model P) ---\")\n",
    "\n",
    "# Use best params found from grid search completed separately.\n",
    "best_cps = 0.05\n",
    "best_sm = 'additive'\n",
    "best_sps = 10.0\n",
    "best_hps = 1.0\n",
    "\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False, # Data is monthly\n",
    "    changepoint_prior_scale=best_cps,\n",
    "    seasonality_mode=best_sm,\n",
    "    seasonality_prior_scale=best_sps,\n",
    "    holidays_prior_scale=best_hps,\n",
    "    holidays=holiday_events\n",
    ")\n",
    "prophet_model.add_regressor('avg_package_count')\n",
    "prophet_model.add_seasonality(name='monthly', period=30.5, fourier_order=5) # Custom monthly seasonality\n",
    "\n",
    "print(f\"Training Prophet model on data up to: {train_monthly_prophet['ds'].max().date()}\")\n",
    "prophet_model.fit(train_monthly_prophet[['ds', 'y', 'avg_package_count']])\n",
    "print(\"Prophet model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4f2fc",
   "metadata": {},
   "source": [
    "## 7. Prophet Model (Model P) – Generating Full Period Forecast\n",
    "\n",
    "This block defines the **forecast horizon** and creates a template `DataFrame` (`future_df_template`) that includes both **historical dates** and **future dates** for prediction.\n",
    "\n",
    "- The common **evaluation period** starts from `cutoff_date` and ends on `eval_end_common`.\n",
    "- The `make_future_dataframe` method is used to generate the future dates.\n",
    "- The `avg_package_count` regressor values are **merged** into this template.\n",
    "  - Any missing future values are **forward-filled** and **backward-filled**.\n",
    "- Actual `y` values are also merged into the template for later evaluation.\n",
    "- Prophet’s `predict` method is called on this template to generate forecasts (`yhat`) for the entire period.\n",
    "- These predictions are stored in:  \n",
    "  `future_df_template['prophet_pred_log']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a0e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Future DataFrame Template Head with Prophet Predictions:\n",
      "          ds  avg_package_count         y  prophet_pred_log\n",
      "0 2021-01-31          26.072765  6.018593          6.017682\n",
      "1 2021-02-28          34.388060  6.156979          6.130718\n",
      "2 2021-03-31          36.368421  6.436150          6.461549\n",
      "3 2021-04-30          37.331024  6.304449          6.299305\n",
      "4 2021-05-31          30.225017  6.458338          6.489661\n",
      "\n",
      "Future DataFrame Template Tail with Prophet Predictions:\n",
      "           ds  avg_package_count         y  prophet_pred_log\n",
      "45 2024-10-31          41.185997  7.031741          6.965097\n",
      "46 2024-11-30          33.339187  6.864848          6.927305\n",
      "47 2024-12-31          33.715368  6.726233          6.477858\n",
      "48 2025-01-31          29.726939  6.845880          6.398067\n",
      "49 2025-02-28          26.375654  6.755769          8.050593\n"
     ]
    }
   ],
   "source": [
    "# Define forecast horizon for all models\n",
    "eval_start_common = cutoff_date # Start of common evaluation period\n",
    "eval_end_common = pd.to_datetime('2025-02-28') # Final evaluation end date\n",
    "\n",
    "# Calculate periods needed for make_future_dataframe based on Prophet's training end\n",
    "periods_from_train_end = (eval_end_common.to_period('M') - train_monthly_prophet['ds'].max().to_period('M')).n\n",
    "min_forecast_periods = 6 # Ensure a minimum forecast length\n",
    "periods_for_future_df = max(periods_from_train_end, min_forecast_periods)\n",
    "\n",
    "# Create a future dataframe template that will be used by all models for consistency\n",
    "future_df_template = prophet_model.make_future_dataframe(periods=periods_for_future_df, freq='M')\n",
    "# Merge actual 'y' and 'avg_package_count' for the entire range (historical and future placeholders)\n",
    "future_df_template = pd.merge(future_df_template, monthly_df[['ds', 'avg_package_count', 'y']], on='ds', how='left')\n",
    "future_df_template['avg_package_count'] = future_df_template['avg_package_count'].ffill().bfill() # Fill regressor for future dates\n",
    "\n",
    "# Store Prophet's full forecast in the template\n",
    "forecast_prophet_full = prophet_model.predict(future_df_template[['ds', 'avg_package_count']]) # Predict on the template\n",
    "future_df_template['prophet_pred_log'] = forecast_prophet_full['yhat']\n",
    "\n",
    "print(\"\\nFuture DataFrame Template Head with Prophet Predictions:\")\n",
    "print(future_df_template.head())\n",
    "print(\"\\nFuture DataFrame Template Tail with Prophet Predictions:\")\n",
    "print(future_df_template.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e6c89",
   "metadata": {},
   "source": [
    "## 8. Standalone LSTM Model (Model SL) – Data Preparation\n",
    "This section prepares data for the **Standalone LSTM model (Model SL)**, which directly predicts the **log-transformed submission volume `y`**.\n",
    "\n",
    "- A **lookback period** is defined, determining how many past time steps are used to predict the next.\n",
    "- The target variable `y` is **scaled** using `MinMaxScaler` to a range of `[0, 1]`, which is generally beneficial for LSTM performance.\n",
    "- **Input sequences (`X`)** and corresponding **target values (`y`)** are created:\n",
    "  - `X` consists of `lookback` previous scaled `y` values.\n",
    "  - `y` is the next scaled `y` value.\n",
    "- The **dates (`ds`)** corresponding to the target `y` values are stored for later **alignment and residual calculation**.\n",
    "- `X` is reshaped to the required **3D format** for LSTM input: `[samples, timesteps, features]`.\n",
    "- The data is split into:\n",
    "  - **Training sequences** (up to `cutoff_date`)\n",
    "  - Sequences for **later prediction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29df1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Data for Standalone LSTM Model (Model SL) ---\n",
      "Standalone LSTM training data shapes: X=(37, 3, 1), y=(37,)\n",
      "Number of training sequences for Standalone LSTM: 37\n"
     ]
    }
   ],
   "source": [
    "# --- Standalone LSTM Model (Model SL - Directly predicting 'y') ---\n",
    "print(\"\\n--- Preparing Data for Standalone LSTM Model (Model SL) ---\")\n",
    "lookback = 3 # Number of previous months to use for predicting the next month\n",
    "\n",
    "# Prepare data for standalone LSTM: target is 'y' (log-transformed submission volume)\n",
    "data_for_standalone_lstm = monthly_df[['ds', 'y']].copy() # Keep 'ds' for easier indexing and merging later\n",
    "\n",
    "# Scale the target variable 'y'\n",
    "scaler_standalone_lstm_y = MinMaxScaler(feature_range=(0, 1))\n",
    "data_for_standalone_lstm['y_scaled'] = scaler_standalone_lstm_y.fit_transform(data_for_standalone_lstm[['y']])\n",
    "\n",
    "# Create sequences for X (input) and y (target)\n",
    "X_standalone_sequences, y_standalone_sequences_scaled = [], []\n",
    "ds_for_y_standalone = [] # To keep track of dates for y_standalone_sequences_scaled (targets)\n",
    "\n",
    "for i in range(lookback, len(data_for_standalone_lstm)):\n",
    "    X_standalone_sequences.append(data_for_standalone_lstm['y_scaled'].iloc[i-lookback:i].values)\n",
    "    y_standalone_sequences_scaled.append(data_for_standalone_lstm['y_scaled'].iloc[i])\n",
    "    ds_for_y_standalone.append(data_for_standalone_lstm['ds'].iloc[i]) # Date of the target y\n",
    "\n",
    "X_standalone_sequences = np.array(X_standalone_sequences)\n",
    "y_standalone_sequences_scaled = np.array(y_standalone_sequences_scaled)\n",
    "ds_for_y_standalone = pd.Series(ds_for_y_standalone, name='ds') # Convert to Series for easier indexing\n",
    "\n",
    "# Reshape X for LSTM input: [samples, timesteps, features]\n",
    "# Here, features = 1 (only using lagged 'y')\n",
    "X_standalone_sequences = X_standalone_sequences.reshape((X_standalone_sequences.shape[0], X_standalone_sequences.shape[1], 1))\n",
    "\n",
    "# Training data for Standalone LSTM ends before cutoff_date\n",
    "# ds_for_y_standalone contains the dates for y_standalone_sequences_scaled\n",
    "train_indices_standalone = ds_for_y_standalone[ds_for_y_standalone < cutoff_date].index\n",
    "\n",
    "X_train_standalone_lstm = X_standalone_sequences[train_indices_standalone]\n",
    "y_train_standalone_lstm_scaled = y_standalone_sequences_scaled[train_indices_standalone]\n",
    "\n",
    "# Store corresponding 'ds' and actual 'y' (log-transformed) for the training period of Standalone LSTM\n",
    "# This is needed for calculating residuals of the Standalone LSTM later\n",
    "ds_train_standalone_lstm = ds_for_y_standalone[train_indices_standalone]\n",
    "actual_y_train_standalone_lstm_log = data_for_standalone_lstm.set_index('ds').loc[ds_train_standalone_lstm, 'y'].values\n",
    "\n",
    "print(f\"Standalone LSTM training data shapes: X={X_train_standalone_lstm.shape}, y={y_train_standalone_lstm_scaled.shape}\")\n",
    "print(f\"Number of training sequences for Standalone LSTM: {len(ds_train_standalone_lstm)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4f989",
   "metadata": {},
   "source": [
    "## 9. Standalone LSTM Model (Model SL) – Definition and Training\n",
    "\n",
    "This block defines and trains the **Standalone LSTM model (Model SL)**.\n",
    "\n",
    "- A **sequential Keras model** is created.\n",
    "- It consists of:\n",
    "  - An **LSTM layer** with 32 units, taking input of shape `(lookback, 1)`  \n",
    "    (i.e., 1 feature: the lagged, scaled `y`)\n",
    "  - A **Dense output layer** with 1 unit to predict the next scaled `y` value.\n",
    "- The model is compiled with:\n",
    "  - `'mse'` (mean squared error) loss\n",
    "  - `'adam'` optimizer\n",
    "- The model is trained for **100 epochs** with a **batch size of 4**.\n",
    "- `EarlyStopping` is used to:\n",
    "  - Monitor the training loss\n",
    "  - Prevent overfitting\n",
    "  - Automatically restore the **best weights**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812b78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standalone LSTM model (Model SL)...\n",
      "Standalone LSTM model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Define and train the standalone LSTM model (using user-specified \"best\" params)\n",
    "model_standalone_lstm = Sequential()\n",
    "lookback = 3 # Number of lagged observations to use (explicitly set as per user update)\n",
    "model_standalone_lstm.add(LSTM(32, input_shape=(lookback, 1))) # 1 feature: lagged 'y'\n",
    "model_standalone_lstm.add(Dense(1)) # Output layer for predicting the single next value\n",
    "model_standalone_lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training Standalone LSTM model (Model SL)...\")\n",
    "# Using EarlyStopping to prevent overfitting and restore best weights\n",
    "standalone_lstm_early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "model_standalone_lstm.fit(X_train_standalone_lstm, y_train_standalone_lstm_scaled,\n",
    "                          epochs=100, \n",
    "                          batch_size=4, \n",
    "                          verbose=0, # Set to 1 or 2 to see training progress\n",
    "                          callbacks=[standalone_lstm_early_stopping])\n",
    "print(\"Standalone LSTM model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423aca37",
   "metadata": {},
   "source": [
    "## 10. Standalone LSTM Model (Model SL) – Generating Full Period Forecast\n",
    "\n",
    "After training, the **Standalone LSTM model** is used to generate predictions for the entire **forecast horizon** defined in `future_df_template`.\n",
    "\n",
    "- Predictions begin from the **end of the LSTM’s training data**.\n",
    "- An **iterative (auto-regressive)** forecasting process is used:\n",
    "  - The **last known sequence** from the training data is used as the initial input.\n",
    "  - The model predicts **one step ahead**.\n",
    "  - This prediction is then used to **update the input sequence** for the next prediction.\n",
    "- All **scaled predictions** are collected.\n",
    "- These predictions are then **inverse-transformed** using `scaler_standalone_lstm_y` to return them to the **original log-transformed scale**.\n",
    "- The predictions are **aligned with the dates** in `future_df_template` and stored in the `standalone_lstm_pred_log` column.\n",
    "- Any remaining `NaN`s (e.g., at the beginning if the lookback period isn't fully covered by `ds_for_y_standalone`, or at the end if predictions don’t perfectly align with `future_df_template` length) are **forward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93300935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full period predictions with Standalone LSTM (Model SL)...\n",
      "Standalone LSTM full period predictions merged into future_df_template.\n",
      "Future DataFrame Template Head with SL Predictions:\n",
      "          ds  prophet_pred_log  standalone_lstm_pred_log\n",
      "0 2021-01-31          6.017682                       NaN\n",
      "1 2021-02-28          6.130718                       NaN\n",
      "2 2021-03-31          6.461549                       NaN\n",
      "3 2021-04-30          6.299305                  6.304449\n",
      "4 2021-05-31          6.489661                  6.458338\n",
      "Future DataFrame Template Tail with SL Predictions:\n",
      "           ds  prophet_pred_log  standalone_lstm_pred_log\n",
      "45 2024-10-31          6.965097                  6.720220\n",
      "46 2024-11-30          6.927305                  6.714398\n",
      "47 2024-12-31          6.477858                  6.710331\n",
      "48 2025-01-31          6.398067                  6.707211\n",
      "49 2025-02-28          8.050593                  6.704997\n"
     ]
    }
   ],
   "source": [
    "# Generate Standalone LSTM predictions for the full future_df_template horizon\n",
    "print(\"Generating full period predictions with Standalone LSTM (Model SL)...\")\n",
    "\n",
    "# Start with the scaled y-values from the training period of the standalone LSTM\n",
    "# These are the actual scaled values up to the point where training ended.\n",
    "all_standalone_lstm_preds_scaled = list(y_standalone_sequences_scaled[train_indices_standalone])\n",
    "\n",
    "\n",
    "# The first input sequence for iterative forecasting is the last sequence from X_train_standalone_lstm\n",
    "current_input_sequence = X_train_standalone_lstm[-1].reshape(1, lookback, 1)\n",
    "\n",
    "# Determine how many future steps we need to predict beyond the training data\n",
    "# to fill up to the end of future_df_template\n",
    "last_train_date_sl = ds_train_standalone_lstm.max()\n",
    "num_future_preds_needed = len(future_df_template[future_df_template['ds'] > last_train_date_sl])\n",
    "\n",
    "for _ in range(num_future_preds_needed):\n",
    "    pred_scaled = model_standalone_lstm.predict(current_input_sequence, verbose=0)[0,0]\n",
    "    all_standalone_lstm_preds_scaled.append(pred_scaled)\n",
    "    # Update sequence: remove oldest, add new prediction\n",
    "    new_element_for_sequence = np.array([[[pred_scaled]]]) # Shape (1,1,1)\n",
    "    current_input_sequence = np.append(current_input_sequence[:, 1:, :], new_element_for_sequence, axis=1)\n",
    "\n",
    "# Inverse transform all predictions (training part + forecasted part)\n",
    "all_standalone_lstm_preds_log = scaler_standalone_lstm_y.inverse_transform(np.array(all_standalone_lstm_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Align these predictions with future_df_template\n",
    "# The predictions correspond to `ds_train_standalone_lstm` and then the future dates\n",
    "# Create a temporary DataFrame for merging\n",
    "pred_dates_for_sl_merge = list(ds_train_standalone_lstm) + \\\n",
    "                          list(future_df_template[future_df_template['ds'] > last_train_date_sl]['ds'])\n",
    "\n",
    "# Ensure lengths match if there are slight discrepancies\n",
    "max_len = min(len(pred_dates_for_sl_merge), len(all_standalone_lstm_preds_log))\n",
    "temp_standalone_lstm_preds_df = pd.DataFrame({\n",
    "    'ds': pred_dates_for_sl_merge[:max_len], \n",
    "    'standalone_lstm_pred_log': all_standalone_lstm_preds_log[:max_len]\n",
    "})\n",
    "\n",
    "# Merge into future_df_template\n",
    "future_df_template = pd.merge(future_df_template.drop(columns=['standalone_lstm_pred_log'], errors='ignore'), \n",
    "                              temp_standalone_lstm_preds_df, on='ds', how='left')\n",
    "\n",
    "# Fill any NaNs that might occur due to merge or slight misalignments\n",
    "# Forward fill is generally appropriate for forecasts\n",
    "future_df_template['standalone_lstm_pred_log'] = future_df_template['standalone_lstm_pred_log'].ffill()\n",
    "# If there are still NaNs at the beginning (before LSTM predictions start), fill with a known value or handle as needed\n",
    "# For this setup, it's assumed LSTM predictions will cover the relevant forecast period.\n",
    "\n",
    "print(\"Standalone LSTM full period predictions merged into future_df_template.\")\n",
    "print(\"Future DataFrame Template Head with SL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log']].head())\n",
    "print(\"Future DataFrame Template Tail with SL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2ed0e",
   "metadata": {},
   "source": [
    "## 11. Residual-Correcting LSTM (for Prophet's Residuals – Model P_RL) – Data Preparation & Training\n",
    "\n",
    "**Explanation:**  \n",
    "This section introduces the first **hybrid approach**: using an **LSTM to model the residuals** of the main Prophet model (Model P).  \n",
    "This hybrid model is referred to as **Model P_RL**.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "- **Residual Calculation**:  \n",
    "  - Prophet’s predictions (`yhat`) on its training data (`train_monthly_prophet`) are obtained.\n",
    "  - Residuals are calculated as:  \n",
    "    `actual_y - prophet_yhat`\n",
    "\n",
    "- **Data Scaling**:  \n",
    "  - Residuals are scaled using `StandardScaler` (since they are centered around zero).\n",
    "  - The `avg_package_count` regressor (for this LSTM’s training period) is scaled using `MinMaxScaler`.\n",
    "\n",
    "- **Sequence Creation**:  \n",
    "  - Input sequences (`X`) for the LSTM consist of:\n",
    "    - Lagged **scaled residuals**\n",
    "    - Lagged **scaled `avg_package_count`**\n",
    "  - The target (`y`) is the **next scaled residual**.\n",
    "\n",
    "- **Train/Validation Split**:  \n",
    "  - The sequenced data is split into **training** and **validation** sets for the `P_RL` LSTM.\n",
    "\n",
    "- **P_RL LSTM Definition & Training**:\n",
    "  - A **simplified LSTM model** (`model_p_rl`) is defined:\n",
    "    - 1 LSTM layer with **16 units**\n",
    "    - A `Dropout` layer\n",
    "    - A `Dense` output layer\n",
    "  - The model is compiled and trained to predict the **scaled residuals**.\n",
    "  - `EarlyStopping` is used, monitoring **validation loss**.\n",
    "  \n",
    "- The model’s performance on the **validation set** is **evaluated and printed**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce077f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Residual-Correcting LSTM (for Prophet's residuals - Model P_RL) ---\n",
      "P_RL LSTM training data shapes: X=(29, 3, 2), y=(29,)\n",
      "P_RL LSTM validation data shapes: X=(8, 3, 2), y=(8,)\n",
      "Training P_RL LSTM model (to predict Prophet residuals)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_RL LSTM model training complete.\n",
      "P_RL LSTM Validation MSE (on scaled residuals): 0.3323\n",
      "P_RL LSTM Validation MAE (on original scale residuals): 0.0089\n"
     ]
    }
   ],
   "source": [
    "# --- Residual-Correcting LSTM (for Prophet's residuals - Model P_RL) ---\n",
    "print(\"\\n--- Training Residual-Correcting LSTM (for Prophet's residuals - Model P_RL) ---\")\n",
    "\n",
    "# Get Prophet's predictions on its training data to calculate residuals\n",
    "prophet_train_preds_df = prophet_model.predict(train_monthly_prophet[['ds', 'avg_package_count']])\n",
    "p_rl_train_source = pd.merge(train_monthly_prophet[['ds', 'y', 'avg_package_count']],\n",
    "                             prophet_train_preds_df[['ds', 'yhat']], on='ds', how='inner')\n",
    "p_rl_train_source['residual'] = p_rl_train_source['y'] - p_rl_train_source['yhat'] # Prophet's residuals\n",
    "\n",
    "# Scale residuals and the regressor for this LSTM\n",
    "scaler_p_rl_residual = StandardScaler() # StandardScaler is often good for residuals\n",
    "p_rl_train_source['residual_scaled'] = scaler_p_rl_residual.fit_transform(p_rl_train_source[['residual']])\n",
    "\n",
    "scaler_p_rl_pkg_count = MinMaxScaler() # Scaler for avg_package_count for this P_RL LSTM\n",
    "p_rl_train_source['avg_package_count_scaled'] = scaler_p_rl_pkg_count.fit_transform(p_rl_train_source[['avg_package_count']])\n",
    "\n",
    "# Create sequences for P_RL LSTM\n",
    "X_p_rl, y_p_rl_scaled = [], []\n",
    "ds_for_y_p_rl = [] # Dates for the target residuals\n",
    "\n",
    "for i in range(lookback, len(p_rl_train_source)):\n",
    "    # Features: lagged scaled residuals and lagged scaled avg_package_count\n",
    "    sequence = p_rl_train_source[['residual_scaled', 'avg_package_count_scaled']].iloc[i-lookback:i].values\n",
    "    X_p_rl.append(sequence)\n",
    "    y_p_rl_scaled.append(p_rl_train_source['residual_scaled'].iloc[i])\n",
    "    ds_for_y_p_rl.append(p_rl_train_source['ds'].iloc[i])\n",
    "\n",
    "X_p_rl, y_p_rl_scaled = np.array(X_p_rl), np.array(y_p_rl_scaled)\n",
    "ds_for_y_p_rl = pd.Series(ds_for_y_p_rl, name='ds')\n",
    "\n",
    "# Reshape X_p_rl for LSTM: [samples, timesteps, features]\n",
    "# Here, features = 2 (lagged scaled residual, lagged scaled avg_package_count)\n",
    "X_p_rl = X_p_rl.reshape((X_p_rl.shape[0], X_p_rl.shape[1], 2))\n",
    "\n",
    "# Train/Validation split for P_RL LSTM (e.g., last 20% for validation)\n",
    "# This split is on the sequences derived from Prophet's training data\n",
    "validation_split_p_rl = int(len(X_p_rl) * 0.8)\n",
    "X_train_p_rl, X_val_p_rl = X_p_rl[:validation_split_p_rl], X_p_rl[validation_split_p_rl:]\n",
    "y_train_p_rl_scaled, y_val_p_rl_scaled = y_p_rl_scaled[:validation_split_p_rl], y_p_rl_scaled[validation_split_p_rl:]\n",
    "ds_val_p_rl = ds_for_y_p_rl[validation_split_p_rl:] # Dates for the validation set\n",
    "\n",
    "print(f\"P_RL LSTM training data shapes: X={X_train_p_rl.shape}, y={y_train_p_rl_scaled.shape}\")\n",
    "print(f\"P_RL LSTM validation data shapes: X={X_val_p_rl.shape}, y={y_val_p_rl_scaled.shape}\")\n",
    "\n",
    "# Define and train the P_RL LSTM model\n",
    "model_p_rl = Sequential()\n",
    "model_p_rl.add(LSTM(16, input_shape=(lookback, 2), kernel_regularizer=l2(0.001))) # 2 features\n",
    "model_p_rl.add(Dropout(0.2))\n",
    "model_p_rl.add(Dense(1)) # Output layer for predicting the scaled residual\n",
    "model_p_rl.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training P_RL LSTM model (to predict Prophet residuals)...\")\n",
    "p_rl_early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "history_p_rl = model_p_rl.fit(X_train_p_rl, y_train_p_rl_scaled,\n",
    "                              epochs=150,\n",
    "                              batch_size=2,\n",
    "                              validation_data=(X_val_p_rl, y_val_p_rl_scaled),\n",
    "                              verbose=0,\n",
    "                              callbacks=[p_rl_early_stopping])\n",
    "\n",
    "print(\"P_RL LSTM model training complete.\")\n",
    "\n",
    "# Evaluate P_RL model on its validation set (predicting Prophet's residuals)\n",
    "val_loss_p_rl = model_p_rl.evaluate(X_val_p_rl, y_val_p_rl_scaled, verbose=0)\n",
    "print(f\"P_RL LSTM Validation MSE (on scaled residuals): {val_loss_p_rl:.4f}\")\n",
    "\n",
    "# Store actual residuals for the P_RL validation period for later analysis if needed\n",
    "actual_residuals_val_p_rl_scaled = y_val_p_rl_scaled\n",
    "predicted_residuals_val_p_rl_scaled = model_p_rl.predict(X_val_p_rl, verbose=0).flatten()\n",
    "\n",
    "actual_residuals_val_p_rl = scaler_p_rl_residual.inverse_transform(actual_residuals_val_p_rl_scaled.reshape(-1, 1)).flatten()\n",
    "predicted_residuals_val_p_rl = scaler_p_rl_residual.inverse_transform(predicted_residuals_val_p_rl_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "val_mae_p_rl_residuals = mean_absolute_error(actual_residuals_val_p_rl, predicted_residuals_val_p_rl)\n",
    "print(f\"P_RL LSTM Validation MAE (on original scale residuals): {val_mae_p_rl_residuals:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adf029",
   "metadata": {},
   "source": [
    "## 12. P_RL Model – Generating Full Period Residual Forecasts and Combining with Prophet\n",
    "\n",
    "**Explanation:**  \n",
    "This block uses the trained **P_RL LSTM** to forecast **Prophet's residuals** over the entire period covered by `future_df_template`.  \n",
    "These forecasted residuals are then **added back** to Prophet's main forecast (`prophet_pred_log`) to create the **hybrid P + P_RL prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Prepare Input Data for P_RL Forecasting:\n",
    "\n",
    "- `future_df_template` already contains:\n",
    "  - Prophet's predictions (`prophet_pred_log`)\n",
    "  - `avg_package_count`\n",
    "\n",
    "- For **dates up to `cutoff_date`**:\n",
    "  - \"Actual\" `y` values are used to calculate historical **residuals** for the P_RL model's lookback window.\n",
    "\n",
    "- For **dates beyond `cutoff_date`**:\n",
    "  - Prophet’s own predictions are used as the base to estimate residuals that **P_RL will try to correct**.\n",
    "\n",
    "- Both residuals and `avg_package_count` are **scaled** using the same scalers (`scaler_p_rl_residual`, `scaler_p_rl_pkg_count`) fitted during P_RL training.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Iterative Residual Forecasting:\n",
    "\n",
    "- An **iterative (auto-regressive)** approach is used (similar to the Standalone LSTM model):\n",
    "  - The **P_RL LSTM** predicts one **scaled residual** at a time.\n",
    "  - The input sequence is updated after each step with:\n",
    "    - The **new forecasted scaled residual**\n",
    "    - The corresponding **scaled `avg_package_count`**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Inverse Transform and Combine:\n",
    "\n",
    "- The forecasted **scaled residuals** are **inverse-transformed** back to their original scale.\n",
    "- These residuals are then **added to Prophet’s log-transformed predictions** (`prophet_pred_log`) to produce the final hybrid prediction.\n",
    "- The combined output is stored in:  \n",
    "  `future_df_template['prophet_plus_p_rl_pred_log']`\n",
    "\n",
    "- Any resulting `NaN`s from the lookback period at the beginning are **forward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102a5917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Full Period P_RL Residual Forecasts & Combining with Prophet ---\n",
      "Starting P_RL iterative prediction from index: 40 (date: 2024-05-31 00:00:00)\n",
      "\n",
      "P_RL residual forecasts generated and combined with Prophet.\n",
      "Future DataFrame Template Head with P+P_RL Predictions:\n",
      "          ds  prophet_pred_log  p_rl_predicted_residual_log  \\\n",
      "0 2021-01-31          6.017682                     0.000911   \n",
      "1 2021-02-28          6.130718                     0.026261   \n",
      "2 2021-03-31          6.461549                    -0.025398   \n",
      "3 2021-04-30          6.299305                     0.005143   \n",
      "4 2021-05-31          6.489661                    -0.031322   \n",
      "\n",
      "   prophet_plus_p_rl_pred_log  \n",
      "0                    6.018593  \n",
      "1                    6.156979  \n",
      "2                    6.436150  \n",
      "3                    6.304449  \n",
      "4                    6.458338  \n",
      "\n",
      "Future DataFrame Template Tail with P+P_RL Predictions:\n",
      "           ds  prophet_pred_log  p_rl_predicted_residual_log  \\\n",
      "45 2024-10-31          6.965097                    -0.000241   \n",
      "46 2024-11-30          6.927305                    -0.000230   \n",
      "47 2024-12-31          6.477858                    -0.000303   \n",
      "48 2025-01-31          6.398067                    -0.000257   \n",
      "49 2025-02-28          8.050593                    -0.000389   \n",
      "\n",
      "    prophet_plus_p_rl_pred_log  \n",
      "45                    6.964856  \n",
      "46                    6.927076  \n",
      "47                    6.477555  \n",
      "48                    6.397810  \n",
      "49                    8.050204  \n"
     ]
    }
   ],
   "source": [
    "# --- P_RL Model - Generating Full Period Residual Forecasts and Combining ---\n",
    "print(\"\\n--- Generating Full Period P_RL Residual Forecasts & Combining with Prophet ---\")\n",
    "\n",
    "# We need to create the input sequences for P_RL for the entire future_df_template period.\n",
    "# This involves:\n",
    "# 1. Prophet's predictions ('prophet_pred_log') from future_df_template.\n",
    "# 2. 'avg_package_count' from future_df_template.\n",
    "# 3. For historical periods (before cutoff_date), use actual 'y' to calculate residuals.\n",
    "#    For future periods (after cutoff_date), the \"residual\" is effectively 0 if we assume Prophet's forecast is the base,\n",
    "#    or we can try to predict deviations from Prophet's future forecasts.\n",
    "#    Here, we'll predict corrections to Prophet's 'yhat' for the whole future_df_template.\n",
    "\n",
    "# Create a temporary DataFrame for P_RL inputs, aligned with future_df_template\n",
    "p_rl_input_df = future_df_template[['ds', 'y', 'prophet_pred_log', 'avg_package_count']].copy()\n",
    "\n",
    "# Calculate residuals: actual 'y' - prophet_pred_log where 'y' is available,\n",
    "# otherwise, the residual to predict is against prophet_pred_log (so, if Prophet is perfect, residual is 0)\n",
    "# For the P_RL model, it was trained on residuals = y_actual - prophet_yhat_on_train_data.\n",
    "# When forecasting, we want to predict future residuals: future_y_actual - future_prophet_yhat.\n",
    "# Since future_y_actual is unknown, P_RL predicts the expected error of Prophet.\n",
    "\n",
    "# For the initial lookback period, we need historical residuals.\n",
    "# We use the residuals calculated on Prophet's training data (p_rl_train_source)\n",
    "# and then transition to iterative prediction.\n",
    "\n",
    "# Scale avg_package_count for the entire period using the P_RL scaler\n",
    "p_rl_input_df['avg_package_count_scaled'] = scaler_p_rl_pkg_count.transform(p_rl_input_df[['avg_package_count']])\n",
    "\n",
    "# Initialize a column for scaled residuals in p_rl_input_df\n",
    "p_rl_input_df['residual_scaled_for_pred'] = np.nan\n",
    "\n",
    "# Fill known scaled residuals from the P_RL training phase\n",
    "# Align p_rl_train_source (which has 'residual_scaled') with p_rl_input_df\n",
    "temp_merge_df = pd.merge(p_rl_input_df[['ds']],\n",
    "                         p_rl_train_source[['ds', 'residual_scaled']],\n",
    "                         on='ds',\n",
    "                         how='left')\n",
    "p_rl_input_df['residual_scaled_for_pred'] = temp_merge_df['residual_scaled']\n",
    "\n",
    "\n",
    "# Iteratively predict residuals for dates where 'residual_scaled_for_pred' is NaN\n",
    "# These NaNs will typically start after the P_RL training data ends.\n",
    "\n",
    "all_p_rl_residual_preds_scaled = []\n",
    "# Find the first index where iterative prediction should start\n",
    "# This is typically lookback steps after the start of p_rl_input_df, or after known residuals end.\n",
    "first_pred_idx = lookback \n",
    "\n",
    "# If there are pre-filled residuals from training, start predicting after them.\n",
    "last_known_residual_idx = p_rl_input_df[p_rl_input_df['residual_scaled_for_pred'].notna()].index.max()\n",
    "if pd.notna(last_known_residual_idx) and last_known_residual_idx + 1 > first_pred_idx :\n",
    "    first_pred_idx = last_known_residual_idx + 1\n",
    "    # Populate all_p_rl_residual_preds_scaled with known historical scaled residuals up to this point\n",
    "    all_p_rl_residual_preds_scaled.extend(p_rl_input_df['residual_scaled_for_pred'].iloc[:first_pred_idx].tolist())\n",
    "else: # If no known residuals or they end too early, fill initial part with zeros or a mean\n",
    "    # For simplicity, if we start predicting from 'lookback', the first few values in all_p_rl_residual_preds_scaled\n",
    "    # will be NaNs if not filled from p_rl_train_source.\n",
    "    # Let's ensure the list starts with values that can form the first input sequence.\n",
    "    # We will use the actual known residuals from p_rl_train_source for the initial part of all_p_rl_residual_preds_scaled.\n",
    "    # The loop below will then append new predictions.\n",
    "    \n",
    "    # If first_pred_idx is still 'lookback', it means we don't have enough history from p_rl_train_source\n",
    "    # directly in p_rl_input_df to start iterative prediction immediately.\n",
    "    # This logic assumes p_rl_train_source's residuals are the ground truth for the start.\n",
    "    # We copy them over.\n",
    "    \n",
    "    # The critical part is the input sequence for the *first prediction*.\n",
    "    # This sequence must come from data scaled consistently with P_RL's training.\n",
    "    \n",
    "    # Let's reconstruct the scaled values for the initial sequence from p_rl_input_df\n",
    "    # using data up to the point before predictions begin.\n",
    "    \n",
    "    # The `all_p_rl_residual_preds_scaled` list will store all scaled residuals: known historical + newly predicted.\n",
    "    # It should be populated with all available historical scaled residuals first.\n",
    "    \n",
    "    # Correctly initialize `all_p_rl_residual_preds_scaled` with values from `p_rl_input_df['residual_scaled_for_pred']`\n",
    "    # up to `first_pred_idx -1`. The loop will then predict from `first_pred_idx` onwards.\n",
    "    \n",
    "    # The issue is that `p_rl_input_df['residual_scaled_for_pred']` might have NaNs even before `first_pred_idx`\n",
    "    # if `p_rl_train_source` didn't cover the beginning of `future_df_template`.\n",
    "    # For simplicity, let's assume `p_rl_train_source` provides enough history.\n",
    "    # The `temp_merge_df` step should have populated these.\n",
    "    \n",
    "    # If `all_p_rl_residual_preds_scaled` is shorter than `first_pred_idx`, it means we have a gap.\n",
    "    # This typically happens if `p_rl_train_source` doesn't align with the start of `future_df_template`.\n",
    "    # We will fill initial NaNs in `residual_scaled_for_pred` with 0 for simplicity before iterative loop.\n",
    "    p_rl_input_df['residual_scaled_for_pred'] = p_rl_input_df['residual_scaled_for_pred'].fillna(0) # Risky, but for now.\n",
    "    all_p_rl_residual_preds_scaled = p_rl_input_df['residual_scaled_for_pred'].iloc[:first_pred_idx].tolist()\n",
    "\n",
    "\n",
    "print(f\"Starting P_RL iterative prediction from index: {first_pred_idx} (date: {p_rl_input_df['ds'].iloc[first_pred_idx]})\")\n",
    "\n",
    "for i in range(first_pred_idx, len(p_rl_input_df)):\n",
    "    # Get the last 'lookback' scaled residuals and avg_package_count_scaled\n",
    "    # Residuals for the sequence come from `all_p_rl_residual_preds_scaled` which is being built\n",
    "    current_residuals_scaled_sequence = np.array(all_p_rl_residual_preds_scaled[i-lookback:i])\n",
    "    current_pkg_count_scaled_sequence = p_rl_input_df['avg_package_count_scaled'].iloc[i-lookback:i].values\n",
    "    \n",
    "    # Combine them into the 2-feature input for P_RL LSTM\n",
    "    current_input_sequence_p_rl = np.stack((current_residuals_scaled_sequence, current_pkg_count_scaled_sequence), axis=-1)\n",
    "    current_input_sequence_p_rl = current_input_sequence_p_rl.reshape(1, lookback, 2) # Reshape for LSTM\n",
    "    \n",
    "    # Predict the next scaled residual\n",
    "    pred_scaled_residual = model_p_rl.predict(current_input_sequence_p_rl, verbose=0)[0,0]\n",
    "    all_p_rl_residual_preds_scaled.append(pred_scaled_residual)\n",
    "\n",
    "# Ensure `all_p_rl_residual_preds_scaled` has the same length as `p_rl_input_df`\n",
    "# If it's shorter (e.g. due to loop range), pad with last prediction or zero\n",
    "if len(all_p_rl_residual_preds_scaled) < len(p_rl_input_df):\n",
    "    padding = [all_p_rl_residual_preds_scaled[-1]] * (len(p_rl_input_df) - len(all_p_rl_residual_preds_scaled))\n",
    "    all_p_rl_residual_preds_scaled.extend(padding)\n",
    "elif len(all_p_rl_residual_preds_scaled) > len(p_rl_input_df):\n",
    "    all_p_rl_residual_preds_scaled = all_p_rl_residual_preds_scaled[:len(p_rl_input_df)]\n",
    "\n",
    "\n",
    "# Inverse transform the predicted scaled residuals\n",
    "predicted_residuals_p_rl_log_scale = scaler_p_rl_residual.inverse_transform(np.array(all_p_rl_residual_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Add these predicted residuals to Prophet's main forecast\n",
    "future_df_template['p_rl_predicted_residual_log'] = predicted_residuals_p_rl_log_scale\n",
    "future_df_template['prophet_plus_p_rl_pred_log'] = future_df_template['prophet_pred_log'] + future_df_template['p_rl_predicted_residual_log']\n",
    "\n",
    "# Handle potential NaNs from lookback at the beginning if any (e.g. if iterative prediction didn't cover all)\n",
    "# The `all_p_rl_residual_preds_scaled` should cover the whole range.\n",
    "# NaNs in 'prophet_plus_p_rl_pred_log' would primarily come from NaNs in 'prophet_pred_log' or 'p_rl_predicted_residual_log'.\n",
    "# `future_df_template['prophet_pred_log']` should be full from Prophet.\n",
    "# `p_rl_predicted_residual_log` should be full from the iterative loop.\n",
    "# A ffill can handle any edge cases.\n",
    "future_df_template['prophet_plus_p_rl_pred_log'] = future_df_template['prophet_plus_p_rl_pred_log'].ffill()\n",
    "\n",
    "\n",
    "print(\"\\nP_RL residual forecasts generated and combined with Prophet.\")\n",
    "print(\"Future DataFrame Template Head with P+P_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'p_rl_predicted_residual_log', 'prophet_plus_p_rl_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with P+P_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'p_rl_predicted_residual_log', 'prophet_plus_p_rl_pred_log']].tail())\n",
    "\n",
    "# Clean up temporary columns if desired\n",
    "# future_df_template.drop(columns=['p_rl_predicted_residual_log'], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75685c",
   "metadata": {},
   "source": [
    "## 13. Residual-Correcting LSTM (for Standalone LSTM's Residuals – Model SL_RL) – Data Preparation & Training\n",
    "\n",
    "**Explanation:**  \n",
    "This section implements the second hybrid approach: an **LSTM that models the residuals of the Standalone LSTM (Model SL)**.  \n",
    "This is referred to as **Model SL_RL**. The methodology closely mirrors that of Model P_RL, but instead works with **Model SL's outputs and residuals**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Residual Calculation:\n",
    "\n",
    "- Predictions from the trained **Standalone LSTM (`model_standalone_lstm`)** are made on its **training data (`X_train_standalone_lstm`)**.\n",
    "- These are **scaled predictions**, which are then **inverse-transformed** to recover the **log scale** (`predicted_y_train_sl_log`).\n",
    "- Residuals are calculated as:  \n",
    "  `actual_y_train_standalone_lstm_log - predicted_y_train_sl_log`\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Data Scaling:\n",
    "\n",
    "- The residuals are **scaled** using `StandardScaler` to center them around zero and normalize the variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Sequence Creation:\n",
    "\n",
    "- Input sequences (`X`) are constructed from **lagged, scaled residuals** of Model SL.\n",
    "- Target values (`y`) are the **next scaled residual** in the sequence.\n",
    "- No external regressor (e.g., `avg_package_count`) is used in this model, under the assumption that **Model SL's residuals are primarily autocorrelated** and self-driven.\n",
    "\n",
    "---\n",
    "\n",
    "### ✂️ Train/Validation Split:\n",
    "\n",
    "- The residual sequences are split into **training** and **validation** sets, typically using an 80/20 ratio.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 SL_RL LSTM Definition & Training:\n",
    "\n",
    "- A simple **LSTM model (`model_sl_rl`)** is defined:\n",
    "  - One LSTM layer with **16 units**\n",
    "  - A **Dropout** layer to prevent overfitting\n",
    "  - A **Dense output** layer for residual prediction\n",
    "\n",
    "- The model is compiled with:\n",
    "  - **Loss function**: `'mse'` (Mean Squared Error)\n",
    "  - **Optimizer**: `'adam'`\n",
    "\n",
    "- The model is trained using:\n",
    "  - A **small batch size** (e.g., 2)\n",
    "  - Up to **150 epochs**\n",
    "  - **EarlyStopping** based on validation loss with best weight restoration\n",
    "\n",
    "- The model’s performance is evaluated on the **validation set**, and the **Validation MSE** is reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d070164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Residual-Correcting LSTM (for Standalone LSTM's residuals - Model SL_RL) ---\n",
      "SL_RL LSTM training data shapes: X=(27, 3, 1), y=(27,)\n",
      "SL_RL LSTM validation data shapes: X=(7, 3, 1), y=(7,)\n",
      "Training SL_RL LSTM model (to predict Standalone LSTM residuals)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL_RL LSTM model training complete.\n",
      "SL_RL LSTM Validation MSE (on scaled SL residuals): 0.6074\n"
     ]
    }
   ],
   "source": [
    "# --- Residual-Correcting LSTM (for Standalone LSTM's residuals - Model SL_RL) ---\n",
    "print(\"\\n--- Training Residual-Correcting LSTM (for Standalone LSTM's residuals - Model SL_RL) ---\")\n",
    "\n",
    "# Get Standalone LSTM's predictions on its training data to calculate residuals\n",
    "# These predictions were originally scaled, so inverse transform them first.\n",
    "predicted_y_train_sl_scaled = model_standalone_lstm.predict(X_train_standalone_lstm, verbose=0)\n",
    "predicted_y_train_sl_log = scaler_standalone_lstm_y.inverse_transform(predicted_y_train_sl_scaled).flatten()\n",
    "\n",
    "# actual_y_train_standalone_lstm_log was stored during SL data preparation (Section 8)\n",
    "sl_rl_train_source = pd.DataFrame({\n",
    "    'ds': ds_train_standalone_lstm, # Dates corresponding to SL training targets\n",
    "    'actual_y_log': actual_y_train_standalone_lstm_log,\n",
    "    'predicted_y_sl_log': predicted_y_train_sl_log\n",
    "})\n",
    "sl_rl_train_source['residual_sl'] = sl_rl_train_source['actual_y_log'] - sl_rl_train_source['predicted_y_sl_log']\n",
    "\n",
    "# Scale residuals for SL_RL LSTM\n",
    "scaler_sl_rl_residual = StandardScaler()\n",
    "sl_rl_train_source['residual_sl_scaled'] = scaler_sl_rl_residual.fit_transform(sl_rl_train_source[['residual_sl']])\n",
    "\n",
    "# Create sequences for SL_RL LSTM (only using lagged scaled residuals of SL)\n",
    "X_sl_rl, y_sl_rl_scaled = [], []\n",
    "ds_for_y_sl_rl = []\n",
    "\n",
    "for i in range(lookback, len(sl_rl_train_source)):\n",
    "    # Feature: lagged scaled residual from Standalone LSTM\n",
    "    sequence = sl_rl_train_source['residual_sl_scaled'].iloc[i-lookback:i].values.reshape(-1, 1) # Reshape for 1 feature\n",
    "    X_sl_rl.append(sequence)\n",
    "    y_sl_rl_scaled.append(sl_rl_train_source['residual_sl_scaled'].iloc[i])\n",
    "    ds_for_y_sl_rl.append(sl_rl_train_source['ds'].iloc[i])\n",
    "\n",
    "X_sl_rl, y_sl_rl_scaled = np.array(X_sl_rl), np.array(y_sl_rl_scaled)\n",
    "ds_for_y_sl_rl = pd.Series(ds_for_y_sl_rl, name='ds')\n",
    "\n",
    "# Reshape X_sl_rl for LSTM: [samples, timesteps, features=1]\n",
    "X_sl_rl = X_sl_rl.reshape((X_sl_rl.shape[0], X_sl_rl.shape[1], 1))\n",
    "\n",
    "# Train/Validation split for SL_RL LSTM\n",
    "validation_split_sl_rl = int(len(X_sl_rl) * 0.8)\n",
    "X_train_sl_rl, X_val_sl_rl = X_sl_rl[:validation_split_sl_rl], X_sl_rl[validation_split_sl_rl:]\n",
    "y_train_sl_rl_scaled, y_val_sl_rl_scaled = y_sl_rl_scaled[:validation_split_sl_rl], y_sl_rl_scaled[validation_split_sl_rl:]\n",
    "\n",
    "print(f\"SL_RL LSTM training data shapes: X={X_train_sl_rl.shape}, y={y_train_sl_rl_scaled.shape}\")\n",
    "print(f\"SL_RL LSTM validation data shapes: X={X_val_sl_rl.shape}, y={y_val_sl_rl_scaled.shape}\")\n",
    "\n",
    "# Define and train the SL_RL LSTM model\n",
    "model_sl_rl = Sequential()\n",
    "model_sl_rl.add(LSTM(16, input_shape=(lookback, 1), kernel_regularizer=l2(0.001))) # 1 feature: SL residual\n",
    "model_sl_rl.add(Dropout(0.2))\n",
    "model_sl_rl.add(Dense(1)) # Output layer for predicting the scaled SL residual\n",
    "model_sl_rl.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training SL_RL LSTM model (to predict Standalone LSTM residuals)...\")\n",
    "sl_rl_early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "history_sl_rl = model_sl_rl.fit(X_train_sl_rl, y_train_sl_rl_scaled,\n",
    "                              epochs=150,\n",
    "                              batch_size=2,\n",
    "                              validation_data=(X_val_sl_rl, y_val_sl_rl_scaled),\n",
    "                              verbose=0,\n",
    "                              callbacks=[sl_rl_early_stopping])\n",
    "print(\"SL_RL LSTM model training complete.\")\n",
    "\n",
    "val_loss_sl_rl = model_sl_rl.evaluate(X_val_sl_rl, y_val_sl_rl_scaled, verbose=0)\n",
    "print(f\"SL_RL LSTM Validation MSE (on scaled SL residuals): {val_loss_sl_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a6bd1",
   "metadata": {},
   "source": [
    "## 14. SL_RL Model – Generating Full Period Residual Forecasts and Combining with Standalone LSTM\n",
    "\n",
    "**Explanation:**  \n",
    "This block uses the trained **SL_RL LSTM** to forecast the **Standalone LSTM (Model SL)** residuals over the full forecast horizon defined in `future_df_template`.  \n",
    "The predicted residuals are added to Model SL’s main predictions (`standalone_lstm_pred_log`) to generate the **hybrid SL + SL_RL forecast**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Prepare Input Data for SL_RL Forecasting:\n",
    "\n",
    "- `future_df_template` already contains:\n",
    "  - Predictions from Model SL (`standalone_lstm_pred_log`)\n",
    "\n",
    "- For dates **up to `cutoff_date`**:\n",
    "  - \"Actual\" `y` values are available and are used to calculate historical **residuals** for the SL_RL model’s lookback window.\n",
    "\n",
    "- For dates **after `cutoff_date`**:\n",
    "  - Model SL’s predictions are used as the base.\n",
    "  - SL_RL attempts to correct their residual errors.\n",
    "\n",
    "- These residuals are **scaled** using `scaler_sl_rl_residual`, which was fit during SL_RL training.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Iterative Residual Forecasting:\n",
    "\n",
    "- The **SL_RL LSTM** predicts one **scaled residual** at a time.\n",
    "- After each prediction:\n",
    "  - The result is used to **update the input sequence**.\n",
    "  - The process continues until the end of the forecast horizon.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Inverse Transform and Combine:\n",
    "\n",
    "- Forecasted residuals (in scaled form) are **inverse-transformed** to their original scale.\n",
    "- These are then **added to the log-transformed predictions** from Model SL (`standalone_lstm_pred_log`).\n",
    "- The combined hybrid output is stored in:  \n",
    "  `future_df_template['standalone_lstm_plus_sl_rl_pred_log']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374d1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Full Period SL_RL Residual Forecasts & Combining with Standalone LSTM ---\n",
      "Starting SL_RL iterative prediction from index: 40 (date: 2024-05-31 00:00:00)\n",
      "\n",
      "SL_RL residual forecasts generated and combined with Standalone LSTM.\n",
      "Future DataFrame Template Head with SL+SL_RL Predictions:\n",
      "          ds  standalone_lstm_pred_log  sl_rl_predicted_residual_log  \\\n",
      "0 2021-01-31                       NaN                           NaN   \n",
      "1 2021-02-28                       NaN                           NaN   \n",
      "2 2021-03-31                       NaN                           NaN   \n",
      "3 2021-04-30                  6.304449                     -0.144849   \n",
      "4 2021-05-31                  6.458338                     -0.031792   \n",
      "\n",
      "   standalone_lstm_plus_sl_rl_pred_log  \n",
      "0                                  NaN  \n",
      "1                                  NaN  \n",
      "2                                  NaN  \n",
      "3                             6.159600  \n",
      "4                             6.426546  \n",
      "\n",
      "Future DataFrame Template Tail with SL+SL_RL Predictions:\n",
      "           ds  standalone_lstm_pred_log  sl_rl_predicted_residual_log  \\\n",
      "45 2024-10-31                  6.720220                      0.041189   \n",
      "46 2024-11-30                  6.714398                      0.041108   \n",
      "47 2024-12-31                  6.710331                      0.041057   \n",
      "48 2025-01-31                  6.707211                      0.041020   \n",
      "49 2025-02-28                  6.704997                      0.041018   \n",
      "\n",
      "    standalone_lstm_plus_sl_rl_pred_log  \n",
      "45                             6.761410  \n",
      "46                             6.755506  \n",
      "47                             6.751388  \n",
      "48                             6.748231  \n",
      "49                             6.746016  \n"
     ]
    }
   ],
   "source": [
    "# --- SL_RL Model - Generating Full Period Residual Forecasts & Combining ---\n",
    "print(\"\\n--- Generating Full Period SL_RL Residual Forecasts & Combining with Standalone LSTM ---\")\n",
    "\n",
    "# We need input sequences for SL_RL for the entire future_df_template period.\n",
    "# This uses 'standalone_lstm_pred_log' as the base.\n",
    "sl_rl_input_df = future_df_template[['ds', 'y', 'standalone_lstm_pred_log']].copy()\n",
    "sl_rl_input_df.rename(columns={'standalone_lstm_pred_log': 'base_pred_log'}, inplace=True)\n",
    "\n",
    "# Calculate residuals: actual 'y' - base_pred_log where 'y' is available.\n",
    "# For the training part of SL_RL, we use the residuals from sl_rl_train_source.\n",
    "# For the forecasting part, SL_RL predicts corrections to 'base_pred_log'.\n",
    "sl_rl_input_df['residual_for_pred'] = sl_rl_input_df['y'] - sl_rl_input_df['base_pred_log']\n",
    "\n",
    "# Scale these residuals using the SL_RL scaler.\n",
    "# Note: transform only, as fit was done on training residuals.\n",
    "# We need to handle NaNs if 'y' is not available for the full future_df_template range.\n",
    "# For forecasting, the \"residual\" is what SL_RL predicts as deviation from standalone_lstm_pred_log.\n",
    "\n",
    "# Initialize a column for scaled residuals in sl_rl_input_df\n",
    "sl_rl_input_df['residual_sl_scaled_for_pred'] = np.nan\n",
    "\n",
    "# Fill known scaled residuals from the SL_RL training phase\n",
    "temp_merge_sl_rl_df = pd.merge(sl_rl_input_df[['ds']],\n",
    "                               sl_rl_train_source[['ds', 'residual_sl_scaled']],\n",
    "                               on='ds',\n",
    "                               how='left')\n",
    "sl_rl_input_df['residual_sl_scaled_for_pred'] = temp_merge_sl_rl_df['residual_sl_scaled']\n",
    "\n",
    "# Iteratively predict SL residuals\n",
    "all_sl_rl_residual_preds_scaled = []\n",
    "first_pred_idx_sl_rl = lookback\n",
    "\n",
    "last_known_sl_rl_residual_idx = sl_rl_input_df[sl_rl_input_df['residual_sl_scaled_for_pred'].notna()].index.max()\n",
    "\n",
    "if pd.notna(last_known_sl_rl_residual_idx) and last_known_sl_rl_residual_idx + 1 > first_pred_idx_sl_rl:\n",
    "    first_pred_idx_sl_rl = last_known_sl_rl_residual_idx + 1\n",
    "    all_sl_rl_residual_preds_scaled.extend(sl_rl_input_df['residual_sl_scaled_for_pred'].iloc[:first_pred_idx_sl_rl].tolist())\n",
    "else:\n",
    "    # Fill initial part with zeros if no history or not enough. This assumes residuals start around zero.\n",
    "    sl_rl_input_df['residual_sl_scaled_for_pred'] = sl_rl_input_df['residual_sl_scaled_for_pred'].fillna(0)\n",
    "    all_sl_rl_residual_preds_scaled = sl_rl_input_df['residual_sl_scaled_for_pred'].iloc[:first_pred_idx_sl_rl].tolist()\n",
    "\n",
    "print(f\"Starting SL_RL iterative prediction from index: {first_pred_idx_sl_rl} (date: {sl_rl_input_df['ds'].iloc[first_pred_idx_sl_rl]})\")\n",
    "\n",
    "for i in range(first_pred_idx_sl_rl, len(sl_rl_input_df)):\n",
    "    current_residuals_sl_scaled_sequence = np.array(all_sl_rl_residual_preds_scaled[i-lookback:i])\n",
    "    current_input_sequence_sl_rl = current_residuals_sl_scaled_sequence.reshape(1, lookback, 1) # 1 feature\n",
    "    \n",
    "    pred_scaled_sl_residual = model_sl_rl.predict(current_input_sequence_sl_rl, verbose=0)[0,0]\n",
    "    all_sl_rl_residual_preds_scaled.append(pred_scaled_sl_residual)\n",
    "\n",
    "# Ensure lengths match\n",
    "if len(all_sl_rl_residual_preds_scaled) < len(sl_rl_input_df):\n",
    "    padding_sl_rl = [all_sl_rl_residual_preds_scaled[-1]] * (len(sl_rl_input_df) - len(all_sl_rl_residual_preds_scaled))\n",
    "    all_sl_rl_residual_preds_scaled.extend(padding_sl_rl)\n",
    "elif len(all_sl_rl_residual_preds_scaled) > len(sl_rl_input_df):\n",
    "    all_sl_rl_residual_preds_scaled = all_sl_rl_residual_preds_scaled[:len(sl_rl_input_df)]\n",
    "\n",
    "# Inverse transform the predicted scaled SL residuals\n",
    "predicted_residuals_sl_rl_log_scale = scaler_sl_rl_residual.inverse_transform(np.array(all_sl_rl_residual_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Add these predicted residuals to Standalone LSTM's main forecast\n",
    "future_df_template['sl_rl_predicted_residual_log'] = predicted_residuals_sl_rl_log_scale\n",
    "future_df_template['standalone_lstm_plus_sl_rl_pred_log'] = future_df_template['standalone_lstm_pred_log'] + future_df_template['sl_rl_predicted_residual_log']\n",
    "future_df_template['standalone_lstm_plus_sl_rl_pred_log'] = future_df_template['standalone_lstm_plus_sl_rl_pred_log'].ffill()\n",
    "\n",
    "print(\"\\nSL_RL residual forecasts generated and combined with Standalone LSTM.\")\n",
    "print(\"Future DataFrame Template Head with SL+SL_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log', 'standalone_lstm_plus_sl_rl_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with SL+SL_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log', 'standalone_lstm_plus_sl_rl_pred_log']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c5ad4",
   "metadata": {},
   "source": [
    "## 15. Bayesian Model Averaging (BMA)\n",
    "\n",
    "**Explanation:**  \n",
    "**Bayesian Model Averaging (BMA)** is a technique for combining forecasts from multiple models.  \n",
    "Rather than selecting a single \"best\" model, BMA creates a **weighted average forecast**, where each model’s contribution is proportional to its **reliability or past performance**.\n",
    "\n",
    "In this demonstration, we’ll use **simple, pre-defined weights** for combining model outputs.  \n",
    "> *Note: In a production setup, these weights can and should be optimized — for example, using validation-set error metrics like MAE, RMSE, or log-likelihood.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Models Included in the Averaging:\n",
    "\n",
    "- **Prophet (Model P)**  \n",
    "  Forecast stored in: `prophet_pred_log`\n",
    "\n",
    "- **Standalone LSTM (Model SL)**  \n",
    "  Forecast stored in: `standalone_lstm_pred_log`\n",
    "\n",
    "- **Prophet + P_RL Hybrid Model**  \n",
    "  Forecast stored in: `prophet_plus_p_rl_pred_log`\n",
    "\n",
    "- **Standalone LSTM + SL_RL Hybrid Model**  \n",
    "  Forecast stored in: `standalone_lstm_plus_sl_rl_pred_log`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Averaging Logic:\n",
    "\n",
    "- All model outputs are in the **log-transformed space**.\n",
    "- BMA produces a final forecast as a **weighted sum of log-scale predictions**.\n",
    "- Each model can be assigned a weight (e.g., 0.25 each for equal weighting).\n",
    "- The result reflects the **combined strength** of different modeling strategies — both classical (Prophet) and neural (LSTM), standalone and hybrid.\n",
    "\n",
    "---\n",
    "\n",
    "### ℹ️ Note:\n",
    "\n",
    "- The `avg_package_count` regressor is **implicitly included** in models based on Prophet, and partially in hybrid versions as well.\n",
    "- BMA doesn’t re-use this regressor directly but benefits from models that already incorporate it during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea11ace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Bayesian Model Averaging ---\n",
      "Warning: NaNs found in standalone_lstm_pred_log. Forward-filling then backward-filling.\n",
      "Warning: NaNs found in standalone_lstm_plus_sl_rl_pred_log. Forward-filling then backward-filling.\n",
      "BMA log predictions calculated.\n",
      "Future DataFrame Template Head with BMA Predictions:\n",
      "          ds  prophet_pred_log  standalone_lstm_pred_log  \\\n",
      "0 2021-01-31          6.017682                  6.304449   \n",
      "1 2021-02-28          6.130718                  6.304449   \n",
      "2 2021-03-31          6.461549                  6.304449   \n",
      "3 2021-04-30          6.299305                  6.304449   \n",
      "4 2021-05-31          6.489661                  6.458338   \n",
      "\n",
      "   prophet_plus_p_rl_pred_log  standalone_lstm_plus_sl_rl_pred_log  \\\n",
      "0                    6.018593                             6.159600   \n",
      "1                    6.156979                             6.159600   \n",
      "2                    6.436150                             6.159600   \n",
      "3                    6.304449                             6.159600   \n",
      "4                    6.458338                             6.426546   \n",
      "\n",
      "   bma_pred_log  \n",
      "0      6.110687  \n",
      "1      6.176078  \n",
      "2      6.323412  \n",
      "3      6.252980  \n",
      "4      6.451909  \n",
      "\n",
      "Future DataFrame Template Tail with BMA Predictions:\n",
      "           ds  prophet_pred_log  standalone_lstm_pred_log  \\\n",
      "45 2024-10-31          6.965097                  6.720220   \n",
      "46 2024-11-30          6.927305                  6.714398   \n",
      "47 2024-12-31          6.477858                  6.710331   \n",
      "48 2025-01-31          6.398067                  6.707211   \n",
      "49 2025-02-28          8.050593                  6.704997   \n",
      "\n",
      "    prophet_plus_p_rl_pred_log  standalone_lstm_plus_sl_rl_pred_log  \\\n",
      "45                    6.964856                             6.761410   \n",
      "46                    6.927076                             6.755506   \n",
      "47                    6.477555                             6.751388   \n",
      "48                    6.397810                             6.748231   \n",
      "49                    8.050204                             6.746016   \n",
      "\n",
      "    bma_pred_log  \n",
      "45      6.856991  \n",
      "46      6.835159  \n",
      "47      6.608359  \n",
      "48      6.566906  \n",
      "49      7.392015  \n"
     ]
    }
   ],
   "source": [
    "# --- Bayesian Model Averaging (BMA) ---\n",
    "print(\"\\n--- Performing Bayesian Model Averaging ---\")\n",
    "\n",
    "# Define weights for each model (these are illustrative and can be optimized)\n",
    "# For simplicity, let's give equal weight to the two hybrid models,\n",
    "# and perhaps less to the standalone ones if hybrids are expected to be better.\n",
    "# Example weights:\n",
    "weights = {\n",
    "    'prophet': 0.15,\n",
    "    'standalone_lstm': 0.15,\n",
    "    'prophet_plus_p_rl': 0.35,\n",
    "    'standalone_lstm_plus_sl_rl': 0.35\n",
    "}\n",
    "\n",
    "# Ensure weights sum to 1 (or normalize them)\n",
    "total_weight = sum(weights.values())\n",
    "if not np.isclose(total_weight, 1.0):\n",
    "    print(f\"Warning: Weights do not sum to 1 (sum={total_weight}). Normalizing weights.\")\n",
    "    weights = {model: wt / total_weight for model, wt in weights.items()}\n",
    "\n",
    "# Check if all necessary prediction columns exist\n",
    "required_cols_for_bma = ['prophet_pred_log', 'standalone_lstm_pred_log', \n",
    "                         'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log']\n",
    "missing_cols = [col for col in required_cols_for_bma if col not in future_df_template.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns for BMA in future_df_template: {missing_cols}\")\n",
    "\n",
    "# Fill NaNs in prediction columns with a fallback (e.g., ffill then bfill, or with Prophet's prediction)\n",
    "# This is crucial as NaNs in any component will result in NaN for BMA.\n",
    "for col in required_cols_for_bma:\n",
    "    if future_df_template[col].isnull().any():\n",
    "        print(f\"Warning: NaNs found in {col}. Forward-filling then backward-filling.\")\n",
    "        future_df_template[col] = future_df_template[col].ffill().bfill()\n",
    "    # As a final fallback, if still NaNs (e.g. entire column was NaN for some reason)\n",
    "    if future_df_template[col].isnull().any():\n",
    "         future_df_template[col] = future_df_template['prophet_pred_log'] # Fallback to Prophet\n",
    "         print(f\"Warning: NaNs persisted in {col} after ffill/bfill. Filled with Prophet predictions.\")\n",
    "\n",
    "\n",
    "# Calculate the BMA forecast (weighted average of log-transformed predictions)\n",
    "future_df_template['bma_pred_log'] = (\n",
    "    weights['prophet'] * future_df_template['prophet_pred_log'] +\n",
    "    weights['standalone_lstm'] * future_df_template['standalone_lstm_pred_log'] +\n",
    "    weights['prophet_plus_p_rl'] * future_df_template['prophet_plus_p_rl_pred_log'] +\n",
    "    weights['standalone_lstm_plus_sl_rl'] * future_df_template['standalone_lstm_plus_sl_rl_pred_log']\n",
    ")\n",
    "\n",
    "print(\"BMA log predictions calculated.\")\n",
    "print(\"Future DataFrame Template Head with BMA Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log', 'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log', 'bma_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with BMA Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log', 'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log', 'bma_pred_log']].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed535db6",
   "metadata": {},
   "source": [
    "## 16. Inverse Transform Predictions\n",
    "\n",
    "**Explanation:**  \n",
    "All model predictions are currently in the **log-transformed scale** because of the `np.log1p` transformation applied to the target variable `y` (see Section 3).  \n",
    "To make these predictions interpretable and comparable to the **original submission volumes**, we need to **inverse transform** them using `np.expm1`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Predictions to Inverse Transform:\n",
    "\n",
    "- `prophet_pred_log`\n",
    "- `standalone_lstm_pred_log`\n",
    "- `prophet_plus_p_rl_pred_log`\n",
    "- `standalone_lstm_plus_sl_rl_pred_log`\n",
    "- `bma_pred_log` (Bayesian Model Averaged output)\n",
    "\n",
    "Each of these columns will be inverse transformed using:\n",
    "\n",
    "```python\n",
    "np.expm1(predicted_log_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f1e75d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inverse Transforming Predictions to Original Scale ---\n",
      "Predictions inverse transformed to original scale.\n",
      "Future DataFrame Template Head with Original Scale Predictions:\n",
      "          ds  y_original  prophet_pred_orig  standalone_lstm_pred_orig  \\\n",
      "0 2021-01-31         410         409.625842                      546.0   \n",
      "1 2021-02-28         471         458.766346                      546.0   \n",
      "2 2021-03-31         623         639.051552                      546.0   \n",
      "3 2021-04-30         546         543.193827                      546.0   \n",
      "4 2021-05-31         637         657.300012                      637.0   \n",
      "\n",
      "   prophet_plus_p_rl_pred_orig  standalone_lstm_plus_sl_rl_pred_orig  \\\n",
      "0                        410.0                            472.238599   \n",
      "1                        471.0                            472.238599   \n",
      "2                        623.0                            472.238599   \n",
      "3                        546.0                            472.238599   \n",
      "4                        637.0                            617.035650   \n",
      "\n",
      "   bma_pred_orig  \n",
      "0     449.648297  \n",
      "1     480.101190  \n",
      "2     556.471930  \n",
      "3     518.558867  \n",
      "4     632.911533  \n",
      "\n",
      "Future DataFrame Template Tail with Original Scale Predictions:\n",
      "           ds  y_original  prophet_pred_orig  standalone_lstm_pred_orig  \\\n",
      "45 2024-10-31        1131        1058.017761                 827.999968   \n",
      "46 2024-11-30         957        1018.742493                 823.187721   \n",
      "47 2024-12-31         833         649.575814                 819.842296   \n",
      "48 2025-01-31         939         599.682525                 817.285552   \n",
      "49 2025-02-28         858        3134.653379                 815.475685   \n",
      "\n",
      "    prophet_plus_p_rl_pred_orig  standalone_lstm_plus_sl_rl_pred_orig  \\\n",
      "45                  1057.762925                            862.859013   \n",
      "46                  1018.508397                            857.774237   \n",
      "47                   649.378828                            854.245414   \n",
      "48                   599.528299                            851.549512   \n",
      "49                  3133.433535                            849.662550   \n",
      "\n",
      "    bma_pred_orig  \n",
      "45     949.502408  \n",
      "46     928.976413  \n",
      "47     740.265262  \n",
      "48     710.166127  \n",
      "49    1621.973498  \n"
     ]
    }
   ],
   "source": [
    "# --- Inverse Transform Predictions ---\n",
    "print(\"\\n--- Inverse Transforming Predictions to Original Scale ---\")\n",
    "\n",
    "# Merge original 'y' values for comparison\n",
    "future_df_template = pd.merge(future_df_template, monthly_df[['ds', 'y_original']], on='ds', how='left')\n",
    "\n",
    "# List of prediction columns (log scale)\n",
    "log_pred_columns = [\n",
    "    'prophet_pred_log',\n",
    "    'standalone_lstm_pred_log',\n",
    "    'prophet_plus_p_rl_pred_log',\n",
    "    'standalone_lstm_plus_sl_rl_pred_log',\n",
    "    'bma_pred_log'\n",
    "]\n",
    "\n",
    "# Inverse transform each prediction column\n",
    "for log_col in log_pred_columns:\n",
    "    original_scale_col = log_col.replace('_log', '_orig')\n",
    "    # Ensure the column exists before trying to transform\n",
    "    if log_col in future_df_template.columns:\n",
    "        future_df_template[original_scale_col] = np.expm1(future_df_template[log_col])\n",
    "        # Handle potential negative predictions if models behave unexpectedly (np.expm1 can handle negatives, but resulting values might be < 0)\n",
    "        # For submission counts, predictions should ideally be non-negative.\n",
    "        future_df_template[original_scale_col] = np.maximum(0, future_df_template[original_scale_col]) \n",
    "    else:\n",
    "        print(f\"Warning: Log prediction column {log_col} not found for inverse transformation.\")\n",
    "\n",
    "print(\"Predictions inverse transformed to original scale.\")\n",
    "print(\"Future DataFrame Template Head with Original Scale Predictions:\")\n",
    "cols_to_show = ['ds', 'y_original'] + [col.replace('_log', '_orig') for col in log_pred_columns if col.replace('_log', '_orig') in future_df_template.columns]\n",
    "print(future_df_template[cols_to_show].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with Original Scale Predictions:\")\n",
    "print(future_df_template[cols_to_show].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1a43c",
   "metadata": {},
   "source": [
    "## 17. Evaluation\n",
    "\n",
    "**Explanation:**  \n",
    "In this final step, we evaluate the performance of **all forecasting models** on the **original (non-log-transformed) scale**.  \n",
    "Two key evaluation metrics are used:\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "- **Mean Absolute Error (MAE)**\n",
    "\n",
    "---\n",
    "\n",
    "### 📅 Evaluation Period:\n",
    "\n",
    "- The evaluation is performed over a **common time range**, defined by:\n",
    "  - `eval_start_common` → typically equal to `cutoff_date`\n",
    "  - `eval_end_common` → end of the forecast horizon\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Evaluation Workflow:\n",
    "\n",
    "1. A new `DataFrame` called `evaluation_df` is created.\n",
    "   - It includes:\n",
    "     - `ds` (date column)\n",
    "     - `y_original` (true values on original scale)\n",
    "     - All **model predictions** after inverse transformation\n",
    "\n",
    "2. For each model:\n",
    "   - RMSE is calculated:\n",
    "     ```python\n",
    "     sqrt(mean_squared_error(actual, predicted))\n",
    "     ```\n",
    "   - MAE is calculated:\n",
    "     ```python\n",
    "     mean_absolute_error(actual, predicted)\n",
    "     ```\n",
    "\n",
    "3. The results for each model are stored in a **summary table** (e.g., `metrics_summary_df`) for easy **side-by-side comparison**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Metrics Summary Output:\n",
    "The summary `DataFrame` might look like this:\n",
    "\n",
    "| Model                    | RMSE    | MAE     |\n",
    "|-------------------------|---------|---------|\n",
    "| Prophet (P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c714015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model Performance on Original Scale ---\n",
      "Prophet (P) - RMSE: 736.9033, MAE: 349.4151\n",
      "Standalone LSTM (SL) - RMSE: 218.4711, MAE: 186.8071\n",
      "Prophet + P_RL - RMSE: 736.4803, MAE: 348.9241\n",
      "SL + SL_RL - RMSE: 189.1734, MAE: 155.2678\n",
      "BMA - RMSE: 276.1177, MAE: 181.9747\n",
      "\n",
      "--- Model Evaluation Summary (Original Scale) ---\n",
      "Evaluation Period: 2024-05-31 to 2025-02-28\n",
      "                  Model        RMSE         MAE\n",
      "3            SL + SL_RL  189.173413  155.267819\n",
      "1  Standalone LSTM (SL)  218.471098  186.807098\n",
      "4                   BMA  276.117732  181.974709\n",
      "2        Prophet + P_RL  736.480297  348.924056\n",
      "0           Prophet (P)  736.903326  349.415085\n",
      "\n",
      "Final Predictions Table (Original Scale - Last 12 Months of Future DF or Full Eval Period):\n",
      "           ds  y_original\n",
      "40 2024-05-31        1028\n",
      "41 2024-06-30        1039\n",
      "42 2024-07-31        1136\n",
      "43 2024-08-31        1180\n",
      "44 2024-09-30        1173\n",
      "45 2024-10-31        1131\n",
      "46 2024-11-30         957\n",
      "47 2024-12-31         833\n",
      "48 2025-01-31         939\n",
      "49 2025-02-28         858\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation ---\n",
    "print(\"\\n--- Evaluating Model Performance on Original Scale ---\")\n",
    "\n",
    "# Define the common evaluation period (already defined as eval_start_common and eval_end_common)\n",
    "evaluation_period_df = future_df_template[\n",
    "    (future_df_template['ds'] >= eval_start_common) &\n",
    "    (future_df_template['ds'] <= eval_end_common)\n",
    "].copy()\n",
    "\n",
    "# Ensure 'y_original' is present for evaluation\n",
    "if 'y_original' not in evaluation_period_df.columns or evaluation_period_df['y_original'].isnull().all():\n",
    "    raise ValueError(\"'y_original' is missing or all NaNs in the evaluation period. Cannot evaluate.\")\n",
    "\n",
    "# Drop rows where y_original is NaN for fair comparison (if any future dates had no actuals)\n",
    "evaluation_period_df.dropna(subset=['y_original'], inplace=True)\n",
    "\n",
    "if evaluation_period_df.empty:\n",
    "    print(\"Warning: Evaluation period is empty after dropping NaNs in y_original. No evaluation possible.\")\n",
    "    results_summary = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "else:\n",
    "    models_to_evaluate = {\n",
    "        'Prophet (P)': 'prophet_pred_orig',\n",
    "        'Standalone LSTM (SL)': 'standalone_lstm_pred_orig',\n",
    "        'Prophet + P_RL': 'prophet_plus_p_rl_pred_orig',\n",
    "        'SL + SL_RL': 'standalone_lstm_plus_sl_rl_pred_orig',\n",
    "        'BMA': 'bma_pred_orig'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name, pred_col_orig in models_to_evaluate.items():\n",
    "        if pred_col_orig in evaluation_period_df.columns and not evaluation_period_df[pred_col_orig].isnull().all():\n",
    "            actuals = evaluation_period_df['y_original']\n",
    "            predictions = evaluation_period_df[pred_col_orig]\n",
    "            \n",
    "            # Ensure no NaNs in predictions for the slice being evaluated\n",
    "            valid_indices = actuals.notna() & predictions.notna()\n",
    "            if not valid_indices.any():\n",
    "                print(f\"Warning: No valid (non-NaN) actuals or predictions for {model_name} in evaluation period.\")\n",
    "                rmse = np.nan\n",
    "                mae = np.nan\n",
    "            else:\n",
    "                rmse = np.sqrt(mean_squared_error(actuals[valid_indices], predictions[valid_indices]))\n",
    "                mae = mean_absolute_error(actuals[valid_indices], predictions[valid_indices])\n",
    "            \n",
    "            results.append({'Model': model_name, 'RMSE': rmse, 'MAE': mae})\n",
    "            print(f\"{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Prediction column {pred_col_orig} for model {model_name} not found or all NaNs in evaluation data. Skipping.\")\n",
    "            results.append({'Model': model_name, 'RMSE': np.nan, 'MAE': np.nan})\n",
    "\n",
    "    results_summary = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n--- Model Evaluation Summary (Original Scale) ---\")\n",
    "print(f\"Evaluation Period: {evaluation_period_df['ds'].min().date()} to {evaluation_period_df['ds'].max().date()}\")\n",
    "if results_summary.empty:\n",
    "    print(\"No models were evaluated.\")\n",
    "else:\n",
    "    print(results_summary.sort_values(by='RMSE'))\n",
    "\n",
    "# Display the final few predictions for context\n",
    "print(\"\\nFinal Predictions Table (Original Scale - Last 12 Months of Future DF or Full Eval Period):\")\n",
    "display_df_end_limit = min(len(future_df_template), 12) \n",
    "if not evaluation_period_df.empty:\n",
    "    display_df = evaluation_period_df.tail(display_df_end_limit)\n",
    "else:\n",
    "    display_df = future_df_template.tail(display_df_end_limit)\n",
    "    \n",
    "cols_to_display_final = ['ds', 'y_original'] + [m[1] for m in models_to_evaluate.values() if m[1] in display_df.columns]\n",
    "print(display_df[cols_to_display_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e0a14",
   "metadata": {},
   "source": [
    "## 18. Conclusion and Model Selection\n",
    "\n",
    "**Explanation:**  \n",
    "Based on the evaluation metrics and model outputs, we can now draw final conclusions and make informed recommendations about model selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Results Summary (User-Provided Metrics):\n",
    "\n",
    "#### ✅ Prophet Model Only (P):\n",
    "- **RMSE**: 736.90  \n",
    "- **MAE**: 349.42\n",
    "\n",
    "#### ✅ Standalone LSTM Model Only (SL):\n",
    "- **RMSE**: 191.33  \n",
    "- **MAE**: 158.62\n",
    "\n",
    "#### ✅ Combined Model (Prophet + Residual LSTM - P + P_RL):  \n",
    "(*`prophet_plus_p_rl_pred_orig` in notebook*)\n",
    "- **RMSE**: 736.90  \n",
    "- **MAE**: 349.42\n",
    "\n",
    "#### ✅ Combined Model (Standalone LSTM + Residual LSTM via BMA - SL + SL_RL):  \n",
    "(*`standalone_lstm_plus_sl_rl_pred_orig`, user's best BMA-weighted hybrid*)\n",
    "- **RMSE**: 173.46  \n",
    "- **MAE**: 144.82  \n",
    "- **BMA Weights Used**: `SL = 0.9097`, `SL+RP = 0.0903`\n",
    "\n",
    "#### ✅ BMA (Notebook's Demonstration Weights):\n",
    "- **RMSE**: *(See `results_summary` output in Section 17)*  \n",
    "- **MAE**: *(See `results_summary` output in Section 17)*\n",
    "\n",
    "> *Note: Validation metrics for `P_RL` (e.g., RMSE: 0.0109, MAE: 0.0088) are from log-scale residuals and not directly comparable to original-scale forecast evaluations.*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Analysis:\n",
    "\n",
    "- **Standalone LSTM (SL)** clearly outperforms **Prophet**, with a significantly lower RMSE.\n",
    "- **Hybrid SL + SL_RL** (Standalone LSTM plus its residual-correcting LSTM) further improves performance.\n",
    "- The **BMA ensemble**, when **heavily weighted toward SL and SL_RL**, produces the **best overall result**.\n",
    "- **Hybrid Prophet + P_RL** offers **no improvement** over Prophet alone, indicating LSTM residual correction may not effectively address Prophet's forecasting errors in this case.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Recommendations:\n",
    "\n",
    "#### ✅ **Primary Model Candidate:**\n",
    "- **Best Performing Model**:  \n",
    "  **Combined SL + SL_RL**, with an RMSE of 173.46 and MAE of 144.82.\n",
    "- **Corresponds to**:  \n",
    "  `standalone_lstm_plus_sl_rl_pred_orig` in the notebook.\n",
    "- **Supported by BMA Weights**:  \n",
    "  `SL = 0.9097`, `SL+RP = 0.0903`\n",
    "\n",
    "#### ⚙️ **Optimize BMA Weights**:\n",
    "- The BMA implementation currently uses **illustrative weights**.\n",
    "- Future versions should optimize these weights using:\n",
    "  - Validation set performance\n",
    "  - Bayesian techniques\n",
    "  - Grid search or optimization libraries\n",
    "\n",
    "#### 🔍 **Code and Pipeline Review**:\n",
    "- Ensure **scaling/transformations** are applied consistently (fit on training only, transform on test).\n",
    "- Validate **input alignment** across all models.\n",
    "- Review external scripts or reporting tools for **typos** (e.g., `bма_weight_P`) in model identifiers or metric labels.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ General Summary:\n",
    "\n",
    "This notebook demonstrates a **robust, multi-model time series forecasting pipeline** that compares:\n",
    "\n",
    "- Classical statistical methods (Prophet)\n",
    "- Neural forecasting (LSTM)\n",
    "- Residual-correcting hybrids (P_RL, SL_RL)\n",
    "- Ensemble methods (BMA)\n",
    "\n",
    "The key takeaway is the **effectiveness of LSTM models**, particularly when enhanced with **residual modeling and ensemble averaging**.\n",
    "\n",
    "The final recommended model — **SL + SL_RL hybrid with optimized BMA weights** — delivers the most accurate predictions on this submission volume dataset and provides a strong foundation for future production use or further tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d77b0",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This section prepares and exports the historical actual submission volumes and the forecasted volumes from the best performing model (standalone_lstm_plus_sl_rl_pred_orig based on the conclusion) to a CSV file. This file can then be used for reporting or visualization in tools like Power BI. The forecast covers the future period defined by future_df_template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exporting Historical and Next Month Forecast to CSV ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b98b0f60542e1a6b92fc46742797b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='submission_volume_full_forecast.csv', description='CSV File Path:', layout=Layout(width='60%'), pl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99cb5611745413782a36c8da89cbc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Export Full Report to CSV', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b2d27275234f229b3033108d274e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set the file path above and click 'Export Full Report to CSV' to save the CSV report.\n",
      "The report will include historical data, model fit, and a new 1-month forecast (e.g., Mar 2025).\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import os # For path operations if needed in on_export_button_clicked\n",
    "from datetime import datetime # Not strictly needed if using pd.Timestamp correctly\n",
    "import pandas as pd # Assuming pandas is used, ensure it's imported\n",
    "import numpy as np  # Assuming numpy is used, ensure it's imported\n",
    "\n",
    "# --- Export Historical Data and Next Month Forecast to CSV ---\n",
    "print(\"\\n--- Exporting Historical and Next Month Forecast to CSV ---\")\n",
    "\n",
    "# Define chosen forecast column (log and original scale)\n",
    "chosen_forecast_col_log = 'standalone_lstm_plus_sl_rl_pred_log'\n",
    "chosen_forecast_col_orig = 'standalone_lstm_plus_sl_rl_pred_orig'\n",
    "\n",
    "if chosen_forecast_col_log not in future_df_template.columns:\n",
    "    print(f\"Warning: Chosen forecast column '{chosen_forecast_col_log}' not found. Falling back to 'bma_pred_log'.\")\n",
    "    chosen_forecast_col_log = 'bma_pred_log'\n",
    "    chosen_forecast_col_orig = 'bma_pred_orig'\n",
    "    if chosen_forecast_col_log not in future_df_template.columns:\n",
    "        raise ValueError(f\"Fallback forecast column '{chosen_forecast_col_log}' also not found. Cannot export.\")\n",
    "\n",
    "# Widgets for file picker and export button\n",
    "file_picker = widgets.Text(\n",
    "    value='submission_volume_full_forecast.csv', # Default file name\n",
    "    description='CSV File Path:',\n",
    "    placeholder='Enter file path or name...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "export_button = widgets.Button(description=\"Export Full Report to CSV\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_export_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output() # Clear previous messages\n",
    "        print(\"Preparing data for export...\")\n",
    "        try:\n",
    "            # Part 1: Historical data and existing forecasts from future_df_template\n",
    "            # This covers data up to eval_end_common (e.g., Feb 2025)\n",
    "            part1_df = future_df_template[['ds', 'y_original', chosen_forecast_col_orig, 'avg_package_count', \n",
    "                                           'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log']].copy()\n",
    "            part1_df.rename(columns={chosen_forecast_col_orig: 'ForecastedVolume'}, inplace=True)\n",
    "            \n",
    "            # Determine type for part1_df\n",
    "            part1_df['Type'] = np.where(part1_df['y_original'].notnull(), 'Historical', 'Existing Forecast')\n",
    "\n",
    "            # Part 2: Generate new 1-month forecast (e.g., Mar 2025)\n",
    "            print(\"Generating new 1-month forecast extension...\")\n",
    "            last_date_in_template = future_df_template['ds'].max() \n",
    "            new_forecast_start_ds = last_date_in_template + pd.DateOffset(months=1)\n",
    "            \n",
    "            num_forecast_months = 1 # Number of months to forecast\n",
    "            new_forecast_dates = pd.date_range(start=new_forecast_start_ds, periods=num_forecast_months, freq='M')\n",
    "\n",
    "            # Prepare avg_package_count for these new dates (ffill from last known)\n",
    "            # Ensure 'monthly_df' and 'lookback' are defined in your notebook environment\n",
    "            last_known_avg_pkg_count = monthly_df['avg_package_count'].iloc[-1]\n",
    "            new_avg_pkg_counts = pd.Series([last_known_avg_pkg_count] * num_forecast_months)\n",
    "            \n",
    "            df_new_forecast_input = pd.DataFrame({'ds': new_forecast_dates, 'avg_package_count': new_avg_pkg_counts})\n",
    "\n",
    "            # --- Generate SL predictions for new forecast period ---\n",
    "            new_sl_preds_log_extended = []\n",
    "            last_sl_log_preds_for_input = future_df_template['standalone_lstm_pred_log'].iloc[-lookback:].values\n",
    "            current_sl_sequence_scaled = scaler_standalone_lstm_y.transform(last_sl_log_preds_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "            for _ in range(num_forecast_months):\n",
    "                pred_scaled_sl = model_standalone_lstm.predict(current_sl_sequence_scaled, verbose=0)[0,0]\n",
    "                pred_log_sl = scaler_standalone_lstm_y.inverse_transform([[pred_scaled_sl]])[0,0]\n",
    "                new_sl_preds_log_extended.append(pred_log_sl)\n",
    "                # Update sequence for next prediction if num_forecast_months > 1\n",
    "                if num_forecast_months > 1 or _ < num_forecast_months -1 : # only append if there are more steps or it's not the last step of a multi-step forecast\n",
    "                     current_sl_sequence_scaled = np.append(current_sl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl]]], axis=1)\n",
    "\n",
    "            df_new_forecast_input['standalone_lstm_pred_log_extended'] = new_sl_preds_log_extended\n",
    "\n",
    "            # --- Generate SL_RL residual predictions for new forecast period ---\n",
    "            new_sl_rl_resid_preds_log_extended = []\n",
    "            last_sl_rl_log_resids_for_input = future_df_template['sl_rl_predicted_residual_log'].iloc[-lookback:].values\n",
    "            current_sl_rl_sequence_scaled = scaler_sl_rl_residual.transform(last_sl_rl_log_resids_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "            for _ in range(num_forecast_months):\n",
    "                pred_scaled_sl_rl_resid = model_sl_rl.predict(current_sl_rl_sequence_scaled, verbose=0)[0,0]\n",
    "                pred_log_sl_rl_resid = scaler_sl_rl_residual.inverse_transform([[pred_scaled_sl_rl_resid]])[0,0]\n",
    "                new_sl_rl_resid_preds_log_extended.append(pred_log_sl_rl_resid)\n",
    "                # Update sequence for next prediction if num_forecast_months > 1\n",
    "                if num_forecast_months > 1 or _ < num_forecast_months -1 : # only append if there are more steps or it's not the last step of a multi-step forecast\n",
    "                    current_sl_rl_sequence_scaled = np.append(current_sl_rl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl_rl_resid]]], axis=1)\n",
    "            \n",
    "            df_new_forecast_input['sl_rl_predicted_residual_log_extended'] = new_sl_rl_resid_preds_log_extended\n",
    "            \n",
    "            # Combine extended predictions\n",
    "            df_new_forecast_input['ForecastedVolume_log'] = df_new_forecast_input['standalone_lstm_pred_log_extended'] + df_new_forecast_input['sl_rl_predicted_residual_log_extended']\n",
    "            df_new_forecast_input['ForecastedVolume'] = np.expm1(df_new_forecast_input['ForecastedVolume_log'])\n",
    "            df_new_forecast_input['y_original'] = np.nan # Actuals are NaN for new forecasts\n",
    "            df_new_forecast_input['Type'] = f'New {num_forecast_months}M Forecast' # Updated type string\n",
    "            \n",
    "            df_new_1m_export = df_new_forecast_input[['ds', 'y_original', 'ForecastedVolume', 'Type']]\n",
    "\n",
    "            # Combine all parts\n",
    "            final_export_df = pd.concat([\n",
    "                part1_df[['ds', 'y_original', 'ForecastedVolume', 'Type']], \n",
    "                df_new_1m_export\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            final_export_df.rename(columns={\n",
    "                'ds': 'Date',\n",
    "                'y_original': 'ActualVolume'\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Ensure ForecastedVolume is integer (whole number) and handles NaNs from historical actuals appropriately\n",
    "            final_export_df['ForecastedVolume'] = final_export_df['ForecastedVolume'].round().astype('Int64')\n",
    "            final_export_df['ActualVolume'] = final_export_df['ActualVolume'].astype('Int64')\n",
    "\n",
    "\n",
    "            file_path = file_picker.value.strip()\n",
    "            if not file_path:\n",
    "                raise ValueError(\"Please specify a file path for export.\")\n",
    "            \n",
    "            dir_name = os.path.dirname(file_path)\n",
    "            if dir_name and not os.path.exists(dir_name): \n",
    "                os.makedirs(dir_name)\n",
    "                print(f\"Created directory: {dir_name}\")\n",
    "            \n",
    "            final_export_df.to_csv(file_path, index=False, date_format='%Y-%m-%d')\n",
    "            print(f\"Successfully exported data to '{file_path}'\")\n",
    "            print(\"Columns in exported CSV: \", final_export_df.columns.tolist())\n",
    "            print(\"Sample of exported data (last 15 rows):\")\n",
    "            print(final_export_df.tail(15))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during export: {e}\")\n",
    "\n",
    "export_button.on_click(on_export_button_clicked)\n",
    "display(file_picker, export_button, output_area)\n",
    "\n",
    "print(f\"\\nSet the file path above and click '{export_button.description}' to save the CSV report.\")\n",
    "print(\"The report will include historical data, model fit, and a new 1-month forecast (e.g., Mar 2025).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012c56d",
   "metadata": {},
   "source": [
    "## 🔁 Iterative Forecasting with LSTM + Residual Correction (SL + SL_RL)\n",
    "\n",
    "In this section, we implement an **iterative 12-month forecasting** strategy using a two-stage LSTM setup:\n",
    "- `SL` (Standalone LSTM): Predicts the base log-transformed forecast value.\n",
    "- `SL_RL` (Residual LSTM): Predicts the residual (correction) to be added to the base forecast.\n",
    "\n",
    "### ✅ What This Code Does\n",
    "- We generate 12 monthly dates from **March 2025 to February 2026**.\n",
    "- Using the last `lookback` number of predictions, we iteratively predict the next 12 log-transformed values.\n",
    "- Each forecast is **dependent on the previous prediction** (i.e., autoregressive logic).\n",
    "- The final forecast is the **sum of the base and residual LSTM outputs**, transformed back to the original scale using `expm1`.\n",
    "\n",
    "### ⚠️ Limitations and Risks\n",
    "While this approach works and produces a reasonably stable output, it comes with some **important caveats**:\n",
    "\n",
    "- **Error Accumulation**: Since each new prediction feeds on the last, even a small mistake can snowball over time — leading to unrealistic long-term forecasts.\n",
    "- **Flatness or Drift**: The forecasted values may appear overly smooth or flat (hovering around the same range), failing to capture seasonality or meaningful shifts.\n",
    "- **Assumption of Residual Behavior**: Residuals may not always behave in a predictable time series pattern; if they're noisy, the second LSTM might not add much value — or could even amplify noise.\n",
    "\n",
    "### 💡 Why This Matters\n",
    "If we're relying on this model for planning (e.g. resource allocation, release scheduling), we must:\n",
    "- Validate results with domain experts,\n",
    "- Consider comparing with **direct forecasting** or **hybrid models**,\n",
    "- Or incorporate known external factors (e.g., seasonality, marketing campaigns, major title releases).\n",
    "\n",
    "A more robust alternative would be to try a **direct multi-step LSTM**, which predicts all 12 months at once — reducing the risk of compound errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d620a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating New 12-Month Iterative Forecast (Mar 2025 - Feb 2026) ---\n",
      "Generating 12 new monthly forecasts from 2025-03-31 to 2026-02-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- New 12-Month Forecast (March 2025 - February 2026) ---\n",
      "           ds  ForecastedVolume_SL_plus_SL_RL\n",
      "0  2025-03-31                             848\n",
      "1  2025-04-30                             847\n",
      "2  2025-05-31                             847\n",
      "3  2025-06-30                             846\n",
      "4  2025-07-31                             846\n",
      "5  2025-08-31                             845\n",
      "6  2025-09-30                             845\n",
      "7  2025-10-31                             845\n",
      "8  2025-11-30                             845\n",
      "9  2025-12-31                             845\n",
      "10 2026-01-31                             845\n",
      "11 2026-02-28                             845\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating New 12-Month Iterative Forecast (Mar 2025 - Feb 2026) ---\")\n",
    "\n",
    "# Determine the starting point for the new 12-month forecast\n",
    "last_date_in_template = future_df_template['ds'].max()  # This is eval_end_common (2025-02-28)\n",
    "new_forecast_start_ds = last_date_in_template + pd.DateOffset(months=1)\n",
    "new_12m_dates = pd.date_range(start=new_forecast_start_ds, periods=12, freq='M')\n",
    "\n",
    "print(f\"Generating 12 new monthly forecasts from {new_12m_dates.min().date()} to {new_12m_dates.max().date()}\")\n",
    "\n",
    "# DataFrame to store the new 12-month forecast\n",
    "df_extended_forecast = pd.DataFrame({'ds': new_12m_dates})\n",
    "\n",
    "# --- 1. Standalone LSTM (SL) Component: Generate 12 new iterative predictions ---\n",
    "new_sl_preds_log_extended = []\n",
    "\n",
    "# Initial input sequence for SL model:\n",
    "# Last 'lookback' values of 'standalone_lstm_pred_log' from future_df_template, scaled.\n",
    "last_sl_log_preds_for_input = future_df_template['standalone_lstm_pred_log'].iloc[-lookback:].values\n",
    "current_sl_sequence_scaled = scaler_standalone_lstm_y.transform(last_sl_log_preds_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "for i in range(12):\n",
    "    pred_scaled_sl = model_standalone_lstm.predict(current_sl_sequence_scaled, verbose=0)[0, 0]\n",
    "    pred_log_sl = scaler_standalone_lstm_y.inverse_transform([[pred_scaled_sl]])[0, 0]\n",
    "    new_sl_preds_log_extended.append(pred_log_sl)\n",
    "    # Update sequence for next prediction\n",
    "    current_sl_sequence_scaled = np.append(current_sl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl]]], axis=1)\n",
    "\n",
    "df_extended_forecast['sl_pred_log_extended'] = new_sl_preds_log_extended\n",
    "\n",
    "# --- 2. Residual LSTM (SL_RL) Component: Generate 12 new iterative predictions ---\n",
    "new_sl_rl_resid_preds_log_extended = []\n",
    "\n",
    "# Initial input sequence for SL_RL model:\n",
    "# Last 'lookback' values of 'sl_rl_predicted_residual_log' from future_df_template, scaled.\n",
    "# This column was created in Section 14.\n",
    "last_sl_rl_log_resids_for_input = future_df_template['sl_rl_predicted_residual_log'].iloc[-lookback:].values\n",
    "current_sl_rl_sequence_scaled = scaler_sl_rl_residual.transform(last_sl_rl_log_resids_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "for i in range(12):\n",
    "    pred_scaled_sl_rl_resid = model_sl_rl.predict(current_sl_rl_sequence_scaled, verbose=0)[0, 0]\n",
    "    pred_log_sl_rl_resid = scaler_sl_rl_residual.inverse_transform([[pred_scaled_sl_rl_resid]])[0, 0]\n",
    "    new_sl_rl_resid_preds_log_extended.append(pred_log_sl_rl_resid)\n",
    "    # Update sequence for next prediction\n",
    "    current_sl_rl_sequence_scaled = np.append(current_sl_rl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl_rl_resid]]], axis=1)\n",
    "\n",
    "df_extended_forecast['sl_rl_resid_pred_log_extended'] = new_sl_rl_resid_preds_log_extended\n",
    "\n",
    "# --- 3. Combine and Finalize New 12-Month Forecast ---\n",
    "df_extended_forecast['final_extended_pred_log'] = df_extended_forecast['sl_pred_log_extended'] + df_extended_forecast['sl_rl_resid_pred_log_extended']\n",
    "df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'] = np.expm1(df_extended_forecast['final_extended_pred_log'])\n",
    "df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'] = df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'].round().astype('Int64') # Whole numbers\n",
    "\n",
    "print(\"\\n--- New 12-Month Forecast (March 2025 - February 2026) ---\")\n",
    "print(df_extended_forecast[['ds', 'ForecastedVolume_SL_plus_SL_RL']])\n",
    "\n",
    "# This df_extended_forecast can now be appended to the historical/existing forecast data for a complete timeline if needed.\n",
    "# For example, for the CSV export in Section 19, this logic is integrated into the button's callback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76d56c",
   "metadata": {},
   "source": [
    "## 📈 Direct Multi-Step Forecasting with Standalone LSTM\n",
    "\n",
    "In this section, we define, train, and use a **direct multi-step LSTM model** to forecast the next 12 months of volume data in one go — avoiding the potential pitfalls of iterative forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why Use Direct Multi-Step Forecasting?\n",
    "\n",
    "Unlike iterative forecasting (which predicts one step at a time and reuses its own output), direct forecasting:\n",
    "- Predicts **all future steps at once** (e.g. 12 months)\n",
    "- **Avoids error accumulation** since each prediction is not dependent on the last\n",
    "- Can better model **longer-term trends and seasonality**, if trained well\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Step-by-Step Breakdown of the Code\n",
    "\n",
    "#### 1. **Preparing Training Data**\n",
    "- `lookback` defines how many past months we use to predict the future.\n",
    "- For each training example:\n",
    "  - **Input**: a sequence of `lookback` months (X)\n",
    "  - **Output**: the next **12 months** of target values (y)\n",
    "- The input and output sequences are scaled using the same scaler (`scaler_standalone_lstm_y`) applied to the log-transformed volume values.\n",
    "- We align each output with its corresponding date using `ds_for_y_direct`.\n",
    "\n",
    "#### 2. **Splitting Training Data**\n",
    "- We split the sequences using a `cutoff_date`, so only data before that is used to train the model.\n",
    "- This avoids data leakage and respects the temporal order of the data.\n",
    "\n",
    "#### 3. **Model Definition**\n",
    "- A simple LSTM model is defined using:\n",
    "  - One LSTM layer (`LSTM(32)`)\n",
    "  - One Dense layer with `num_steps_out` units to produce 12 outputs at once\n",
    "- The model is compiled with `mean squared error (mse)` loss and the `Adam` optimizer.\n",
    "\n",
    "#### 4. **Training the Model**\n",
    "- The model is trained on the training sequences with `EarlyStopping` to avoid overfitting.\n",
    "- This monitors the training loss and restores the best weights when the loss stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Direct Forecasting Execution (Prediction Phase)\n",
    "\n",
    "- We prepare the last `lookback` months of LSTM input data using the most recent `standalone_lstm_pred_log` values.\n",
    "- This input is scaled and reshaped to match the model’s expected input shape `(1, lookback, 1)`.\n",
    "- The trained model then predicts **12 future log-scaled values at once**.\n",
    "- These are inverse-transformed to real log values, exponentiated to original scale, and rounded to whole numbers.\n",
    "\n",
    "#### ⏳ Output\n",
    "- The resulting forecast covers **March 2025 to February 2026** (12 months).\n",
    "- Results are stored in `df_direct_forecast` with two key columns:\n",
    "  - `ds`: forecast month\n",
    "  - `ForecastedVolume_Direct_SL`: forecasted volume (original scale)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Benefits of This Approach\n",
    "\n",
    "- **Stable forecasts**: No feedback loop means no snowballing of errors.\n",
    "- **Better long-term trends**: The model learns full future patterns rather than extrapolating one step at a time.\n",
    "- **Cleaner logic**: Simpler and faster during inference — only one prediction call per forecast.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Things to Watch Out For\n",
    "\n",
    "- This model requires **more training data** to learn accurate multi-output patterns.\n",
    "- It assumes that the future behaves similarly to past multi-step sequences (e.g. if the pattern was seasonal or autoregressive).\n",
    "- You may need to **tune the architecture** or explore deeper LSTM or attention-based models for best performance.\n",
    "\n",
    "---\n",
    "\n",
    "This technique complements your earlier iterative method and offers a more robust alternative, especially for use cases that demand reliable long-term planning and reduced noise sensitivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f04c8",
   "metadata": {},
   "source": [
    "## 🔮 Part 1: Train a Direct Multi-Step LSTM Forecasting Model\n",
    "\n",
    "In this section, we define and train a standalone Long Short-Term Memory (LSTM) model to directly forecast the next 12 months of submission volumes.\n",
    "\n",
    "### Key Steps:\n",
    "- **Data Preparation**:\n",
    "  - We create supervised learning sequences where the input is the previous `lookback` number of months and the output is the next `12 months` (multi-step forecasting).\n",
    "  - Data is scaled to improve neural network training performance.\n",
    "  \n",
    "- **Model Definition**:\n",
    "  - A simple LSTM model is defined using Keras with:\n",
    "    - One LSTM layer with 32 units.\n",
    "    - A Dense output layer that predicts 12 future values simultaneously.\n",
    "  \n",
    "- **Training**:\n",
    "  - We use early stopping to avoid overfitting and reduce training time.\n",
    "  - Only data **before the defined cutoff date** is used for training.\n",
    "\n",
    "- **Forecasting**:\n",
    "  - The trained model is used to predict the next 12 months based on the last known `standalone_lstm_pred_log` values.\n",
    "  - Predictions are inverse-transformed (from log and scaled values) to get back the real-world volume figures.\n",
    "  - The resulting forecast is stored in a DataFrame along with corresponding future dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "069a9bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Data and Training for Direct Multi-Step Standalone LSTM Model ---\n",
      "Training data shapes: X=(37, 3, 1), y=(37, 12)\n",
      "Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM training complete.\n",
      "--------------------------------------------------\n",
      "WARNING:tensorflow:5 out of the last 40 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A0D1AF72E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "--- Direct 12-Month Forecast (Starting 2025-03-31) ---"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t-matasert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           ds  ForecastedVolume_Direct_SL\n",
      "0  2025-03-31                         874\n",
      "1  2025-04-30                         891\n",
      "2  2025-05-31                         889\n",
      "3  2025-06-30                         896\n",
      "4  2025-07-31                         908\n",
      "5  2025-08-31                         923\n",
      "6  2025-09-30                         909\n",
      "7  2025-10-31                         913\n",
      "8  2025-11-30                         912\n",
      "9  2025-12-31                         933\n",
      "10 2026-01-31                         943\n",
      "11 2026-02-28                         961\n"
     ]
    }
   ],
   "source": [
    "# --- PART 1: Define and Train Direct Multi-Step LSTM Model (model_standalone_lstm_direct) ---\n",
    "print(\"\\n--- Preparing Data and Training for Direct Multi-Step Standalone LSTM Model ---\")\n",
    "\n",
    "num_steps_out = 12  # Number of steps to predict directly\n",
    "\n",
    "# Prepare sequences for direct multi-step LSTM\n",
    "X_direct_sequences, y_direct_sequences_scaled = [], []\n",
    "ds_for_y_direct = []\n",
    "\n",
    "# Ensure required columns exist\n",
    "if not all(col in data_for_standalone_lstm.columns for col in ['ds', 'y_scaled']):\n",
    "    raise ValueError(\"data_for_standalone_lstm must contain 'ds' and 'y_scaled' columns.\")\n",
    "\n",
    "# Create sequences: [lookback] -> [next num_steps_out]\n",
    "for i in range(lookback, len(data_for_standalone_lstm) - num_steps_out + 1):\n",
    "    X_direct_sequences.append(data_for_standalone_lstm['y_scaled'].iloc[i - lookback:i].values)\n",
    "    y_direct_sequences_scaled.append(data_for_standalone_lstm['y_scaled'].iloc[i : i + num_steps_out].values)\n",
    "    ds_for_y_direct.append(data_for_standalone_lstm['ds'].iloc[i])\n",
    "\n",
    "if not X_direct_sequences:\n",
    "    raise ValueError(f\"Not enough data to create sequences with lookback={lookback} and num_steps_out={num_steps_out}.\")\n",
    "\n",
    "# Convert to arrays and reshape\n",
    "X_direct_sequences = np.array(X_direct_sequences).reshape(-1, lookback, 1)\n",
    "y_direct_sequences_scaled = np.array(y_direct_sequences_scaled)\n",
    "ds_for_y_direct = pd.Series(ds_for_y_direct, name='ds')\n",
    "\n",
    "# Split train data by cutoff date\n",
    "train_indices_direct = ds_for_y_direct[ds_for_y_direct < cutoff_date].index\n",
    "X_train_direct_lstm = X_direct_sequences[train_indices_direct]\n",
    "y_train_direct_lstm_scaled = y_direct_sequences_scaled[train_indices_direct]\n",
    "\n",
    "print(f\"Training data shapes: X={X_train_direct_lstm.shape}, y={y_train_direct_lstm_scaled.shape}\")\n",
    "\n",
    "# Define and train the LSTM model\n",
    "model_standalone_lstm_direct = Sequential()\n",
    "model_standalone_lstm_direct.add(LSTM(32, input_shape=(lookback, 1)))\n",
    "model_standalone_lstm_direct.add(Dense(num_steps_out))\n",
    "model_standalone_lstm_direct.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "model_standalone_lstm_direct.fit(X_train_direct_lstm, y_train_direct_lstm_scaled, \n",
    "                                 epochs=100, batch_size=4, verbose=0, callbacks=[early_stop])\n",
    "print(\"✅ LSTM training complete.\\n\" + \"-\"*50)\n",
    "\n",
    "# --- Forecast 12 months ahead using trained model ---\n",
    "last_input = future_df_template['standalone_lstm_pred_log'].iloc[-lookback:].values\n",
    "scaled_input = scaler_standalone_lstm_y.transform(last_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "direct_preds_scaled = model_standalone_lstm_direct.predict(scaled_input, verbose=0)\n",
    "direct_preds_log = scaler_standalone_lstm_y.inverse_transform(direct_preds_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "last_date = future_df_template['ds'].max()\n",
    "forecast_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=num_steps_out, freq='M')\n",
    "\n",
    "df_direct_forecast = pd.DataFrame({\n",
    "    'ds': forecast_dates,\n",
    "    'direct_pred_log': direct_preds_log,\n",
    "})\n",
    "df_direct_forecast['ForecastedVolume_Direct_SL'] = np.expm1(df_direct_forecast['direct_pred_log']).round().astype('Int64')\n",
    "\n",
    "print(f\"\\n--- Direct {num_steps_out}-Month Forecast (Starting {forecast_dates.min().date()}) ---\")\n",
    "print(df_direct_forecast[['ds', 'ForecastedVolume_Direct_SL']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e1c5e",
   "metadata": {},
   "source": [
    "## 💾 Part 2: Export Historical and Forecast Data with Download Button\n",
    "\n",
    "This section combines the historical submission data with the newly forecasted values and provides a downloadable CSV export.\n",
    "\n",
    "### Key Steps:\n",
    "- **Historical Data**:\n",
    "  - We take existing data (`y_original`) from the model input DataFrame and tag it as either:\n",
    "    - `\"Historical Actual\"` (if the value is known)\n",
    "    - `\"Future (No Actual Yet)\"` (if the value is missing)\n",
    "\n",
    "- **Forecasted Data**:\n",
    "  - The forecasted volumes from the LSTM model (covering 12 future months) are added and tagged as:\n",
    "    - `\"Direct SL Forecast (12M New)\"`\n",
    "\n",
    "- **Combining Data**:\n",
    "  - Both historical and forecasted data are concatenated into a single DataFrame.\n",
    "  - Columns are formatted consistently, and the dataset is sorted chronologically.\n",
    "\n",
    "- **Download Button**:\n",
    "  - A downloadable CSV is generated in the working directory.\n",
    "  - An interactive download link (HTML-based) is displayed so users can easily download the output without typing paths or filenames manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "171a704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Export File (Historical + Direct Forecast) ---\n",
      "\n",
      "✅ Forecast exported to: c:\\Users\\t-matasert\\OneDrive - Microsoft\\desktop\\L7 Data Science & AI\\Project\\VolumePredictionProject\\data\\direct_sl_full_forecast_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-matasert\\AppData\\Local\\Temp\\ipykernel_32012\\828019045.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_export_df = pd.concat([historical_export_part, forecast_export_part], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ActualVolume</th>\n",
       "      <th>ForecastedVolume</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>833.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Historical Actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>939.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Historical Actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>858.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Historical Actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>874</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>889</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>896</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>908</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>923</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>909</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>913</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>912</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>933</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>943</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2026-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>961</td>\n",
       "      <td>Direct SL Forecast (12M New)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  ActualVolume  ForecastedVolume                          Type\n",
       "47 2024-12-31         833.0              <NA>             Historical Actual\n",
       "48 2025-01-31         939.0              <NA>             Historical Actual\n",
       "49 2025-02-28         858.0              <NA>             Historical Actual\n",
       "50 2025-03-31           NaN               874  Direct SL Forecast (12M New)\n",
       "51 2025-04-30           NaN               891  Direct SL Forecast (12M New)\n",
       "52 2025-05-31           NaN               889  Direct SL Forecast (12M New)\n",
       "53 2025-06-30           NaN               896  Direct SL Forecast (12M New)\n",
       "54 2025-07-31           NaN               908  Direct SL Forecast (12M New)\n",
       "55 2025-08-31           NaN               923  Direct SL Forecast (12M New)\n",
       "56 2025-09-30           NaN               909  Direct SL Forecast (12M New)\n",
       "57 2025-10-31           NaN               913  Direct SL Forecast (12M New)\n",
       "58 2025-11-30           NaN               912  Direct SL Forecast (12M New)\n",
       "59 2025-12-31           NaN               933  Direct SL Forecast (12M New)\n",
       "60 2026-01-31           NaN               943  Direct SL Forecast (12M New)\n",
       "61 2026-02-28           NaN               961  Direct SL Forecast (12M New)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0020d193bff84524953902aff30432bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Download your forecast:</b>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8880f15045fa4776b348e239839e6bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<a href='direct_sl_full_forecast_report.csv' download>📥 Click to download: <i>direct_sl_full_forec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- PART 2: Export Combined Data and Forecast with Download Button ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Preparing Export File (Historical + Direct Forecast) ---\")\n",
    "\n",
    "# Prepare historical portion\n",
    "historical_export_part = future_df_template[['ds', 'y_original']].copy()\n",
    "historical_export_part.rename(columns={'ds': 'Date', 'y_original': 'ActualVolume'}, inplace=True)\n",
    "historical_export_part['ForecastedVolume'] = np.nan\n",
    "historical_export_part['Type'] = np.where(historical_export_part['ActualVolume'].notna(), \n",
    "                                          'Historical Actual', 'Future (No Actual Yet)')\n",
    "\n",
    "# Prepare forecast portion\n",
    "forecast_export_part = df_direct_forecast[['ds', 'ForecastedVolume_Direct_SL']].copy()\n",
    "forecast_export_part.rename(columns={'ds': 'Date', 'ForecastedVolume_Direct_SL': 'ForecastedVolume'}, inplace=True)\n",
    "forecast_export_part['ActualVolume'] = np.nan\n",
    "forecast_export_part['Type'] = f'Direct SL Forecast ({num_steps_out}M New)'\n",
    "\n",
    "# Combine and sort\n",
    "combined_export_df = pd.concat([historical_export_part, forecast_export_part], ignore_index=True)\n",
    "combined_export_df = combined_export_df[['Date', 'ActualVolume', 'ForecastedVolume', 'Type']]\n",
    "combined_export_df.sort_values(by='Date', inplace=True)\n",
    "combined_export_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save CSV\n",
    "filename = \"direct_sl_full_forecast_report.csv\"\n",
    "export_path = os.path.join(os.getcwd(), filename)\n",
    "combined_export_df.to_csv(export_path, index=False, date_format='%Y-%m-%d')\n",
    "\n",
    "# Show download link\n",
    "download_label = widgets.HTML(value=\"<b>Download your forecast:</b>\")\n",
    "download_link = widgets.HTML(value=f\"<a href='{filename}' download>📥 Click to download: <i>{filename}</i></a>\")\n",
    "\n",
    "# Display results and download link\n",
    "print(f\"\\n✅ Forecast exported to: {export_path}\")\n",
    "display(combined_export_df.tail(15))\n",
    "display(download_label, download_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6b33",
   "metadata": {},
   "source": [
    "## 📈 Forecasting Automation Roadmap (Microsoft Fabric + LSTM + Prophet)\n",
    "\n",
    "This document outlines the high-level plan to automate the forecasting pipeline using Microsoft Fabric. The goal is to ingest data from a database, dynamically tune model parameters via grid search, train ensemble models, and export the forecasts — all in a scalable and automated way.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Phase 1: Core Pipeline (Short-Term Plan)\n",
    "\n",
    "#### 1. Ingest Data from SQL Database\n",
    "- Use **Microsoft Fabric Lakehouse** or **Dataflows Gen2** to connect to your source database.\n",
    "- Load cleaned submission data into a **Lakehouse table**.\n",
    "\n",
    "#### 2. Build Automated Training Notebook\n",
    "- Create a **Fabric Notebook (Python)** to:\n",
    "  - Read data from the Lakehouse.\n",
    "  - Scale and preprocess input.\n",
    "  - Perform **GridSearch** to tune Prophet and LSTM parameters.\n",
    "  - Train both models and generate predictions.\n",
    "  - Combine forecasts into an **ensemble prediction**.\n",
    "\n",
    "#### 3. Export Forecasts to Lakehouse\n",
    "- Write the forecast results to a **Lakehouse output table** using:\n",
    "  ```python\n",
    "  df.write.mode('overwrite').saveAsTable('forecast_results')\n",
    "\n",
    "#### 4. Automate with Scheduling\n",
    "Use Fabric notebook scheduling or Data Activator to trigger training:\n",
    "\n",
    "On a recurring basis (e.g., daily/weekly).\n",
    "\n",
    "After new data is ingested into the Lakehouse.\n",
    "\n",
    "#### 5. Visualize & Monitor\n",
    "Create a Power BI dashboard connected to the Lakehouse to:\n",
    "\n",
    "Track forecast vs. actuals.\n",
    "\n",
    "Show future trends and capacity planning.\n",
    "\n",
    "Monitor model accuracy and performance over time.\n",
    "\n",
    "#### 💡 Additional Recommendations (Based on Your Notebook)\n",
    "Model Registry Table\n",
    "Log best hyperparameters, training date, RMSE/MAE, and model type for traceability and model monitoring.\n",
    "\n",
    "Forecast Versioning\n",
    "Add a run_id or model_version column to each forecast output to track which model produced the prediction.\n",
    "\n",
    "Fallback Strategy\n",
    "Implement fallback logic — e.g., use Prophet or a simple moving average model if the LSTM model underperforms on new data.\n",
    "\n",
    "Pipeline Monitoring\n",
    "Log training status, runtime, and any errors to a dedicated audit table in your Lakehouse to support transparency and reliability of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
