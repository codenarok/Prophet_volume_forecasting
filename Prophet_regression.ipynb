{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b880fd",
   "metadata": {},
   "source": [
    "# Submission Volume Prediction: A Comparative Analysis\n",
    "\n",
    "This notebook walks through the process of forecasting submission volumes using various time series models, including **Prophet**, **LSTM**, and **hybrid approaches combined with Bayesian Model Averaging (BMA)**.  \n",
    "The goal is to identify the most accurate model for this specific dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup: Importing Libraries and Loading Data\n",
    "\n",
    "**Explanation:**  \n",
    "This first block imports all necessary Python libraries for:\n",
    "- Data manipulation\n",
    "- Time series modeling\n",
    "- Neural networks\n",
    "- Evaluation\n",
    "\n",
    "It also defines the path to the input CSV file and loads the dataset into a `pandas` DataFrame.  \n",
    "Basic error handling is included to ensure the file is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3dccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import matplotlib.pyplot as plt # Plotting is currently commented out\n",
    "# import seaborn as sns # Plotting is currently commented out\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Load the cleaned submission data exported from SQL\n",
    "CSV_PATH = \"c:/Users/t-matasert/OneDrive - Microsoft/desktop/L7 Data Science & AI/Project/VolumePredictionProject/data/input/SQL_Results_Submissions_Prophet_v3.csv\"\n",
    "\n",
    "# Read CSV and confirm it loaded properly\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(\"Original Data Head:\")\n",
    "    print(df.head(10))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CSV file not found at: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d86caa",
   "metadata": {},
   "source": [
    "## 2. Initial Data Preprocessing\n",
    "\n",
    "This block focuses on cleaning the raw data. It involves:\n",
    "\n",
    "- Stripping any leading/trailing whitespace from column names.\n",
    "- Converting the `SubmissionCreatedDate` column to datetime objects, ensuring correct parsing of **day-first** dates.\n",
    "- Standardizing the `BypassFlag` based on the `IsBypassed` column, mapping `'bypass'` and `'bypass once released'` to `1` and others to `0`.\n",
    "- Filtering out rows where `BypassFlag` is `1` (i.e., keeping only non-bypassed submissions).\n",
    "- Excluding specified `SubmissionType` categories that are not relevant for this volume prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert date column, handling potential dayfirst format\n",
    "df['SubmissionCreatedDate'] = pd.to_datetime(df['SubmissionCreatedDate'], dayfirst=True)\n",
    "\n",
    "# Update the BypassFlag logic to include 'bypass once released' as bypass\n",
    "df['BypassFlag'] = df['IsBypassed'].apply(lambda x: 1 if str(x).lower() in ['bypass', 'bypass once released'] else 0)\n",
    "\n",
    "# Filter out rows where BypassFlag is 1\n",
    "df = df[df['BypassFlag'] == 0]\n",
    "\n",
    "# Filter out unnecessary SubmissionTypes\n",
    "submission_types_to_exclude_prophet = ['Handheld Verified','Stub', 'Canary File', 'Hub App (General)', 'Closed Beta - Not Tested',\n",
    "                                     'Other', 'Compilation Disc', 'External Beta', 'Internal Beta', 'Beyond Console', 'Open Beta', 'Mouse & Keyboard']\n",
    "df = df[~df['SubmissionType'].astype(str).str.lower().isin([st.lower() for st in submission_types_to_exclude_prophet])]\n",
    "\n",
    "print(\"\\nData shape after initial preprocessing:\", df.shape)\n",
    "print(\"Processed Data Head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11be17b",
   "metadata": {},
   "source": [
    "## 3. Data Aggregation and Feature Engineering for Time Series\n",
    "\n",
    "The preprocessed data is now aggregated to a **monthly frequency**. Key steps include:\n",
    "\n",
    "- Resampling the data by month (`'M'`) based on `SubmissionCreatedDate`.\n",
    "- Calculating the **distinct count of `SubmissionID`** for each month to get the **monthly submission volume**. This becomes our target variable `y`.\n",
    "- Storing the original `y` values before transformation for later comparison.\n",
    "- Applying a **log transformation** (`np.log1p`) to the target variable `y`. This helps stabilize variance and often improves model performance for time series with **exponential trends**.\n",
    "- Calculating the **average `PackageCount`** per month and adding it as an **exogenous regressor** (`avg_package_count`) to the monthly DataFrame.\n",
    "- Any missing values in this regressor are **forward-filled** and then **backward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data monthly: count distinct SubmissionID per month\n",
    "monthly_df = df.resample('M', on='SubmissionCreatedDate')['SubmissionID'].nunique().reset_index(name='y')\n",
    "monthly_df.columns = ['ds', 'y'] # Prophet requires 'ds' for date and 'y' for target\n",
    "\n",
    "# Store original 'y' for later comparison and log transform the target variable 'y'\n",
    "monthly_df['y_original'] = monthly_df['y']\n",
    "monthly_df['y'] = np.log1p(monthly_df['y'])\n",
    "\n",
    "# Add average PackageCount per month as a regressor\n",
    "monthly_avg_package = df.resample('M', on='SubmissionCreatedDate')['PackageCount'].mean().reset_index()\n",
    "monthly_avg_package.columns = ['ds', 'avg_package_count']\n",
    "monthly_df = pd.merge(monthly_df, monthly_avg_package, on='ds', how='left')\n",
    "\n",
    "# Fill NaNs in avg_package_count that might arise from resampling, then ffill/bfill\n",
    "monthly_df['avg_package_count'] = monthly_df['avg_package_count'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(\"\\nMonthly Aggregated Data Head:\")\n",
    "print(monthly_df.head())\n",
    "print(\"\\nMonthly Aggregated Data Info:\")\n",
    "monthly_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df32ee4",
   "metadata": {},
   "source": [
    "## 4. Defining Holiday Effects\n",
    "\n",
    "Prophet allows for the inclusion of custom **holiday effects**.  \n",
    "This block defines a `DataFrame` specifying **\"year-end slowdown\"** periods around **Christmas** and **New Year**.\n",
    "\n",
    "For each holiday instance:\n",
    "- A **lower window** and an **upper window** are defined.\n",
    "- These windows capture the effects **leading up to and following** the specific holiday dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holiday slowdown dataframe for Prophet\n",
    "holiday_events = pd.DataFrame({\n",
    "    'holiday': 'year_end_slowdown',\n",
    "    'ds': pd.to_datetime([\n",
    "        '2021-12-24', '2021-12-25', '2022-01-01',\n",
    "        '2022-12-24', '2022-12-25', '2023-01-01',\n",
    "        '2023-12-24', '2023-12-25', '2024-01-01',\n",
    "        '2024-12-24', '2024-12-25', '2025-01-01' # Include future holidays for forecasting\n",
    "    ]),\n",
    "    'lower_window': -3, # Affects 3 days before\n",
    "    'upper_window': 3  # Affects 3 days after\n",
    "})\n",
    "print(\"\\nHoliday Events DataFrame:\")\n",
    "print(holiday_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517a31a",
   "metadata": {},
   "source": [
    "## 5. Defining Training Cutoff Date\n",
    "\n",
    "**Explanation:**  \n",
    "A `cutoff_date` is defined to separate **historical data** used for training the models from the data used for **evaluation** (the \"future\").\n",
    "\n",
    "- All models will be trained on data **before** this date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test split for Prophet and training cutoff for LSTMs\n",
    "cutoff_date = pd.to_datetime('2024-05-01') # Marks end of training for Prophet and LSTMs\n",
    "train_monthly_prophet = monthly_df[monthly_df['ds'] < cutoff_date].copy()\n",
    "\n",
    "print(f\"\\nData up to {cutoff_date.date()} will be used for training the initial models.\")\n",
    "print(f\"Prophet training data shape: {train_monthly_prophet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b5c55",
   "metadata": {},
   "source": [
    "## 6. Prophet Model (Model P) – Initialization and Training\n",
    "\n",
    "This block initializes the main **Prophet model** (referred to as **Model P**).\n",
    "\n",
    "- It uses pre-determined **best hyperparameters** for:\n",
    "  - `changepoint_prior_scale`\n",
    "  - `seasonality_mode`\n",
    "  - `seasonality_prior_scale`\n",
    "  - `holidays_prior_scale`\n",
    "- **Yearly seasonality** is enabled.\n",
    "- **Weekly seasonality** is disabled (since the data is monthly).\n",
    "- `avg_package_count` is added as an **external regressor**.\n",
    "- A custom **monthly seasonality** is also added.\n",
    "- The model is then **trained (fit)** using the `train_monthly_prophet` DataFrame, which includes:\n",
    "  - The **log-transformed target `y`**\n",
    "  - The **`avg_package_count` regressor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prophet Model (Model P) ---\n",
    "print(\"\\n--- Training Prophet Model (Model P) ---\")\n",
    "\n",
    "# Use best params found from grid search completed separately.\n",
    "best_cps = 0.05\n",
    "best_sm = 'additive'\n",
    "best_sps = 10.0\n",
    "best_hps = 1.0\n",
    "\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False, # Data is monthly\n",
    "    changepoint_prior_scale=best_cps,\n",
    "    seasonality_mode=best_sm,\n",
    "    seasonality_prior_scale=best_sps,\n",
    "    holidays_prior_scale=best_hps,\n",
    "    holidays=holiday_events\n",
    ")\n",
    "prophet_model.add_regressor('avg_package_count')\n",
    "prophet_model.add_seasonality(name='monthly', period=30.5, fourier_order=5) # Custom monthly seasonality\n",
    "\n",
    "print(f\"Training Prophet model on data up to: {train_monthly_prophet['ds'].max().date()}\")\n",
    "prophet_model.fit(train_monthly_prophet[['ds', 'y', 'avg_package_count']])\n",
    "print(\"Prophet model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4f2fc",
   "metadata": {},
   "source": [
    "## 7. Prophet Model (Model P) – Generating Full Period Forecast\n",
    "\n",
    "This block defines the **forecast horizon** and creates a template `DataFrame` (`future_df_template`) that includes both **historical dates** and **future dates** for prediction.\n",
    "\n",
    "- The common **evaluation period** starts from `cutoff_date` and ends on `eval_end_common`.\n",
    "- The `make_future_dataframe` method is used to generate the future dates.\n",
    "- The `avg_package_count` regressor values are **merged** into this template.\n",
    "  - Any missing future values are **forward-filled** and **backward-filled**.\n",
    "- Actual `y` values are also merged into the template for later evaluation.\n",
    "- Prophet’s `predict` method is called on this template to generate forecasts (`yhat`) for the entire period.\n",
    "- These predictions are stored in:  \n",
    "  `future_df_template['prophet_pred_log']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forecast horizon for all models\n",
    "eval_start_common = cutoff_date # Start of common evaluation period\n",
    "eval_end_common = pd.to_datetime('2025-02-28') # Final evaluation end date\n",
    "\n",
    "# Calculate periods needed for make_future_dataframe based on Prophet's training end\n",
    "periods_from_train_end = (eval_end_common.to_period('M') - train_monthly_prophet['ds'].max().to_period('M')).n\n",
    "min_forecast_periods = 6 # Ensure a minimum forecast length\n",
    "periods_for_future_df = max(periods_from_train_end, min_forecast_periods)\n",
    "\n",
    "# Create a future dataframe template that will be used by all models for consistency\n",
    "future_df_template = prophet_model.make_future_dataframe(periods=periods_for_future_df, freq='M')\n",
    "# Merge actual 'y' and 'avg_package_count' for the entire range (historical and future placeholders)\n",
    "future_df_template = pd.merge(future_df_template, monthly_df[['ds', 'avg_package_count', 'y']], on='ds', how='left')\n",
    "future_df_template['avg_package_count'] = future_df_template['avg_package_count'].ffill().bfill() # Fill regressor for future dates\n",
    "\n",
    "# Store Prophet's full forecast in the template\n",
    "forecast_prophet_full = prophet_model.predict(future_df_template[['ds', 'avg_package_count']]) # Predict on the template\n",
    "future_df_template['prophet_pred_log'] = forecast_prophet_full['yhat']\n",
    "\n",
    "print(\"\\nFuture DataFrame Template Head with Prophet Predictions:\")\n",
    "print(future_df_template.head())\n",
    "print(\"\\nFuture DataFrame Template Tail with Prophet Predictions:\")\n",
    "print(future_df_template.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e6c89",
   "metadata": {},
   "source": [
    "## 8. Standalone LSTM Model (Model SL) – Data Preparation\n",
    "This section prepares data for the **Standalone LSTM model (Model SL)**, which directly predicts the **log-transformed submission volume `y`**.\n",
    "\n",
    "- A **lookback period** is defined, determining how many past time steps are used to predict the next.\n",
    "- The target variable `y` is **scaled** using `MinMaxScaler` to a range of `[0, 1]`, which is generally beneficial for LSTM performance.\n",
    "- **Input sequences (`X`)** and corresponding **target values (`y`)** are created:\n",
    "  - `X` consists of `lookback` previous scaled `y` values.\n",
    "  - `y` is the next scaled `y` value.\n",
    "- The **dates (`ds`)** corresponding to the target `y` values are stored for later **alignment and residual calculation**.\n",
    "- `X` is reshaped to the required **3D format** for LSTM input: `[samples, timesteps, features]`.\n",
    "- The data is split into:\n",
    "  - **Training sequences** (up to `cutoff_date`)\n",
    "  - Sequences for **later prediction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standalone LSTM Model (Model SL - Directly predicting 'y') ---\n",
    "print(\"\\n--- Preparing Data for Standalone LSTM Model (Model SL) ---\")\n",
    "lookback = 3 # Number of previous months to use for predicting the next month\n",
    "\n",
    "# Prepare data for standalone LSTM: target is 'y' (log-transformed submission volume)\n",
    "data_for_standalone_lstm = monthly_df[['ds', 'y']].copy() # Keep 'ds' for easier indexing and merging later\n",
    "\n",
    "# Scale the target variable 'y'\n",
    "scaler_standalone_lstm_y = MinMaxScaler(feature_range=(0, 1))\n",
    "data_for_standalone_lstm['y_scaled'] = scaler_standalone_lstm_y.fit_transform(data_for_standalone_lstm[['y']])\n",
    "\n",
    "# Create sequences for X (input) and y (target)\n",
    "X_standalone_sequences, y_standalone_sequences_scaled = [], []\n",
    "ds_for_y_standalone = [] # To keep track of dates for y_standalone_sequences_scaled (targets)\n",
    "\n",
    "for i in range(lookback, len(data_for_standalone_lstm)):\n",
    "    X_standalone_sequences.append(data_for_standalone_lstm['y_scaled'].iloc[i-lookback:i].values)\n",
    "    y_standalone_sequences_scaled.append(data_for_standalone_lstm['y_scaled'].iloc[i])\n",
    "    ds_for_y_standalone.append(data_for_standalone_lstm['ds'].iloc[i]) # Date of the target y\n",
    "\n",
    "X_standalone_sequences = np.array(X_standalone_sequences)\n",
    "y_standalone_sequences_scaled = np.array(y_standalone_sequences_scaled)\n",
    "ds_for_y_standalone = pd.Series(ds_for_y_standalone, name='ds') # Convert to Series for easier indexing\n",
    "\n",
    "# Reshape X for LSTM input: [samples, timesteps, features]\n",
    "# Here, features = 1 (only using lagged 'y')\n",
    "X_standalone_sequences = X_standalone_sequences.reshape((X_standalone_sequences.shape[0], X_standalone_sequences.shape[1], 1))\n",
    "\n",
    "# Training data for Standalone LSTM ends before cutoff_date\n",
    "# ds_for_y_standalone contains the dates for y_standalone_sequences_scaled\n",
    "train_indices_standalone = ds_for_y_standalone[ds_for_y_standalone < cutoff_date].index\n",
    "\n",
    "X_train_standalone_lstm = X_standalone_sequences[train_indices_standalone]\n",
    "y_train_standalone_lstm_scaled = y_standalone_sequences_scaled[train_indices_standalone]\n",
    "\n",
    "# Store corresponding 'ds' and actual 'y' (log-transformed) for the training period of Standalone LSTM\n",
    "# This is needed for calculating residuals of the Standalone LSTM later\n",
    "ds_train_standalone_lstm = ds_for_y_standalone[train_indices_standalone]\n",
    "actual_y_train_standalone_lstm_log = data_for_standalone_lstm.set_index('ds').loc[ds_train_standalone_lstm, 'y'].values\n",
    "\n",
    "print(f\"Standalone LSTM training data shapes: X={X_train_standalone_lstm.shape}, y={y_train_standalone_lstm_scaled.shape}\")\n",
    "print(f\"Number of training sequences for Standalone LSTM: {len(ds_train_standalone_lstm)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4f989",
   "metadata": {},
   "source": [
    "## 9. Standalone LSTM Model (Model SL) – Definition and Training\n",
    "\n",
    "This block defines and trains the **Standalone LSTM model (Model SL)**.\n",
    "\n",
    "- A **sequential Keras model** is created.\n",
    "- It consists of:\n",
    "  - An **LSTM layer** with 32 units, taking input of shape `(lookback, 1)`  \n",
    "    (i.e., 1 feature: the lagged, scaled `y`)\n",
    "  - A **Dense output layer** with 1 unit to predict the next scaled `y` value.\n",
    "- The model is compiled with:\n",
    "  - `'mse'` (mean squared error) loss\n",
    "  - `'adam'` optimizer\n",
    "- The model is trained for **100 epochs** with a **batch size of 4**.\n",
    "- `EarlyStopping` is used to:\n",
    "  - Monitor the training loss\n",
    "  - Prevent overfitting\n",
    "  - Automatically restore the **best weights**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the standalone LSTM model (using user-specified \"best\" params)\n",
    "model_standalone_lstm = Sequential()\n",
    "lookback = 3 # Number of lagged observations to use (explicitly set as per user update)\n",
    "model_standalone_lstm.add(LSTM(32, input_shape=(lookback, 1))) # 1 feature: lagged 'y'\n",
    "model_standalone_lstm.add(Dense(1)) # Output layer for predicting the single next value\n",
    "model_standalone_lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training Standalone LSTM model (Model SL)...\")\n",
    "# Using EarlyStopping to prevent overfitting and restore best weights\n",
    "standalone_lstm_early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "model_standalone_lstm.fit(X_train_standalone_lstm, y_train_standalone_lstm_scaled,\n",
    "                          epochs=100, \n",
    "                          batch_size=4, \n",
    "                          verbose=0, # Set to 1 or 2 to see training progress\n",
    "                          callbacks=[standalone_lstm_early_stopping])\n",
    "print(\"Standalone LSTM model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423aca37",
   "metadata": {},
   "source": [
    "## 10. Standalone LSTM Model (Model SL) – Generating Full Period Forecast\n",
    "\n",
    "After training, the **Standalone LSTM model** is used to generate predictions for the entire **forecast horizon** defined in `future_df_template`.\n",
    "\n",
    "- Predictions begin from the **end of the LSTM’s training data**.\n",
    "- An **iterative (auto-regressive)** forecasting process is used:\n",
    "  - The **last known sequence** from the training data is used as the initial input.\n",
    "  - The model predicts **one step ahead**.\n",
    "  - This prediction is then used to **update the input sequence** for the next prediction.\n",
    "- All **scaled predictions** are collected.\n",
    "- These predictions are then **inverse-transformed** using `scaler_standalone_lstm_y` to return them to the **original log-transformed scale**.\n",
    "- The predictions are **aligned with the dates** in `future_df_template` and stored in the `standalone_lstm_pred_log` column.\n",
    "- Any remaining `NaN`s (e.g., at the beginning if the lookback period isn't fully covered by `ds_for_y_standalone`, or at the end if predictions don’t perfectly align with `future_df_template` length) are **forward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93300935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Standalone LSTM predictions for the full future_df_template horizon\n",
    "print(\"Generating full period predictions with Standalone LSTM (Model SL)...\")\n",
    "\n",
    "# Start with the scaled y-values from the training period of the standalone LSTM\n",
    "# These are the actual scaled values up to the point where training ended.\n",
    "all_standalone_lstm_preds_scaled = list(y_standalone_sequences_scaled[train_indices_standalone])\n",
    "\n",
    "\n",
    "# The first input sequence for iterative forecasting is the last sequence from X_train_standalone_lstm\n",
    "current_input_sequence = X_train_standalone_lstm[-1].reshape(1, lookback, 1)\n",
    "\n",
    "# Determine how many future steps we need to predict beyond the training data\n",
    "# to fill up to the end of future_df_template\n",
    "last_train_date_sl = ds_train_standalone_lstm.max()\n",
    "num_future_preds_needed = len(future_df_template[future_df_template['ds'] > last_train_date_sl])\n",
    "\n",
    "for _ in range(num_future_preds_needed):\n",
    "    pred_scaled = model_standalone_lstm.predict(current_input_sequence, verbose=0)[0,0]\n",
    "    all_standalone_lstm_preds_scaled.append(pred_scaled)\n",
    "    # Update sequence: remove oldest, add new prediction\n",
    "    new_element_for_sequence = np.array([[[pred_scaled]]]) # Shape (1,1,1)\n",
    "    current_input_sequence = np.append(current_input_sequence[:, 1:, :], new_element_for_sequence, axis=1)\n",
    "\n",
    "# Inverse transform all predictions (training part + forecasted part)\n",
    "all_standalone_lstm_preds_log = scaler_standalone_lstm_y.inverse_transform(np.array(all_standalone_lstm_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Align these predictions with future_df_template\n",
    "# The predictions correspond to `ds_train_standalone_lstm` and then the future dates\n",
    "# Create a temporary DataFrame for merging\n",
    "pred_dates_for_sl_merge = list(ds_train_standalone_lstm) + \\\n",
    "                          list(future_df_template[future_df_template['ds'] > last_train_date_sl]['ds'])\n",
    "\n",
    "# Ensure lengths match if there are slight discrepancies\n",
    "max_len = min(len(pred_dates_for_sl_merge), len(all_standalone_lstm_preds_log))\n",
    "temp_standalone_lstm_preds_df = pd.DataFrame({\n",
    "    'ds': pred_dates_for_sl_merge[:max_len], \n",
    "    'standalone_lstm_pred_log': all_standalone_lstm_preds_log[:max_len]\n",
    "})\n",
    "\n",
    "# Merge into future_df_template\n",
    "future_df_template = pd.merge(future_df_template.drop(columns=['standalone_lstm_pred_log'], errors='ignore'), \n",
    "                              temp_standalone_lstm_preds_df, on='ds', how='left')\n",
    "\n",
    "# Fill any NaNs that might occur due to merge or slight misalignments\n",
    "# Forward fill is generally appropriate for forecasts\n",
    "future_df_template['standalone_lstm_pred_log'] = future_df_template['standalone_lstm_pred_log'].ffill()\n",
    "# If there are still NaNs at the beginning (before LSTM predictions start), fill with a known value or handle as needed\n",
    "# For this setup, it's assumed LSTM predictions will cover the relevant forecast period.\n",
    "\n",
    "print(\"Standalone LSTM full period predictions merged into future_df_template.\")\n",
    "print(\"Future DataFrame Template Head with SL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log']].head())\n",
    "print(\"Future DataFrame Template Tail with SL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2ed0e",
   "metadata": {},
   "source": [
    "## 11. Residual-Correcting LSTM (for Prophet's Residuals – Model P_RL) – Data Preparation & Training\n",
    "\n",
    "**Explanation:**  \n",
    "This section introduces the first **hybrid approach**: using an **LSTM to model the residuals** of the main Prophet model (Model P).  \n",
    "This hybrid model is referred to as **Model P_RL**.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "- **Residual Calculation**:  \n",
    "  - Prophet’s predictions (`yhat`) on its training data (`train_monthly_prophet`) are obtained.\n",
    "  - Residuals are calculated as:  \n",
    "    `actual_y - prophet_yhat`\n",
    "\n",
    "- **Data Scaling**:  \n",
    "  - Residuals are scaled using `StandardScaler` (since they are centered around zero).\n",
    "  - The `avg_package_count` regressor (for this LSTM’s training period) is scaled using `MinMaxScaler`.\n",
    "\n",
    "- **Sequence Creation**:  \n",
    "  - Input sequences (`X`) for the LSTM consist of:\n",
    "    - Lagged **scaled residuals**\n",
    "    - Lagged **scaled `avg_package_count`**\n",
    "  - The target (`y`) is the **next scaled residual**.\n",
    "\n",
    "- **Train/Validation Split**:  \n",
    "  - The sequenced data is split into **training** and **validation** sets for the `P_RL` LSTM.\n",
    "\n",
    "- **P_RL LSTM Definition & Training**:\n",
    "  - A **simplified LSTM model** (`model_p_rl`) is defined:\n",
    "    - 1 LSTM layer with **16 units**\n",
    "    - A `Dropout` layer\n",
    "    - A `Dense` output layer\n",
    "  - The model is compiled and trained to predict the **scaled residuals**.\n",
    "  - `EarlyStopping` is used, monitoring **validation loss**.\n",
    "  \n",
    "- The model’s performance on the **validation set** is **evaluated and printed**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Residual-Correcting LSTM (for Prophet's residuals - Model P_RL) ---\n",
    "print(\"\\n--- Training Residual-Correcting LSTM (for Prophet's residuals - Model P_RL) ---\")\n",
    "\n",
    "# Get Prophet's predictions on its training data to calculate residuals\n",
    "prophet_train_preds_df = prophet_model.predict(train_monthly_prophet[['ds', 'avg_package_count']])\n",
    "p_rl_train_source = pd.merge(train_monthly_prophet[['ds', 'y', 'avg_package_count']],\n",
    "                             prophet_train_preds_df[['ds', 'yhat']], on='ds', how='inner')\n",
    "p_rl_train_source['residual'] = p_rl_train_source['y'] - p_rl_train_source['yhat'] # Prophet's residuals\n",
    "\n",
    "# Scale residuals and the regressor for this LSTM\n",
    "scaler_p_rl_residual = StandardScaler() # StandardScaler is often good for residuals\n",
    "p_rl_train_source['residual_scaled'] = scaler_p_rl_residual.fit_transform(p_rl_train_source[['residual']])\n",
    "\n",
    "scaler_p_rl_pkg_count = MinMaxScaler() # Scaler for avg_package_count for this P_RL LSTM\n",
    "p_rl_train_source['avg_package_count_scaled'] = scaler_p_rl_pkg_count.fit_transform(p_rl_train_source[['avg_package_count']])\n",
    "\n",
    "# Create sequences for P_RL LSTM\n",
    "X_p_rl, y_p_rl_scaled = [], []\n",
    "ds_for_y_p_rl = [] # Dates for the target residuals\n",
    "\n",
    "for i in range(lookback, len(p_rl_train_source)):\n",
    "    # Features: lagged scaled residuals and lagged scaled avg_package_count\n",
    "    sequence = p_rl_train_source[['residual_scaled', 'avg_package_count_scaled']].iloc[i-lookback:i].values\n",
    "    X_p_rl.append(sequence)\n",
    "    y_p_rl_scaled.append(p_rl_train_source['residual_scaled'].iloc[i])\n",
    "    ds_for_y_p_rl.append(p_rl_train_source['ds'].iloc[i])\n",
    "\n",
    "X_p_rl, y_p_rl_scaled = np.array(X_p_rl), np.array(y_p_rl_scaled)\n",
    "ds_for_y_p_rl = pd.Series(ds_for_y_p_rl, name='ds')\n",
    "\n",
    "# Reshape X_p_rl for LSTM: [samples, timesteps, features]\n",
    "# Here, features = 2 (lagged scaled residual, lagged scaled avg_package_count)\n",
    "X_p_rl = X_p_rl.reshape((X_p_rl.shape[0], X_p_rl.shape[1], 2))\n",
    "\n",
    "# Train/Validation split for P_RL LSTM (e.g., last 20% for validation)\n",
    "# This split is on the sequences derived from Prophet's training data\n",
    "validation_split_p_rl = int(len(X_p_rl) * 0.8)\n",
    "X_train_p_rl, X_val_p_rl = X_p_rl[:validation_split_p_rl], X_p_rl[validation_split_p_rl:]\n",
    "y_train_p_rl_scaled, y_val_p_rl_scaled = y_p_rl_scaled[:validation_split_p_rl], y_p_rl_scaled[validation_split_p_rl:]\n",
    "ds_val_p_rl = ds_for_y_p_rl[validation_split_p_rl:] # Dates for the validation set\n",
    "\n",
    "print(f\"P_RL LSTM training data shapes: X={X_train_p_rl.shape}, y={y_train_p_rl_scaled.shape}\")\n",
    "print(f\"P_RL LSTM validation data shapes: X={X_val_p_rl.shape}, y={y_val_p_rl_scaled.shape}\")\n",
    "\n",
    "# Define and train the P_RL LSTM model\n",
    "model_p_rl = Sequential()\n",
    "model_p_rl.add(LSTM(16, input_shape=(lookback, 2), kernel_regularizer=l2(0.001))) # 2 features\n",
    "model_p_rl.add(Dropout(0.2))\n",
    "model_p_rl.add(Dense(1)) # Output layer for predicting the scaled residual\n",
    "model_p_rl.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training P_RL LSTM model (to predict Prophet residuals)...\")\n",
    "p_rl_early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "history_p_rl = model_p_rl.fit(X_train_p_rl, y_train_p_rl_scaled,\n",
    "                              epochs=150,\n",
    "                              batch_size=2,\n",
    "                              validation_data=(X_val_p_rl, y_val_p_rl_scaled),\n",
    "                              verbose=0,\n",
    "                              callbacks=[p_rl_early_stopping])\n",
    "\n",
    "print(\"P_RL LSTM model training complete.\")\n",
    "\n",
    "# Evaluate P_RL model on its validation set (predicting Prophet's residuals)\n",
    "val_loss_p_rl = model_p_rl.evaluate(X_val_p_rl, y_val_p_rl_scaled, verbose=0)\n",
    "print(f\"P_RL LSTM Validation MSE (on scaled residuals): {val_loss_p_rl:.4f}\")\n",
    "\n",
    "# Store actual residuals for the P_RL validation period for later analysis if needed\n",
    "actual_residuals_val_p_rl_scaled = y_val_p_rl_scaled\n",
    "predicted_residuals_val_p_rl_scaled = model_p_rl.predict(X_val_p_rl, verbose=0).flatten()\n",
    "\n",
    "actual_residuals_val_p_rl = scaler_p_rl_residual.inverse_transform(actual_residuals_val_p_rl_scaled.reshape(-1, 1)).flatten()\n",
    "predicted_residuals_val_p_rl = scaler_p_rl_residual.inverse_transform(predicted_residuals_val_p_rl_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "val_mae_p_rl_residuals = mean_absolute_error(actual_residuals_val_p_rl, predicted_residuals_val_p_rl)\n",
    "print(f\"P_RL LSTM Validation MAE (on original scale residuals): {val_mae_p_rl_residuals:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adf029",
   "metadata": {},
   "source": [
    "## 12. P_RL Model – Generating Full Period Residual Forecasts and Combining with Prophet\n",
    "\n",
    "**Explanation:**  \n",
    "This block uses the trained **P_RL LSTM** to forecast **Prophet's residuals** over the entire period covered by `future_df_template`.  \n",
    "These forecasted residuals are then **added back** to Prophet's main forecast (`prophet_pred_log`) to create the **hybrid P + P_RL prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Prepare Input Data for P_RL Forecasting:\n",
    "\n",
    "- `future_df_template` already contains:\n",
    "  - Prophet's predictions (`prophet_pred_log`)\n",
    "  - `avg_package_count`\n",
    "\n",
    "- For **dates up to `cutoff_date`**:\n",
    "  - \"Actual\" `y` values are used to calculate historical **residuals** for the P_RL model's lookback window.\n",
    "\n",
    "- For **dates beyond `cutoff_date`**:\n",
    "  - Prophet’s own predictions are used as the base to estimate residuals that **P_RL will try to correct**.\n",
    "\n",
    "- Both residuals and `avg_package_count` are **scaled** using the same scalers (`scaler_p_rl_residual`, `scaler_p_rl_pkg_count`) fitted during P_RL training.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Iterative Residual Forecasting:\n",
    "\n",
    "- An **iterative (auto-regressive)** approach is used (similar to the Standalone LSTM model):\n",
    "  - The **P_RL LSTM** predicts one **scaled residual** at a time.\n",
    "  - The input sequence is updated after each step with:\n",
    "    - The **new forecasted scaled residual**\n",
    "    - The corresponding **scaled `avg_package_count`**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Inverse Transform and Combine:\n",
    "\n",
    "- The forecasted **scaled residuals** are **inverse-transformed** back to their original scale.\n",
    "- These residuals are then **added to Prophet’s log-transformed predictions** (`prophet_pred_log`) to produce the final hybrid prediction.\n",
    "- The combined output is stored in:  \n",
    "  `future_df_template['prophet_plus_p_rl_pred_log']`\n",
    "\n",
    "- Any resulting `NaN`s from the lookback period at the beginning are **forward-filled**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- P_RL Model - Generating Full Period Residual Forecasts and Combining ---\n",
    "print(\"\\n--- Generating Full Period P_RL Residual Forecasts & Combining with Prophet ---\")\n",
    "\n",
    "# We need to create the input sequences for P_RL for the entire future_df_template period.\n",
    "# This involves:\n",
    "# 1. Prophet's predictions ('prophet_pred_log') from future_df_template.\n",
    "# 2. 'avg_package_count' from future_df_template.\n",
    "# 3. For historical periods (before cutoff_date), use actual 'y' to calculate residuals.\n",
    "#    For future periods (after cutoff_date), the \"residual\" is effectively 0 if we assume Prophet's forecast is the base,\n",
    "#    or we can try to predict deviations from Prophet's future forecasts.\n",
    "#    Here, we'll predict corrections to Prophet's 'yhat' for the whole future_df_template.\n",
    "\n",
    "# Create a temporary DataFrame for P_RL inputs, aligned with future_df_template\n",
    "p_rl_input_df = future_df_template[['ds', 'y', 'prophet_pred_log', 'avg_package_count']].copy()\n",
    "\n",
    "# Calculate residuals: actual 'y' - prophet_pred_log where 'y' is available,\n",
    "# otherwise, the residual to predict is against prophet_pred_log (so, if Prophet is perfect, residual is 0)\n",
    "# For the P_RL model, it was trained on residuals = y_actual - prophet_yhat_on_train_data.\n",
    "# When forecasting, we want to predict future residuals: future_y_actual - future_prophet_yhat.\n",
    "# Since future_y_actual is unknown, P_RL predicts the expected error of Prophet.\n",
    "\n",
    "# For the initial lookback period, we need historical residuals.\n",
    "# We use the residuals calculated on Prophet's training data (p_rl_train_source)\n",
    "# and then transition to iterative prediction.\n",
    "\n",
    "# Scale avg_package_count for the entire period using the P_RL scaler\n",
    "p_rl_input_df['avg_package_count_scaled'] = scaler_p_rl_pkg_count.transform(p_rl_input_df[['avg_package_count']])\n",
    "\n",
    "# Initialize a column for scaled residuals in p_rl_input_df\n",
    "p_rl_input_df['residual_scaled_for_pred'] = np.nan\n",
    "\n",
    "# Fill known scaled residuals from the P_RL training phase\n",
    "# Align p_rl_train_source (which has 'residual_scaled') with p_rl_input_df\n",
    "temp_merge_df = pd.merge(p_rl_input_df[['ds']],\n",
    "                         p_rl_train_source[['ds', 'residual_scaled']],\n",
    "                         on='ds',\n",
    "                         how='left')\n",
    "p_rl_input_df['residual_scaled_for_pred'] = temp_merge_df['residual_scaled']\n",
    "\n",
    "\n",
    "# Iteratively predict residuals for dates where 'residual_scaled_for_pred' is NaN\n",
    "# These NaNs will typically start after the P_RL training data ends.\n",
    "\n",
    "all_p_rl_residual_preds_scaled = []\n",
    "# Find the first index where iterative prediction should start\n",
    "# This is typically lookback steps after the start of p_rl_input_df, or after known residuals end.\n",
    "first_pred_idx = lookback \n",
    "\n",
    "# If there are pre-filled residuals from training, start predicting after them.\n",
    "last_known_residual_idx = p_rl_input_df[p_rl_input_df['residual_scaled_for_pred'].notna()].index.max()\n",
    "if pd.notna(last_known_residual_idx) and last_known_residual_idx + 1 > first_pred_idx :\n",
    "    first_pred_idx = last_known_residual_idx + 1\n",
    "    # Populate all_p_rl_residual_preds_scaled with known historical scaled residuals up to this point\n",
    "    all_p_rl_residual_preds_scaled.extend(p_rl_input_df['residual_scaled_for_pred'].iloc[:first_pred_idx].tolist())\n",
    "else: # If no known residuals or they end too early, fill initial part with zeros or a mean\n",
    "    # For simplicity, if we start predicting from 'lookback', the first few values in all_p_rl_residual_preds_scaled\n",
    "    # will be NaNs if not filled from p_rl_train_source.\n",
    "    # Let's ensure the list starts with values that can form the first input sequence.\n",
    "    # We will use the actual known residuals from p_rl_train_source for the initial part of all_p_rl_residual_preds_scaled.\n",
    "    # The loop below will then append new predictions.\n",
    "    \n",
    "    # If first_pred_idx is still 'lookback', it means we don't have enough history from p_rl_train_source\n",
    "    # directly in p_rl_input_df to start iterative prediction immediately.\n",
    "    # This logic assumes p_rl_train_source's residuals are the ground truth for the start.\n",
    "    # We copy them over.\n",
    "    \n",
    "    # The critical part is the input sequence for the *first prediction*.\n",
    "    # This sequence must come from data scaled consistently with P_RL's training.\n",
    "    \n",
    "    # Let's reconstruct the scaled values for the initial sequence from p_rl_input_df\n",
    "    # using data up to the point before predictions begin.\n",
    "    \n",
    "    # The `all_p_rl_residual_preds_scaled` list will store all scaled residuals: known historical + newly predicted.\n",
    "    # It should be populated with all available historical scaled residuals first.\n",
    "    \n",
    "    # Correctly initialize `all_p_rl_residual_preds_scaled` with values from `p_rl_input_df['residual_scaled_for_pred']`\n",
    "    # up to `first_pred_idx -1`. The loop will then predict from `first_pred_idx` onwards.\n",
    "    \n",
    "    # The issue is that `p_rl_input_df['residual_scaled_for_pred']` might have NaNs even before `first_pred_idx`\n",
    "    # if `p_rl_train_source` didn't cover the beginning of `future_df_template`.\n",
    "    # For simplicity, let's assume `p_rl_train_source` provides enough history.\n",
    "    # The `temp_merge_df` step should have populated these.\n",
    "    \n",
    "    # If `all_p_rl_residual_preds_scaled` is shorter than `first_pred_idx`, it means we have a gap.\n",
    "    # This typically happens if `p_rl_train_source` doesn't align with the start of `future_df_template`.\n",
    "    # We will fill initial NaNs in `residual_scaled_for_pred` with 0 for simplicity before iterative loop.\n",
    "    p_rl_input_df['residual_scaled_for_pred'] = p_rl_input_df['residual_scaled_for_pred'].fillna(0) # Risky, but for now.\n",
    "    all_p_rl_residual_preds_scaled = p_rl_input_df['residual_scaled_for_pred'].iloc[:first_pred_idx].tolist()\n",
    "\n",
    "\n",
    "print(f\"Starting P_RL iterative prediction from index: {first_pred_idx} (date: {p_rl_input_df['ds'].iloc[first_pred_idx]})\")\n",
    "\n",
    "for i in range(first_pred_idx, len(p_rl_input_df)):\n",
    "    # Get the last 'lookback' scaled residuals and avg_package_count_scaled\n",
    "    # Residuals for the sequence come from `all_p_rl_residual_preds_scaled` which is being built\n",
    "    current_residuals_scaled_sequence = np.array(all_p_rl_residual_preds_scaled[i-lookback:i])\n",
    "    current_pkg_count_scaled_sequence = p_rl_input_df['avg_package_count_scaled'].iloc[i-lookback:i].values\n",
    "    \n",
    "    # Combine them into the 2-feature input for P_RL LSTM\n",
    "    current_input_sequence_p_rl = np.stack((current_residuals_scaled_sequence, current_pkg_count_scaled_sequence), axis=-1)\n",
    "    current_input_sequence_p_rl = current_input_sequence_p_rl.reshape(1, lookback, 2) # Reshape for LSTM\n",
    "    \n",
    "    # Predict the next scaled residual\n",
    "    pred_scaled_residual = model_p_rl.predict(current_input_sequence_p_rl, verbose=0)[0,0]\n",
    "    all_p_rl_residual_preds_scaled.append(pred_scaled_residual)\n",
    "\n",
    "# Ensure `all_p_rl_residual_preds_scaled` has the same length as `p_rl_input_df`\n",
    "# If it's shorter (e.g. due to loop range), pad with last prediction or zero\n",
    "if len(all_p_rl_residual_preds_scaled) < len(p_rl_input_df):\n",
    "    padding = [all_p_rl_residual_preds_scaled[-1]] * (len(p_rl_input_df) - len(all_p_rl_residual_preds_scaled))\n",
    "    all_p_rl_residual_preds_scaled.extend(padding)\n",
    "elif len(all_p_rl_residual_preds_scaled) > len(p_rl_input_df):\n",
    "    all_p_rl_residual_preds_scaled = all_p_rl_residual_preds_scaled[:len(p_rl_input_df)]\n",
    "\n",
    "\n",
    "# Inverse transform the predicted scaled residuals\n",
    "predicted_residuals_p_rl_log_scale = scaler_p_rl_residual.inverse_transform(np.array(all_p_rl_residual_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Add these predicted residuals to Prophet's main forecast\n",
    "future_df_template['p_rl_predicted_residual_log'] = predicted_residuals_p_rl_log_scale\n",
    "future_df_template['prophet_plus_p_rl_pred_log'] = future_df_template['prophet_pred_log'] + future_df_template['p_rl_predicted_residual_log']\n",
    "\n",
    "# Handle potential NaNs from lookback at the beginning if any (e.g. if iterative prediction didn't cover all)\n",
    "# The `all_p_rl_residual_preds_scaled` should cover the whole range.\n",
    "# NaNs in 'prophet_plus_p_rl_pred_log' would primarily come from NaNs in 'prophet_pred_log' or 'p_rl_predicted_residual_log'.\n",
    "# `future_df_template['prophet_pred_log']` should be full from Prophet.\n",
    "# `p_rl_predicted_residual_log` should be full from the iterative loop.\n",
    "# A ffill can handle any edge cases.\n",
    "future_df_template['prophet_plus_p_rl_pred_log'] = future_df_template['prophet_plus_p_rl_pred_log'].ffill()\n",
    "\n",
    "\n",
    "print(\"\\nP_RL residual forecasts generated and combined with Prophet.\")\n",
    "print(\"Future DataFrame Template Head with P+P_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'p_rl_predicted_residual_log', 'prophet_plus_p_rl_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with P+P_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'p_rl_predicted_residual_log', 'prophet_plus_p_rl_pred_log']].tail())\n",
    "\n",
    "# Clean up temporary columns if desired\n",
    "# future_df_template.drop(columns=['p_rl_predicted_residual_log'], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75685c",
   "metadata": {},
   "source": [
    "## 13. Residual-Correcting LSTM (for Standalone LSTM's Residuals – Model SL_RL) – Data Preparation & Training\n",
    "\n",
    "**Explanation:**  \n",
    "This section implements the second hybrid approach: an **LSTM that models the residuals of the Standalone LSTM (Model SL)**.  \n",
    "This is referred to as **Model SL_RL**. The methodology closely mirrors that of Model P_RL, but instead works with **Model SL's outputs and residuals**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Residual Calculation:\n",
    "\n",
    "- Predictions from the trained **Standalone LSTM (`model_standalone_lstm`)** are made on its **training data (`X_train_standalone_lstm`)**.\n",
    "- These are **scaled predictions**, which are then **inverse-transformed** to recover the **log scale** (`predicted_y_train_sl_log`).\n",
    "- Residuals are calculated as:  \n",
    "  `actual_y_train_standalone_lstm_log - predicted_y_train_sl_log`\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Data Scaling:\n",
    "\n",
    "- The residuals are **scaled** using `StandardScaler` to center them around zero and normalize the variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Sequence Creation:\n",
    "\n",
    "- Input sequences (`X`) are constructed from **lagged, scaled residuals** of Model SL.\n",
    "- Target values (`y`) are the **next scaled residual** in the sequence.\n",
    "- No external regressor (e.g., `avg_package_count`) is used in this model, under the assumption that **Model SL's residuals are primarily autocorrelated** and self-driven.\n",
    "\n",
    "---\n",
    "\n",
    "### ✂️ Train/Validation Split:\n",
    "\n",
    "- The residual sequences are split into **training** and **validation** sets, typically using an 80/20 ratio.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 SL_RL LSTM Definition & Training:\n",
    "\n",
    "- A simple **LSTM model (`model_sl_rl`)** is defined:\n",
    "  - One LSTM layer with **16 units**\n",
    "  - A **Dropout** layer to prevent overfitting\n",
    "  - A **Dense output** layer for residual prediction\n",
    "\n",
    "- The model is compiled with:\n",
    "  - **Loss function**: `'mse'` (Mean Squared Error)\n",
    "  - **Optimizer**: `'adam'`\n",
    "\n",
    "- The model is trained using:\n",
    "  - A **small batch size** (e.g., 2)\n",
    "  - Up to **150 epochs**\n",
    "  - **EarlyStopping** based on validation loss with best weight restoration\n",
    "\n",
    "- The model’s performance is evaluated on the **validation set**, and the **Validation MSE** is reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d070164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Residual-Correcting LSTM (for Standalone LSTM's residuals - Model SL_RL) ---\n",
    "print(\"\\n--- Training Residual-Correcting LSTM (for Standalone LSTM's residuals - Model SL_RL) ---\")\n",
    "\n",
    "# Get Standalone LSTM's predictions on its training data to calculate residuals\n",
    "# These predictions were originally scaled, so inverse transform them first.\n",
    "predicted_y_train_sl_scaled = model_standalone_lstm.predict(X_train_standalone_lstm, verbose=0)\n",
    "predicted_y_train_sl_log = scaler_standalone_lstm_y.inverse_transform(predicted_y_train_sl_scaled).flatten()\n",
    "\n",
    "# actual_y_train_standalone_lstm_log was stored during SL data preparation (Section 8)\n",
    "sl_rl_train_source = pd.DataFrame({\n",
    "    'ds': ds_train_standalone_lstm, # Dates corresponding to SL training targets\n",
    "    'actual_y_log': actual_y_train_standalone_lstm_log,\n",
    "    'predicted_y_sl_log': predicted_y_train_sl_log\n",
    "})\n",
    "sl_rl_train_source['residual_sl'] = sl_rl_train_source['actual_y_log'] - sl_rl_train_source['predicted_y_sl_log']\n",
    "\n",
    "# Scale residuals for SL_RL LSTM\n",
    "scaler_sl_rl_residual = StandardScaler()\n",
    "sl_rl_train_source['residual_sl_scaled'] = scaler_sl_rl_residual.fit_transform(sl_rl_train_source[['residual_sl']])\n",
    "\n",
    "# Create sequences for SL_RL LSTM (only using lagged scaled residuals of SL)\n",
    "X_sl_rl, y_sl_rl_scaled = [], []\n",
    "ds_for_y_sl_rl = []\n",
    "\n",
    "for i in range(lookback, len(sl_rl_train_source)):\n",
    "    # Feature: lagged scaled residual from Standalone LSTM\n",
    "    sequence = sl_rl_train_source['residual_sl_scaled'].iloc[i-lookback:i].values.reshape(-1, 1) # Reshape for 1 feature\n",
    "    X_sl_rl.append(sequence)\n",
    "    y_sl_rl_scaled.append(sl_rl_train_source['residual_sl_scaled'].iloc[i])\n",
    "    ds_for_y_sl_rl.append(sl_rl_train_source['ds'].iloc[i])\n",
    "\n",
    "X_sl_rl, y_sl_rl_scaled = np.array(X_sl_rl), np.array(y_sl_rl_scaled)\n",
    "ds_for_y_sl_rl = pd.Series(ds_for_y_sl_rl, name='ds')\n",
    "\n",
    "# Reshape X_sl_rl for LSTM: [samples, timesteps, features=1]\n",
    "X_sl_rl = X_sl_rl.reshape((X_sl_rl.shape[0], X_sl_rl.shape[1], 1))\n",
    "\n",
    "# Train/Validation split for SL_RL LSTM\n",
    "validation_split_sl_rl = int(len(X_sl_rl) * 0.8)\n",
    "X_train_sl_rl, X_val_sl_rl = X_sl_rl[:validation_split_sl_rl], X_sl_rl[validation_split_sl_rl:]\n",
    "y_train_sl_rl_scaled, y_val_sl_rl_scaled = y_sl_rl_scaled[:validation_split_sl_rl], y_sl_rl_scaled[validation_split_sl_rl:]\n",
    "\n",
    "print(f\"SL_RL LSTM training data shapes: X={X_train_sl_rl.shape}, y={y_train_sl_rl_scaled.shape}\")\n",
    "print(f\"SL_RL LSTM validation data shapes: X={X_val_sl_rl.shape}, y={y_val_sl_rl_scaled.shape}\")\n",
    "\n",
    "# Define and train the SL_RL LSTM model\n",
    "model_sl_rl = Sequential()\n",
    "model_sl_rl.add(LSTM(16, input_shape=(lookback, 1), kernel_regularizer=l2(0.001))) # 1 feature: SL residual\n",
    "model_sl_rl.add(Dropout(0.2))\n",
    "model_sl_rl.add(Dense(1)) # Output layer for predicting the scaled SL residual\n",
    "model_sl_rl.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(\"Training SL_RL LSTM model (to predict Standalone LSTM residuals)...\")\n",
    "sl_rl_early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "history_sl_rl = model_sl_rl.fit(X_train_sl_rl, y_train_sl_rl_scaled,\n",
    "                              epochs=150,\n",
    "                              batch_size=2,\n",
    "                              validation_data=(X_val_sl_rl, y_val_sl_rl_scaled),\n",
    "                              verbose=0,\n",
    "                              callbacks=[sl_rl_early_stopping])\n",
    "print(\"SL_RL LSTM model training complete.\")\n",
    "\n",
    "val_loss_sl_rl = model_sl_rl.evaluate(X_val_sl_rl, y_val_sl_rl_scaled, verbose=0)\n",
    "print(f\"SL_RL LSTM Validation MSE (on scaled SL residuals): {val_loss_sl_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a6bd1",
   "metadata": {},
   "source": [
    "## 14. SL_RL Model – Generating Full Period Residual Forecasts and Combining with Standalone LSTM\n",
    "\n",
    "**Explanation:**  \n",
    "This block uses the trained **SL_RL LSTM** to forecast the **Standalone LSTM (Model SL)** residuals over the full forecast horizon defined in `future_df_template`.  \n",
    "The predicted residuals are added to Model SL’s main predictions (`standalone_lstm_pred_log`) to generate the **hybrid SL + SL_RL forecast**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Prepare Input Data for SL_RL Forecasting:\n",
    "\n",
    "- `future_df_template` already contains:\n",
    "  - Predictions from Model SL (`standalone_lstm_pred_log`)\n",
    "\n",
    "- For dates **up to `cutoff_date`**:\n",
    "  - \"Actual\" `y` values are available and are used to calculate historical **residuals** for the SL_RL model’s lookback window.\n",
    "\n",
    "- For dates **after `cutoff_date`**:\n",
    "  - Model SL’s predictions are used as the base.\n",
    "  - SL_RL attempts to correct their residual errors.\n",
    "\n",
    "- These residuals are **scaled** using `scaler_sl_rl_residual`, which was fit during SL_RL training.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Iterative Residual Forecasting:\n",
    "\n",
    "- The **SL_RL LSTM** predicts one **scaled residual** at a time.\n",
    "- After each prediction:\n",
    "  - The result is used to **update the input sequence**.\n",
    "  - The process continues until the end of the forecast horizon.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Inverse Transform and Combine:\n",
    "\n",
    "- Forecasted residuals (in scaled form) are **inverse-transformed** to their original scale.\n",
    "- These are then **added to the log-transformed predictions** from Model SL (`standalone_lstm_pred_log`).\n",
    "- The combined hybrid output is stored in:  \n",
    "  `future_df_template['standalone_lstm_plus_sl_rl_pred_log']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SL_RL Model - Generating Full Period Residual Forecasts & Combining ---\n",
    "print(\"\\n--- Generating Full Period SL_RL Residual Forecasts & Combining with Standalone LSTM ---\")\n",
    "\n",
    "# We need input sequences for SL_RL for the entire future_df_template period.\n",
    "# This uses 'standalone_lstm_pred_log' as the base.\n",
    "sl_rl_input_df = future_df_template[['ds', 'y', 'standalone_lstm_pred_log']].copy()\n",
    "sl_rl_input_df.rename(columns={'standalone_lstm_pred_log': 'base_pred_log'}, inplace=True)\n",
    "\n",
    "# Calculate residuals: actual 'y' - base_pred_log where 'y' is available.\n",
    "# For the training part of SL_RL, we use the residuals from sl_rl_train_source.\n",
    "# For the forecasting part, SL_RL predicts corrections to 'base_pred_log'.\n",
    "sl_rl_input_df['residual_for_pred'] = sl_rl_input_df['y'] - sl_rl_input_df['base_pred_log']\n",
    "\n",
    "# Scale these residuals using the SL_RL scaler.\n",
    "# Note: transform only, as fit was done on training residuals.\n",
    "# We need to handle NaNs if 'y' is not available for the full future_df_template range.\n",
    "# For forecasting, the \"residual\" is what SL_RL predicts as deviation from standalone_lstm_pred_log.\n",
    "\n",
    "# Initialize a column for scaled residuals in sl_rl_input_df\n",
    "sl_rl_input_df['residual_sl_scaled_for_pred'] = np.nan\n",
    "\n",
    "# Fill known scaled residuals from the SL_RL training phase\n",
    "temp_merge_sl_rl_df = pd.merge(sl_rl_input_df[['ds']],\n",
    "                               sl_rl_train_source[['ds', 'residual_sl_scaled']],\n",
    "                               on='ds',\n",
    "                               how='left')\n",
    "sl_rl_input_df['residual_sl_scaled_for_pred'] = temp_merge_sl_rl_df['residual_sl_scaled']\n",
    "\n",
    "# Iteratively predict SL residuals\n",
    "all_sl_rl_residual_preds_scaled = []\n",
    "first_pred_idx_sl_rl = lookback\n",
    "\n",
    "last_known_sl_rl_residual_idx = sl_rl_input_df[sl_rl_input_df['residual_sl_scaled_for_pred'].notna()].index.max()\n",
    "\n",
    "if pd.notna(last_known_sl_rl_residual_idx) and last_known_sl_rl_residual_idx + 1 > first_pred_idx_sl_rl:\n",
    "    first_pred_idx_sl_rl = last_known_sl_rl_residual_idx + 1\n",
    "    all_sl_rl_residual_preds_scaled.extend(sl_rl_input_df['residual_sl_scaled_for_pred'].iloc[:first_pred_idx_sl_rl].tolist())\n",
    "else:\n",
    "    # Fill initial part with zeros if no history or not enough. This assumes residuals start around zero.\n",
    "    sl_rl_input_df['residual_sl_scaled_for_pred'] = sl_rl_input_df['residual_sl_scaled_for_pred'].fillna(0)\n",
    "    all_sl_rl_residual_preds_scaled = sl_rl_input_df['residual_sl_scaled_for_pred'].iloc[:first_pred_idx_sl_rl].tolist()\n",
    "\n",
    "print(f\"Starting SL_RL iterative prediction from index: {first_pred_idx_sl_rl} (date: {sl_rl_input_df['ds'].iloc[first_pred_idx_sl_rl]})\")\n",
    "\n",
    "for i in range(first_pred_idx_sl_rl, len(sl_rl_input_df)):\n",
    "    current_residuals_sl_scaled_sequence = np.array(all_sl_rl_residual_preds_scaled[i-lookback:i])\n",
    "    current_input_sequence_sl_rl = current_residuals_sl_scaled_sequence.reshape(1, lookback, 1) # 1 feature\n",
    "    \n",
    "    pred_scaled_sl_residual = model_sl_rl.predict(current_input_sequence_sl_rl, verbose=0)[0,0]\n",
    "    all_sl_rl_residual_preds_scaled.append(pred_scaled_sl_residual)\n",
    "\n",
    "# Ensure lengths match\n",
    "if len(all_sl_rl_residual_preds_scaled) < len(sl_rl_input_df):\n",
    "    padding_sl_rl = [all_sl_rl_residual_preds_scaled[-1]] * (len(sl_rl_input_df) - len(all_sl_rl_residual_preds_scaled))\n",
    "    all_sl_rl_residual_preds_scaled.extend(padding_sl_rl)\n",
    "elif len(all_sl_rl_residual_preds_scaled) > len(sl_rl_input_df):\n",
    "    all_sl_rl_residual_preds_scaled = all_sl_rl_residual_preds_scaled[:len(sl_rl_input_df)]\n",
    "\n",
    "# Inverse transform the predicted scaled SL residuals\n",
    "predicted_residuals_sl_rl_log_scale = scaler_sl_rl_residual.inverse_transform(np.array(all_sl_rl_residual_preds_scaled).reshape(-1,1)).flatten()\n",
    "\n",
    "# Add these predicted residuals to Standalone LSTM's main forecast\n",
    "future_df_template['sl_rl_predicted_residual_log'] = predicted_residuals_sl_rl_log_scale\n",
    "future_df_template['standalone_lstm_plus_sl_rl_pred_log'] = future_df_template['standalone_lstm_pred_log'] + future_df_template['sl_rl_predicted_residual_log']\n",
    "future_df_template['standalone_lstm_plus_sl_rl_pred_log'] = future_df_template['standalone_lstm_plus_sl_rl_pred_log'].ffill()\n",
    "\n",
    "print(\"\\nSL_RL residual forecasts generated and combined with Standalone LSTM.\")\n",
    "print(\"Future DataFrame Template Head with SL+SL_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log', 'standalone_lstm_plus_sl_rl_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with SL+SL_RL Predictions:\")\n",
    "print(future_df_template[['ds', 'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log', 'standalone_lstm_plus_sl_rl_pred_log']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c5ad4",
   "metadata": {},
   "source": [
    "## 15. Bayesian Model Averaging (BMA)\n",
    "\n",
    "**Explanation:**  \n",
    "**Bayesian Model Averaging (BMA)** is a technique for combining forecasts from multiple models.  \n",
    "Rather than selecting a single \"best\" model, BMA creates a **weighted average forecast**, where each model’s contribution is proportional to its **reliability or past performance**.\n",
    "\n",
    "In this demonstration, we’ll use **simple, pre-defined weights** for combining model outputs.  \n",
    "> *Note: In a production setup, these weights can and should be optimized — for example, using validation-set error metrics like MAE, RMSE, or log-likelihood.*\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Models Included in the Averaging:\n",
    "\n",
    "- **Prophet (Model P)**  \n",
    "  Forecast stored in: `prophet_pred_log`\n",
    "\n",
    "- **Standalone LSTM (Model SL)**  \n",
    "  Forecast stored in: `standalone_lstm_pred_log`\n",
    "\n",
    "- **Prophet + P_RL Hybrid Model**  \n",
    "  Forecast stored in: `prophet_plus_p_rl_pred_log`\n",
    "\n",
    "- **Standalone LSTM + SL_RL Hybrid Model**  \n",
    "  Forecast stored in: `standalone_lstm_plus_sl_rl_pred_log`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Averaging Logic:\n",
    "\n",
    "- All model outputs are in the **log-transformed space**.\n",
    "- BMA produces a final forecast as a **weighted sum of log-scale predictions**.\n",
    "- Each model can be assigned a weight (e.g., 0.25 each for equal weighting).\n",
    "- The result reflects the **combined strength** of different modeling strategies — both classical (Prophet) and neural (LSTM), standalone and hybrid.\n",
    "\n",
    "---\n",
    "\n",
    "### ℹ️ Note:\n",
    "\n",
    "- The `avg_package_count` regressor is **implicitly included** in models based on Prophet, and partially in hybrid versions as well.\n",
    "- BMA doesn’t re-use this regressor directly but benefits from models that already incorporate it during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bayesian Model Averaging (BMA) ---\n",
    "print(\"\\n--- Performing Bayesian Model Averaging ---\")\n",
    "\n",
    "# Define weights for each model (these are illustrative and can be optimized)\n",
    "# For simplicity, let's give equal weight to the two hybrid models,\n",
    "# and perhaps less to the standalone ones if hybrids are expected to be better.\n",
    "# Example weights:\n",
    "weights = {\n",
    "    'prophet': 0.15,\n",
    "    'standalone_lstm': 0.15,\n",
    "    'prophet_plus_p_rl': 0.35,\n",
    "    'standalone_lstm_plus_sl_rl': 0.35\n",
    "}\n",
    "\n",
    "# Ensure weights sum to 1 (or normalize them)\n",
    "total_weight = sum(weights.values())\n",
    "if not np.isclose(total_weight, 1.0):\n",
    "    print(f\"Warning: Weights do not sum to 1 (sum={total_weight}). Normalizing weights.\")\n",
    "    weights = {model: wt / total_weight for model, wt in weights.items()}\n",
    "\n",
    "# Check if all necessary prediction columns exist\n",
    "required_cols_for_bma = ['prophet_pred_log', 'standalone_lstm_pred_log', \n",
    "                         'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log']\n",
    "missing_cols = [col for col in required_cols_for_bma if col not in future_df_template.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns for BMA in future_df_template: {missing_cols}\")\n",
    "\n",
    "# Fill NaNs in prediction columns with a fallback (e.g., ffill then bfill, or with Prophet's prediction)\n",
    "# This is crucial as NaNs in any component will result in NaN for BMA.\n",
    "for col in required_cols_for_bma:\n",
    "    if future_df_template[col].isnull().any():\n",
    "        print(f\"Warning: NaNs found in {col}. Forward-filling then backward-filling.\")\n",
    "        future_df_template[col] = future_df_template[col].ffill().bfill()\n",
    "    # As a final fallback, if still NaNs (e.g. entire column was NaN for some reason)\n",
    "    if future_df_template[col].isnull().any():\n",
    "         future_df_template[col] = future_df_template['prophet_pred_log'] # Fallback to Prophet\n",
    "         print(f\"Warning: NaNs persisted in {col} after ffill/bfill. Filled with Prophet predictions.\")\n",
    "\n",
    "\n",
    "# Calculate the BMA forecast (weighted average of log-transformed predictions)\n",
    "future_df_template['bma_pred_log'] = (\n",
    "    weights['prophet'] * future_df_template['prophet_pred_log'] +\n",
    "    weights['standalone_lstm'] * future_df_template['standalone_lstm_pred_log'] +\n",
    "    weights['prophet_plus_p_rl'] * future_df_template['prophet_plus_p_rl_pred_log'] +\n",
    "    weights['standalone_lstm_plus_sl_rl'] * future_df_template['standalone_lstm_plus_sl_rl_pred_log']\n",
    ")\n",
    "\n",
    "print(\"BMA log predictions calculated.\")\n",
    "print(\"Future DataFrame Template Head with BMA Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log', 'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log', 'bma_pred_log']].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with BMA Predictions:\")\n",
    "print(future_df_template[['ds', 'prophet_pred_log', 'standalone_lstm_pred_log', 'prophet_plus_p_rl_pred_log', 'standalone_lstm_plus_sl_rl_pred_log', 'bma_pred_log']].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed535db6",
   "metadata": {},
   "source": [
    "## 16. Inverse Transform Predictions\n",
    "\n",
    "**Explanation:**  \n",
    "All model predictions are currently in the **log-transformed scale** because of the `np.log1p` transformation applied to the target variable `y` (see Section 3).  \n",
    "To make these predictions interpretable and comparable to the **original submission volumes**, we need to **inverse transform** them using `np.expm1`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Predictions to Inverse Transform:\n",
    "\n",
    "- `prophet_pred_log`\n",
    "- `standalone_lstm_pred_log`\n",
    "- `prophet_plus_p_rl_pred_log`\n",
    "- `standalone_lstm_plus_sl_rl_pred_log`\n",
    "- `bma_pred_log` (Bayesian Model Averaged output)\n",
    "\n",
    "Each of these columns will be inverse transformed using:\n",
    "\n",
    "```python\n",
    "np.expm1(predicted_log_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inverse Transform Predictions ---\n",
    "print(\"\\n--- Inverse Transforming Predictions to Original Scale ---\")\n",
    "\n",
    "# Merge original 'y' values for comparison\n",
    "future_df_template = pd.merge(future_df_template, monthly_df[['ds', 'y_original']], on='ds', how='left')\n",
    "\n",
    "# List of prediction columns (log scale)\n",
    "log_pred_columns = [\n",
    "    'prophet_pred_log',\n",
    "    'standalone_lstm_pred_log',\n",
    "    'prophet_plus_p_rl_pred_log',\n",
    "    'standalone_lstm_plus_sl_rl_pred_log',\n",
    "    'bma_pred_log'\n",
    "]\n",
    "\n",
    "# Inverse transform each prediction column\n",
    "for log_col in log_pred_columns:\n",
    "    original_scale_col = log_col.replace('_log', '_orig')\n",
    "    # Ensure the column exists before trying to transform\n",
    "    if log_col in future_df_template.columns:\n",
    "        future_df_template[original_scale_col] = np.expm1(future_df_template[log_col])\n",
    "        # Handle potential negative predictions if models behave unexpectedly (np.expm1 can handle negatives, but resulting values might be < 0)\n",
    "        # For submission counts, predictions should ideally be non-negative.\n",
    "        future_df_template[original_scale_col] = np.maximum(0, future_df_template[original_scale_col]) \n",
    "    else:\n",
    "        print(f\"Warning: Log prediction column {log_col} not found for inverse transformation.\")\n",
    "\n",
    "print(\"Predictions inverse transformed to original scale.\")\n",
    "print(\"Future DataFrame Template Head with Original Scale Predictions:\")\n",
    "cols_to_show = ['ds', 'y_original'] + [col.replace('_log', '_orig') for col in log_pred_columns if col.replace('_log', '_orig') in future_df_template.columns]\n",
    "print(future_df_template[cols_to_show].head())\n",
    "print(\"\\nFuture DataFrame Template Tail with Original Scale Predictions:\")\n",
    "print(future_df_template[cols_to_show].tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1a43c",
   "metadata": {},
   "source": [
    "## 17. Evaluation\n",
    "\n",
    "**Explanation:**  \n",
    "In this final step, we evaluate the performance of **all forecasting models** on the **original (non-log-transformed) scale**.  \n",
    "Two key evaluation metrics are used:\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "- **Mean Absolute Error (MAE)**\n",
    "\n",
    "---\n",
    "\n",
    "### 📅 Evaluation Period:\n",
    "\n",
    "- The evaluation is performed over a **common time range**, defined by:\n",
    "  - `eval_start_common` → typically equal to `cutoff_date`\n",
    "  - `eval_end_common` → end of the forecast horizon\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Evaluation Workflow:\n",
    "\n",
    "1. A new `DataFrame` called `evaluation_df` is created.\n",
    "   - It includes:\n",
    "     - `ds` (date column)\n",
    "     - `y_original` (true values on original scale)\n",
    "     - All **model predictions** after inverse transformation\n",
    "\n",
    "2. For each model:\n",
    "   - RMSE is calculated:\n",
    "     ```python\n",
    "     sqrt(mean_squared_error(actual, predicted))\n",
    "     ```\n",
    "   - MAE is calculated:\n",
    "     ```python\n",
    "     mean_absolute_error(actual, predicted)\n",
    "     ```\n",
    "\n",
    "3. The results for each model are stored in a **summary table** (e.g., `metrics_summary_df`) for easy **side-by-side comparison**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Metrics Summary Output:\n",
    "The summary `DataFrame` might look like this:\n",
    "\n",
    "| Model                    | RMSE    | MAE     |\n",
    "|-------------------------|---------|---------|\n",
    "| Prophet (P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c714015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation ---\n",
    "print(\"\\n--- Evaluating Model Performance on Original Scale ---\")\n",
    "\n",
    "# Define the common evaluation period (already defined as eval_start_common and eval_end_common)\n",
    "evaluation_period_df = future_df_template[\n",
    "    (future_df_template['ds'] >= eval_start_common) &\n",
    "    (future_df_template['ds'] <= eval_end_common)\n",
    "].copy()\n",
    "\n",
    "# Ensure 'y_original' is present for evaluation\n",
    "if 'y_original' not in evaluation_period_df.columns or evaluation_period_df['y_original'].isnull().all():\n",
    "    raise ValueError(\"'y_original' is missing or all NaNs in the evaluation period. Cannot evaluate.\")\n",
    "\n",
    "# Drop rows where y_original is NaN for fair comparison (if any future dates had no actuals)\n",
    "evaluation_period_df.dropna(subset=['y_original'], inplace=True)\n",
    "\n",
    "if evaluation_period_df.empty:\n",
    "    print(\"Warning: Evaluation period is empty after dropping NaNs in y_original. No evaluation possible.\")\n",
    "    results_summary = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "else:\n",
    "    models_to_evaluate = {\n",
    "        'Prophet (P)': 'prophet_pred_orig',\n",
    "        'Standalone LSTM (SL)': 'standalone_lstm_pred_orig',\n",
    "        'Prophet + P_RL': 'prophet_plus_p_rl_pred_orig',\n",
    "        'SL + SL_RL': 'standalone_lstm_plus_sl_rl_pred_orig',\n",
    "        'BMA': 'bma_pred_orig'\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name, pred_col_orig in models_to_evaluate.items():\n",
    "        if pred_col_orig in evaluation_period_df.columns and not evaluation_period_df[pred_col_orig].isnull().all():\n",
    "            actuals = evaluation_period_df['y_original']\n",
    "            predictions = evaluation_period_df[pred_col_orig]\n",
    "            \n",
    "            # Ensure no NaNs in predictions for the slice being evaluated\n",
    "            valid_indices = actuals.notna() & predictions.notna()\n",
    "            if not valid_indices.any():\n",
    "                print(f\"Warning: No valid (non-NaN) actuals or predictions for {model_name} in evaluation period.\")\n",
    "                rmse = np.nan\n",
    "                mae = np.nan\n",
    "            else:\n",
    "                rmse = np.sqrt(mean_squared_error(actuals[valid_indices], predictions[valid_indices]))\n",
    "                mae = mean_absolute_error(actuals[valid_indices], predictions[valid_indices])\n",
    "            \n",
    "            results.append({'Model': model_name, 'RMSE': rmse, 'MAE': mae})\n",
    "            print(f\"{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: Prediction column {pred_col_orig} for model {model_name} not found or all NaNs in evaluation data. Skipping.\")\n",
    "            results.append({'Model': model_name, 'RMSE': np.nan, 'MAE': np.nan})\n",
    "\n",
    "    results_summary = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n--- Model Evaluation Summary (Original Scale) ---\")\n",
    "print(f\"Evaluation Period: {evaluation_period_df['ds'].min().date()} to {evaluation_period_df['ds'].max().date()}\")\n",
    "if results_summary.empty:\n",
    "    print(\"No models were evaluated.\")\n",
    "else:\n",
    "    print(results_summary.sort_values(by='RMSE'))\n",
    "\n",
    "# Display the final few predictions for context\n",
    "print(\"\\nFinal Predictions Table (Original Scale - Last 12 Months of Future DF or Full Eval Period):\")\n",
    "display_df_end_limit = min(len(future_df_template), 12) \n",
    "if not evaluation_period_df.empty:\n",
    "    display_df = evaluation_period_df.tail(display_df_end_limit)\n",
    "else:\n",
    "    display_df = future_df_template.tail(display_df_end_limit)\n",
    "    \n",
    "cols_to_display_final = ['ds', 'y_original'] + [m[1] for m in models_to_evaluate.values() if m[1] in display_df.columns]\n",
    "print(display_df[cols_to_display_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e0a14",
   "metadata": {},
   "source": [
    "## 18. Conclusion and Model Selection\n",
    "\n",
    "**Explanation:**  \n",
    "Based on the evaluation metrics and model outputs, we can now draw final conclusions and make informed recommendations about model selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Results Summary (User-Provided Metrics):\n",
    "\n",
    "#### ✅ Prophet Model Only (P):\n",
    "- **RMSE**: 736.90  \n",
    "- **MAE**: 349.42\n",
    "\n",
    "#### ✅ Standalone LSTM Model Only (SL):\n",
    "- **RMSE**: 191.33  \n",
    "- **MAE**: 158.62\n",
    "\n",
    "#### ✅ Combined Model (Prophet + Residual LSTM - P + P_RL):  \n",
    "(*`prophet_plus_p_rl_pred_orig` in notebook*)\n",
    "- **RMSE**: 736.90  \n",
    "- **MAE**: 349.42\n",
    "\n",
    "#### ✅ Combined Model (Standalone LSTM + Residual LSTM via BMA - SL + SL_RL):  \n",
    "(*`standalone_lstm_plus_sl_rl_pred_orig`, user's best BMA-weighted hybrid*)\n",
    "- **RMSE**: 173.46  \n",
    "- **MAE**: 144.82  \n",
    "- **BMA Weights Used**: `SL = 0.9097`, `SL+RP = 0.0903`\n",
    "\n",
    "#### ✅ BMA (Notebook's Demonstration Weights):\n",
    "- **RMSE**: *(See `results_summary` output in Section 17)*  \n",
    "- **MAE**: *(See `results_summary` output in Section 17)*\n",
    "\n",
    "> *Note: Validation metrics for `P_RL` (e.g., RMSE: 0.0109, MAE: 0.0088) are from log-scale residuals and not directly comparable to original-scale forecast evaluations.*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Analysis:\n",
    "\n",
    "- **Standalone LSTM (SL)** clearly outperforms **Prophet**, with a significantly lower RMSE.\n",
    "- **Hybrid SL + SL_RL** (Standalone LSTM plus its residual-correcting LSTM) further improves performance.\n",
    "- The **BMA ensemble**, when **heavily weighted toward SL and SL_RL**, produces the **best overall result**.\n",
    "- **Hybrid Prophet + P_RL** offers **no improvement** over Prophet alone, indicating LSTM residual correction may not effectively address Prophet's forecasting errors in this case.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Recommendations:\n",
    "\n",
    "#### ✅ **Primary Model Candidate:**\n",
    "- **Best Performing Model**:  \n",
    "  **Combined SL + SL_RL**, with an RMSE of 173.46 and MAE of 144.82.\n",
    "- **Corresponds to**:  \n",
    "  `standalone_lstm_plus_sl_rl_pred_orig` in the notebook.\n",
    "- **Supported by BMA Weights**:  \n",
    "  `SL = 0.9097`, `SL+RP = 0.0903`\n",
    "\n",
    "#### ⚙️ **Optimize BMA Weights**:\n",
    "- The BMA implementation currently uses **illustrative weights**.\n",
    "- Future versions should optimize these weights using:\n",
    "  - Validation set performance\n",
    "  - Bayesian techniques\n",
    "  - Grid search or optimization libraries\n",
    "\n",
    "#### 🔍 **Code and Pipeline Review**:\n",
    "- Ensure **scaling/transformations** are applied consistently (fit on training only, transform on test).\n",
    "- Validate **input alignment** across all models.\n",
    "- Review external scripts or reporting tools for **typos** (e.g., `bма_weight_P`) in model identifiers or metric labels.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ General Summary:\n",
    "\n",
    "This notebook demonstrates a **robust, multi-model time series forecasting pipeline** that compares:\n",
    "\n",
    "- Classical statistical methods (Prophet)\n",
    "- Neural forecasting (LSTM)\n",
    "- Residual-correcting hybrids (P_RL, SL_RL)\n",
    "- Ensemble methods (BMA)\n",
    "\n",
    "The key takeaway is the **effectiveness of LSTM models**, particularly when enhanced with **residual modeling and ensemble averaging**.\n",
    "\n",
    "The final recommended model — **SL + SL_RL hybrid with optimized BMA weights** — delivers the most accurate predictions on this submission volume dataset and provides a strong foundation for future production use or further tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d77b0",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This section prepares and exports the historical actual submission volumes and the forecasted volumes from the best performing model (standalone_lstm_plus_sl_rl_pred_orig based on the conclusion) to a CSV file. This file can then be used for reporting or visualization in tools like Power BI. The forecast covers the future period defined by future_df_template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import os # For path operations if needed in on_export_button_clicked\n",
    "from datetime import datetime # Not strictly needed if using pd.Timestamp correctly\n",
    "import pandas as pd # Assuming pandas is used, ensure it's imported\n",
    "import numpy as np  # Assuming numpy is used, ensure it's imported\n",
    "\n",
    "# --- Export Historical Data and Next Month Forecast to CSV ---\n",
    "print(\"\\n--- Exporting Historical and Next Month Forecast to CSV ---\")\n",
    "\n",
    "# Define chosen forecast column (log and original scale)\n",
    "chosen_forecast_col_log = 'standalone_lstm_plus_sl_rl_pred_log'\n",
    "chosen_forecast_col_orig = 'standalone_lstm_plus_sl_rl_pred_orig'\n",
    "\n",
    "if chosen_forecast_col_log not in future_df_template.columns:\n",
    "    print(f\"Warning: Chosen forecast column '{chosen_forecast_col_log}' not found. Falling back to 'bma_pred_log'.\")\n",
    "    chosen_forecast_col_log = 'bma_pred_log'\n",
    "    chosen_forecast_col_orig = 'bma_pred_orig'\n",
    "    if chosen_forecast_col_log not in future_df_template.columns:\n",
    "        raise ValueError(f\"Fallback forecast column '{chosen_forecast_col_log}' also not found. Cannot export.\")\n",
    "\n",
    "# Widgets for file picker and export button\n",
    "file_picker = widgets.Text(\n",
    "    value='submission_volume_full_forecast.csv', # Default file name\n",
    "    description='CSV File Path:',\n",
    "    placeholder='Enter file path or name...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "export_button = widgets.Button(description=\"Export Full Report to CSV\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_export_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output() # Clear previous messages\n",
    "        print(\"Preparing data for export...\")\n",
    "        try:\n",
    "            # Part 1: Historical data and existing forecasts from future_df_template\n",
    "            # This covers data up to eval_end_common (e.g., Feb 2025)\n",
    "            part1_df = future_df_template[['ds', 'y_original', chosen_forecast_col_orig, 'avg_package_count', \n",
    "                                           'standalone_lstm_pred_log', 'sl_rl_predicted_residual_log']].copy()\n",
    "            part1_df.rename(columns={chosen_forecast_col_orig: 'ForecastedVolume'}, inplace=True)\n",
    "            \n",
    "            # Determine type for part1_df\n",
    "            part1_df['Type'] = np.where(part1_df['y_original'].notnull(), 'Historical', 'Existing Forecast')\n",
    "\n",
    "            # Part 2: Generate new 1-month forecast (e.g., Mar 2025)\n",
    "            print(\"Generating new 1-month forecast extension...\")\n",
    "            last_date_in_template = future_df_template['ds'].max() \n",
    "            new_forecast_start_ds = last_date_in_template + pd.DateOffset(months=1)\n",
    "            \n",
    "            num_forecast_months = 1 # Number of months to forecast\n",
    "            new_forecast_dates = pd.date_range(start=new_forecast_start_ds, periods=num_forecast_months, freq='M')\n",
    "\n",
    "            # Prepare avg_package_count for these new dates (ffill from last known)\n",
    "            # Ensure 'monthly_df' and 'lookback' are defined in your notebook environment\n",
    "            last_known_avg_pkg_count = monthly_df['avg_package_count'].iloc[-1]\n",
    "            new_avg_pkg_counts = pd.Series([last_known_avg_pkg_count] * num_forecast_months)\n",
    "            \n",
    "            df_new_forecast_input = pd.DataFrame({'ds': new_forecast_dates, 'avg_package_count': new_avg_pkg_counts})\n",
    "\n",
    "            # --- Generate SL predictions for new forecast period ---\n",
    "            new_sl_preds_log_extended = []\n",
    "            last_sl_log_preds_for_input = future_df_template['standalone_lstm_pred_log'].iloc[-lookback:].values\n",
    "            current_sl_sequence_scaled = scaler_standalone_lstm_y.transform(last_sl_log_preds_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "            for _ in range(num_forecast_months):\n",
    "                pred_scaled_sl = model_standalone_lstm.predict(current_sl_sequence_scaled, verbose=0)[0,0]\n",
    "                pred_log_sl = scaler_standalone_lstm_y.inverse_transform([[pred_scaled_sl]])[0,0]\n",
    "                new_sl_preds_log_extended.append(pred_log_sl)\n",
    "                # Update sequence for next prediction if num_forecast_months > 1\n",
    "                if num_forecast_months > 1 or _ < num_forecast_months -1 : # only append if there are more steps or it's not the last step of a multi-step forecast\n",
    "                     current_sl_sequence_scaled = np.append(current_sl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl]]], axis=1)\n",
    "\n",
    "            df_new_forecast_input['standalone_lstm_pred_log_extended'] = new_sl_preds_log_extended\n",
    "\n",
    "            # --- Generate SL_RL residual predictions for new forecast period ---\n",
    "            new_sl_rl_resid_preds_log_extended = []\n",
    "            last_sl_rl_log_resids_for_input = future_df_template['sl_rl_predicted_residual_log'].iloc[-lookback:].values\n",
    "            current_sl_rl_sequence_scaled = scaler_sl_rl_residual.transform(last_sl_rl_log_resids_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "            for _ in range(num_forecast_months):\n",
    "                pred_scaled_sl_rl_resid = model_sl_rl.predict(current_sl_rl_sequence_scaled, verbose=0)[0,0]\n",
    "                pred_log_sl_rl_resid = scaler_sl_rl_residual.inverse_transform([[pred_scaled_sl_rl_resid]])[0,0]\n",
    "                new_sl_rl_resid_preds_log_extended.append(pred_log_sl_rl_resid)\n",
    "                # Update sequence for next prediction if num_forecast_months > 1\n",
    "                if num_forecast_months > 1 or _ < num_forecast_months -1 : # only append if there are more steps or it's not the last step of a multi-step forecast\n",
    "                    current_sl_rl_sequence_scaled = np.append(current_sl_rl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl_rl_resid]]], axis=1)\n",
    "            \n",
    "            df_new_forecast_input['sl_rl_predicted_residual_log_extended'] = new_sl_rl_resid_preds_log_extended\n",
    "            \n",
    "            # Combine extended predictions\n",
    "            df_new_forecast_input['ForecastedVolume_log'] = df_new_forecast_input['standalone_lstm_pred_log_extended'] + df_new_forecast_input['sl_rl_predicted_residual_log_extended']\n",
    "            df_new_forecast_input['ForecastedVolume'] = np.expm1(df_new_forecast_input['ForecastedVolume_log'])\n",
    "            df_new_forecast_input['y_original'] = np.nan # Actuals are NaN for new forecasts\n",
    "            df_new_forecast_input['Type'] = f'New {num_forecast_months}M Forecast' # Updated type string\n",
    "            \n",
    "            df_new_1m_export = df_new_forecast_input[['ds', 'y_original', 'ForecastedVolume', 'Type']]\n",
    "\n",
    "            # Combine all parts\n",
    "            final_export_df = pd.concat([\n",
    "                part1_df[['ds', 'y_original', 'ForecastedVolume', 'Type']], \n",
    "                df_new_1m_export\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            final_export_df.rename(columns={\n",
    "                'ds': 'Date',\n",
    "                'y_original': 'ActualVolume'\n",
    "            }, inplace=True)\n",
    "\n",
    "            # Ensure ForecastedVolume is integer (whole number) and handles NaNs from historical actuals appropriately\n",
    "            final_export_df['ForecastedVolume'] = final_export_df['ForecastedVolume'].round().astype('Int64')\n",
    "            final_export_df['ActualVolume'] = final_export_df['ActualVolume'].astype('Int64')\n",
    "\n",
    "\n",
    "            file_path = file_picker.value.strip()\n",
    "            if not file_path:\n",
    "                raise ValueError(\"Please specify a file path for export.\")\n",
    "            \n",
    "            dir_name = os.path.dirname(file_path)\n",
    "            if dir_name and not os.path.exists(dir_name): \n",
    "                os.makedirs(dir_name)\n",
    "                print(f\"Created directory: {dir_name}\")\n",
    "            \n",
    "            final_export_df.to_csv(file_path, index=False, date_format='%Y-%m-%d')\n",
    "            print(f\"Successfully exported data to '{file_path}'\")\n",
    "            print(\"Columns in exported CSV: \", final_export_df.columns.tolist())\n",
    "            print(\"Sample of exported data (last 15 rows):\")\n",
    "            print(final_export_df.tail(15))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during export: {e}\")\n",
    "\n",
    "export_button.on_click(on_export_button_clicked)\n",
    "display(file_picker, export_button, output_area)\n",
    "\n",
    "print(f\"\\nSet the file path above and click '{export_button.description}' to save the CSV report.\")\n",
    "print(\"The report will include historical data, model fit, and a new 1-month forecast (e.g., Mar 2025).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating New 12-Month Iterative Forecast (Mar 2025 - Feb 2026) ---\")\n",
    "\n",
    "# Determine the starting point for the new 12-month forecast\n",
    "last_date_in_template = future_df_template['ds'].max()  # This is eval_end_common (2025-02-28)\n",
    "new_forecast_start_ds = last_date_in_template + pd.DateOffset(months=1)\n",
    "new_12m_dates = pd.date_range(start=new_forecast_start_ds, periods=12, freq='M')\n",
    "\n",
    "print(f\"Generating 12 new monthly forecasts from {new_12m_dates.min().date()} to {new_12m_dates.max().date()}\")\n",
    "\n",
    "# DataFrame to store the new 12-month forecast\n",
    "df_extended_forecast = pd.DataFrame({'ds': new_12m_dates})\n",
    "\n",
    "# --- 1. Standalone LSTM (SL) Component: Generate 12 new iterative predictions ---\n",
    "new_sl_preds_log_extended = []\n",
    "\n",
    "# Initial input sequence for SL model:\n",
    "# Last 'lookback' values of 'standalone_lstm_pred_log' from future_df_template, scaled.\n",
    "last_sl_log_preds_for_input = future_df_template['standalone_lstm_pred_log'].iloc[-lookback:].values\n",
    "current_sl_sequence_scaled = scaler_standalone_lstm_y.transform(last_sl_log_preds_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "for i in range(12):\n",
    "    pred_scaled_sl = model_standalone_lstm.predict(current_sl_sequence_scaled, verbose=0)[0, 0]\n",
    "    pred_log_sl = scaler_standalone_lstm_y.inverse_transform([[pred_scaled_sl]])[0, 0]\n",
    "    new_sl_preds_log_extended.append(pred_log_sl)\n",
    "    # Update sequence for next prediction\n",
    "    current_sl_sequence_scaled = np.append(current_sl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl]]], axis=1)\n",
    "\n",
    "df_extended_forecast['sl_pred_log_extended'] = new_sl_preds_log_extended\n",
    "\n",
    "# --- 2. Residual LSTM (SL_RL) Component: Generate 12 new iterative predictions ---\n",
    "new_sl_rl_resid_preds_log_extended = []\n",
    "\n",
    "# Initial input sequence for SL_RL model:\n",
    "# Last 'lookback' values of 'sl_rl_predicted_residual_log' from future_df_template, scaled.\n",
    "# This column was created in Section 14.\n",
    "last_sl_rl_log_resids_for_input = future_df_template['sl_rl_predicted_residual_log'].iloc[-lookback:].values\n",
    "current_sl_rl_sequence_scaled = scaler_sl_rl_residual.transform(last_sl_rl_log_resids_for_input.reshape(-1, 1)).reshape(1, lookback, 1)\n",
    "\n",
    "for i in range(12):\n",
    "    pred_scaled_sl_rl_resid = model_sl_rl.predict(current_sl_rl_sequence_scaled, verbose=0)[0, 0]\n",
    "    pred_log_sl_rl_resid = scaler_sl_rl_residual.inverse_transform([[pred_scaled_sl_rl_resid]])[0, 0]\n",
    "    new_sl_rl_resid_preds_log_extended.append(pred_log_sl_rl_resid)\n",
    "    # Update sequence for next prediction\n",
    "    current_sl_rl_sequence_scaled = np.append(current_sl_rl_sequence_scaled[:, 1:, :], [[[pred_scaled_sl_rl_resid]]], axis=1)\n",
    "\n",
    "df_extended_forecast['sl_rl_resid_pred_log_extended'] = new_sl_rl_resid_preds_log_extended\n",
    "\n",
    "# --- 3. Combine and Finalize New 12-Month Forecast ---\n",
    "df_extended_forecast['final_extended_pred_log'] = df_extended_forecast['sl_pred_log_extended'] + df_extended_forecast['sl_rl_resid_pred_log_extended']\n",
    "df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'] = np.expm1(df_extended_forecast['final_extended_pred_log'])\n",
    "df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'] = df_extended_forecast['ForecastedVolume_SL_plus_SL_RL'].round().astype('Int64') # Whole numbers\n",
    "\n",
    "print(\"\\n--- New 12-Month Forecast (March 2025 - February 2026) ---\")\n",
    "print(df_extended_forecast[['ds', 'ForecastedVolume_SL_plus_SL_RL']])\n",
    "\n",
    "# This df_extended_forecast can now be appended to the historical/existing forecast data for a complete timeline if needed.\n",
    "# For example, for the CSV export in Section 19, this logic is integrated into the button's callback.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
